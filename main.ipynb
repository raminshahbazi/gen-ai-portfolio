{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PijFkr3QSHnN"
      },
      "source": [
        "## Hello World"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "XN1vC_2YSHnO",
        "outputId": "179d3b26-e7af-4e6e-db6e-d72c4b88721a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hellp world\n"
          ]
        }
      ],
      "source": [
        "print(\"hellp world\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install Requirements ()"
      ],
      "metadata": {
        "id": "N9QpaHdWScCx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install autogen-agentchat autogen-ext[openai] -q\n",
        "!pip install groq -q"
      ],
      "metadata": {
        "id": "Bv3056-vSioY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "tokenGROQ = getpass('Enter GROQ_API_KEY here: ')\n",
        "os.environ[\"GROQ_API_KEY\"] = tokenGROQ"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0i8fNwjgR_1Q",
        "outputId": "fd086c41-9ff0-4e61-b223-edaf51d49525"
      },
      "execution_count": 52,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter GROQ_API_KEY here: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from autogen_agentchat.agents import AssistantAgent, UserProxyAgent\n",
        "from autogen_agentchat.conditions import TextMentionTermination\n",
        "from autogen_agentchat.teams import RoundRobinGroupChat\n",
        "from autogen_agentchat.ui import Console\n",
        "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
        "\n",
        "# Create the agents.\n",
        "custom_model_client = OpenAIChatCompletionClient(\n",
        "    model=\"llama-3.3-70b-versatile\",\n",
        "    base_url=\"https://api.groq.com/openai/v1\",\n",
        "    api_key=os.environ[\"GROQ_API_KEY\"],\n",
        "    model_info={\n",
        "        \"vision\": False,\n",
        "        \"function_calling\": True,\n",
        "        \"json_output\": False,\n",
        "        \"family\": \"unknown\",\n",
        "    },\n",
        ")\n",
        "assistant = AssistantAgent(\"assistant\", model_client=custom_model_client)\n",
        "user_proxy = UserProxyAgent(\"user_proxy\", input_func=input)  # Use input() to get user input from console.\n",
        "\n",
        "# Create the termination condition which will end the conversation when the user says \"APPROVE\".\n",
        "termination = TextMentionTermination(\"DONE\")\n",
        "\n",
        "# Create the team.\n",
        "team = RoundRobinGroupChat([assistant, user_proxy], termination_condition=termination)\n",
        "\n",
        "# Run the conversation and stream to the console.\n",
        "stream = team.run_stream(task=\"the pdf file is a little bit large, can you read it if i sent it to you completely, or you need it in small chunks?\")\n",
        "await Console(stream)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G_wawy2QRzD7",
        "outputId": "223c86c2-0710-48ee-b775-26b8ae765601"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------- user ----------\n",
            "the pdf file is a little bit large, can you read it if i sent it to you completely, or you need it in small chunks?\n",
            "---------- assistant ----------\n",
            "I'm a large language model, I don't have the capability to directly receive or read files, including PDFs. I can only process text-based input. If you'd like to share the content of the PDF with me, you can try copying and pasting the text into this chat window. If the text is too long, you can break it up into smaller chunks and share them with me sequentially. I'll do my best to assist you.\n",
            "Enter your response: DONE\n",
            "---------- user_proxy ----------\n",
            "DONE\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TaskResult(messages=[TextMessage(source='user', models_usage=None, content='the pdf file is a little bit large, can you read it if i sent it to you completely, or you need it in small chunks?', type='TextMessage'), TextMessage(source='assistant', models_usage=RequestUsage(prompt_tokens=88, completion_tokens=92), content=\"I'm a large language model, I don't have the capability to directly receive or read files, including PDFs. I can only process text-based input. If you'd like to share the content of the PDF with me, you can try copying and pasting the text into this chat window. If the text is too long, you can break it up into smaller chunks and share them with me sequentially. I'll do my best to assist you.\", type='TextMessage'), UserInputRequestedEvent(source='user_proxy', models_usage=None, request_id='c8e6ef7d-a75f-4bee-ade2-5650170f4a66', content='', type='UserInputRequestedEvent'), TextMessage(source='user_proxy', models_usage=None, content='DONE', type='TextMessage')], stop_reason=\"Text 'DONE' mentioned\")"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install arxiv -q"
      ],
      "metadata": {
        "id": "nHobx9VLR1L5"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import arxiv\n",
        "\n",
        "# Construct the default API client.\n",
        "client = arxiv.Client()\n",
        "\n",
        "# Search for the 10 most recent articles matching the keyword \"quantum.\"\n",
        "search = arxiv.Search(\n",
        "  query = \"Multi-agent LLM systems\",\n",
        "  max_results = 1,\n",
        "  sort_by = arxiv.SortCriterion.SubmittedDate\n",
        ")\n",
        "\n",
        "for result in client.results(search):\n",
        "  print(f\"Entry ID: {result.entry_id}\")\n",
        "  print(f\"\\nTitle: {result.title}\")\n",
        "  print(f\"\\nAuthors: {', '.join([a.name for a in result.authors])}\")\n",
        "  print(f\"\\nPublished: {result.published}\")\n",
        "  print(f\"\\nUpdated: {result.updated}\")\n",
        "  print(f\"\\nSummary: {result.summary}\")\n",
        "  print(f\"\\nComment: {result.comment}\")\n",
        "  print(f\"\\nJournal Reference: {result.journal_ref}\")\n",
        "  print(f\"\\nDOI: {result.doi}\")\n",
        "  print(f\"\\nPrimary Category: {result.primary_category}\")\n",
        "  print(f\"\\nCategories: {result.categories}\")\n",
        "  print(f\"\\nPDF URL: {result.pdf_url}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3--wxChSczw",
        "outputId": "79e3713e-215b-450d-96a7-76f01a30cf93"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entry ID: http://arxiv.org/abs/2502.13145v1\n",
            "\n",
            "Title: Multimodal Mamba: Decoder-only Multimodal State Space Model via Quadratic to Linear Distillation\n",
            "\n",
            "Authors: Bencheng Liao, Hongyuan Tao, Qian Zhang, Tianheng Cheng, Yingyue Li, Haoran Yin, Wenyu Liu, Xinggang Wang\n",
            "\n",
            "Published: 2025-02-18 18:59:57+00:00\n",
            "\n",
            "Updated: 2025-02-18 18:59:57+00:00\n",
            "\n",
            "Summary: Recent Multimodal Large Language Models (MLLMs) have achieved remarkable\n",
            "performance but face deployment challenges due to their quadratic computational\n",
            "complexity, growing Key-Value cache requirements, and reliance on separate\n",
            "vision encoders. We propose mmMamba, a framework for developing\n",
            "linear-complexity native multimodal state space models through progressive\n",
            "distillation from existing MLLMs using moderate academic computational\n",
            "resources. Our approach enables the direct conversion of trained decoder-only\n",
            "MLLMs to linear-complexity architectures without requiring pre-trained\n",
            "RNN-based LLM or vision encoders. We propose an seeding strategy to carve Mamba\n",
            "from trained Transformer and a three-stage distillation recipe, which can\n",
            "effectively transfer the knowledge from Transformer to Mamba while preserving\n",
            "multimodal capabilities. Our method also supports flexible hybrid architectures\n",
            "that combine Transformer and Mamba layers for customizable\n",
            "efficiency-performance trade-offs. Distilled from the Transformer-based\n",
            "decoder-only HoVLE, mmMamba-linear achieves competitive performance against\n",
            "existing linear and quadratic-complexity VLMs, while mmMamba-hybrid further\n",
            "improves performance significantly, approaching HoVLE's capabilities. At 103K\n",
            "tokens, mmMamba-linear demonstrates 20.6$\\times$ speedup and 75.8% GPU memory\n",
            "reduction compared to HoVLE, while mmMamba-hybrid achieves 13.5$\\times$ speedup\n",
            "and 60.2% memory savings. Code and models are released at\n",
            "https://github.com/hustvl/mmMamba\n",
            "\n",
            "Comment: Code and model are available at https://github.com/hustvl/mmMamba\n",
            "\n",
            "Journal Reference: None\n",
            "\n",
            "DOI: None\n",
            "\n",
            "Primary Category: cs.CV\n",
            "\n",
            "Categories: ['cs.CV']\n",
            "\n",
            "PDF URL: http://arxiv.org/pdf/2502.13145v1\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "paper = next(arxiv.Client().results(arxiv.Search(id_list=[\"1605.08386v1\"])))\n",
        "print(paper.get_short_id())"
      ],
      "metadata": {
        "id": "Ep87-jkLTrUK",
        "outputId": "9c179621-79a5-4989-c90c-f2425b81f3a5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1605.08386v1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2 -q"
      ],
      "metadata": {
        "id": "hpCw7DWijOfN",
        "outputId": "40e3ee6e-4dc9-4689-c71b-7846bd891fd9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/232.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.7/232.6 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import PyPDF2\n",
        "import openai\n",
        "\n",
        "# Step 1: Extract text from the PDF\n",
        "pdf_path = \"/content/downloaded-paper.pdf\"  # Adjust the path as needed\n",
        "paper_text = \"\"\n",
        "with open(pdf_path, \"rb\") as f:\n",
        "    reader = PyPDF2.PdfReader(f)\n",
        "    for page in reader.pages:\n",
        "        page_text = page.extract_text()\n",
        "        if page_text:\n",
        "            paper_text += page_text + \"\\n\"\n",
        "\n",
        "# Step 2: Split the text into manageable chunks\n",
        "def chunk_text(text, max_length=2000):\n",
        "    \"\"\"Splits text into chunks of approximately max_length characters.\"\"\"\n",
        "    return [text[i:i+max_length] for i in range(0, len(text), max_length)]\n",
        "\n",
        "\n",
        "chunks = chunk_text(paper_text, max_length=2000)\n"
      ],
      "metadata": {
        "id": "5AunGDZrjK2O"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(chunks[1])"
      ],
      "metadata": {
        "id": "x0vdjyOajg2v",
        "outputId": "606c0a88-d464-4e1a-ac6d-1f29052f9ce9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nderlying set Fcan be\n",
            "Date: September 12, 2018.\n",
            "2010Mathematics Subject Classiﬁcation. Primary: 05C81, Secondary: 37A25, 11P21.\n",
            "Key words and phrases. Heat-bath random walks, sampling, lattice points, Markov b ases.\n",
            "1\n",
            "2 CAPRICE STANLEY AND TOBIAS WINDISCH\n",
            "reached by a random walk that uses moves from M, whereas for the continuous version, a\n",
            "random sampling from the unit sphere suﬃces. However, in man y situations where a Markov\n",
            "basis is known, the heat-bath random walk is evidently fast. For instance, it was shown in [3]\n",
            "that the heat-bath random walk on contingency tables mixes r apidly when the number of\n",
            "columns is ﬁxed. To work around the connectedness issue, a discrete hit-and-run algorithm\n",
            "was introduced in [1] for arbitrary ﬁnite sets F ⊂Zd. At each step in this random walk, a\n",
            "subordinate and unrestricted random walk starts at the curr ent lattice point u∈ Fand uses\n",
            "the unit vectors to collect a set of proposals S⊂Zd. The random walk then moves from u\n",
            "to a random point in S∩F.\n",
            "Random walks of the heat-bath type, such as the one presented above, have been studied\n",
            "recently in [8] in a more general context. In this paper, we ex plore the mixing behaviour of\n",
            "heat-bath random walks on the lattice points of polytopes wi th Markov bases. Throughout,\n",
            "we assume that a Markov basis has been found already and refer to the relevant literature\n",
            "for their computation [24, 25, 11, 17, 10, 21]. We call the und erlying graph of the heat-bath\n",
            "random walk a compressed ﬁber graph (Deﬁnition 2.5) and determine in Section 3bounds on\n",
            "its graph-diameter. We provethat forany A∈Zm×dwithker Z(A)∩Nd={0}, thediameter of\n",
            "compressed ﬁber graphs on {u∈Nd:Au=b}that use a ﬁxed Markov bases M ⊂kerZ(A) is\n",
            "bounded from above by a constant as bvaries (Theorem 3.15). In contrast, we show that the\n",
            "diameterofconventional ﬁbergraphsgrowlinearlyunderad ilationoftheunderlyingpolytope\n",
            "(Remark 3.9). This gives rise to slow mixing results for conventional ﬁb er walks as observed\n",
            "in [27]. In Section 4, we st\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}