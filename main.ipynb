{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PijFkr3QSHnN"
      },
      "source": [
        "## Hello World"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "XN1vC_2YSHnO",
        "outputId": "179d3b26-e7af-4e6e-db6e-d72c4b88721a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hellp world\n"
          ]
        }
      ],
      "source": [
        "print(\"hellp world\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install Requirements ()"
      ],
      "metadata": {
        "id": "N9QpaHdWScCx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install autogen-agentchat autogen-ext[openai] -q\n",
        "!pip install groq -q"
      ],
      "metadata": {
        "id": "Bv3056-vSioY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "tokenGROQ = getpass('Enter GROQ_API_KEY here: ')\n",
        "os.environ[\"GROQ_API_KEY\"] = tokenGROQ"
      ],
      "metadata": {
        "id": "0i8fNwjgR_1Q",
        "outputId": "fd086c41-9ff0-4e61-b223-edaf51d49525",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 52,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter GROQ_API_KEY here: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from autogen_agentchat.agents import AssistantAgent, UserProxyAgent\n",
        "from autogen_agentchat.conditions import TextMentionTermination\n",
        "from autogen_agentchat.teams import RoundRobinGroupChat\n",
        "from autogen_agentchat.ui import Console\n",
        "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
        "\n",
        "# Create the agents.\n",
        "custom_model_client = OpenAIChatCompletionClient(\n",
        "    model=\"llama-3.3-70b-versatile\",\n",
        "    base_url=\"https://api.groq.com/openai/v1\",\n",
        "    api_key=os.environ[\"GROQ_API_KEY\"],\n",
        "    model_info={\n",
        "        \"vision\": False,\n",
        "        \"function_calling\": True,\n",
        "        \"json_output\": False,\n",
        "        \"family\": \"unknown\",\n",
        "    },\n",
        ")\n",
        "assistant = AssistantAgent(\"assistant\", model_client=custom_model_client)\n",
        "user_proxy = UserProxyAgent(\"user_proxy\", input_func=input)  # Use input() to get user input from console.\n",
        "\n",
        "# Create the termination condition which will end the conversation when the user says \"APPROVE\".\n",
        "termination = TextMentionTermination(\"APPROVE\")\n",
        "\n",
        "# Create the team.\n",
        "team = RoundRobinGroupChat([assistant, user_proxy], termination_condition=termination)\n",
        "\n",
        "# Run the conversation and stream to the console.\n",
        "stream = team.run_stream(task=\"Write a 4-line poem about the ocean.\")\n",
        "await Console(stream)"
      ],
      "metadata": {
        "id": "G_wawy2QRzD7",
        "outputId": "e5bee6c1-6e57-4468-99a5-2fbbca560310",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------- user ----------\n",
            "Write a 4-line poem about the ocean.\n",
            "---------- assistant ----------\n",
            "The ocean's waves crash on the shore,\n",
            "A soothing sound that I adore,\n",
            "The salty scent and seaweed's sway,\n",
            "A calming world at the end of the day.\n",
            "\n",
            "TERMINATE\n",
            "Enter your response: APPROVE\n",
            "---------- user_proxy ----------\n",
            "APPROVE\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TaskResult(messages=[TextMessage(source='user', models_usage=None, content='Write a 4-line poem about the ocean.', type='TextMessage'), TextMessage(source='assistant', models_usage=RequestUsage(prompt_tokens=69, completion_tokens=39), content=\"The ocean's waves crash on the shore,\\nA soothing sound that I adore,\\nThe salty scent and seaweed's sway,\\nA calming world at the end of the day.\\n\\nTERMINATE\", type='TextMessage'), UserInputRequestedEvent(source='user_proxy', models_usage=None, request_id='503c1641-bb37-4a4f-ba59-b60ebfb0ff23', content='', type='UserInputRequestedEvent'), TextMessage(source='user_proxy', models_usage=None, content='APPROVE', type='TextMessage')], stop_reason=\"Text 'APPROVE' mentioned\")"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install arxiv -q"
      ],
      "metadata": {
        "id": "nHobx9VLR1L5"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import arxiv\n",
        "\n",
        "# Construct the default API client.\n",
        "client = arxiv.Client()\n",
        "\n",
        "# Search for the 10 most recent articles matching the keyword \"quantum.\"\n",
        "search = arxiv.Search(\n",
        "  query = \"Multi-agent LLM systems\",\n",
        "  max_results = 1,\n",
        "  sort_by = arxiv.SortCriterion.SubmittedDate\n",
        ")\n",
        "\n",
        "for result in client.results(search):\n",
        "  print(f\"Entry ID: {result.entry_id}\")\n",
        "  print(f\"\\nTitle: {result.title}\")\n",
        "  print(f\"\\nAuthors: {', '.join([a.name for a in result.authors])}\")\n",
        "  print(f\"\\nPublished: {result.published}\")\n",
        "  print(f\"\\nUpdated: {result.updated}\")\n",
        "  print(f\"\\nSummary: {result.summary}\")\n",
        "  print(f\"\\nComment: {result.comment}\")\n",
        "  print(f\"\\nJournal Reference: {result.journal_ref}\")\n",
        "  print(f\"\\nDOI: {result.doi}\")\n",
        "  print(f\"\\nPrimary Category: {result.primary_category}\")\n",
        "  print(f\"\\nCategories: {result.categories}\")\n",
        "  print(f\"\\nPDF URL: {result.pdf_url}\\n\")"
      ],
      "metadata": {
        "id": "Q3--wxChSczw",
        "outputId": "79e3713e-215b-450d-96a7-76f01a30cf93",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entry ID: http://arxiv.org/abs/2502.13145v1\n",
            "\n",
            "Title: Multimodal Mamba: Decoder-only Multimodal State Space Model via Quadratic to Linear Distillation\n",
            "\n",
            "Authors: Bencheng Liao, Hongyuan Tao, Qian Zhang, Tianheng Cheng, Yingyue Li, Haoran Yin, Wenyu Liu, Xinggang Wang\n",
            "\n",
            "Published: 2025-02-18 18:59:57+00:00\n",
            "\n",
            "Updated: 2025-02-18 18:59:57+00:00\n",
            "\n",
            "Summary: Recent Multimodal Large Language Models (MLLMs) have achieved remarkable\n",
            "performance but face deployment challenges due to their quadratic computational\n",
            "complexity, growing Key-Value cache requirements, and reliance on separate\n",
            "vision encoders. We propose mmMamba, a framework for developing\n",
            "linear-complexity native multimodal state space models through progressive\n",
            "distillation from existing MLLMs using moderate academic computational\n",
            "resources. Our approach enables the direct conversion of trained decoder-only\n",
            "MLLMs to linear-complexity architectures without requiring pre-trained\n",
            "RNN-based LLM or vision encoders. We propose an seeding strategy to carve Mamba\n",
            "from trained Transformer and a three-stage distillation recipe, which can\n",
            "effectively transfer the knowledge from Transformer to Mamba while preserving\n",
            "multimodal capabilities. Our method also supports flexible hybrid architectures\n",
            "that combine Transformer and Mamba layers for customizable\n",
            "efficiency-performance trade-offs. Distilled from the Transformer-based\n",
            "decoder-only HoVLE, mmMamba-linear achieves competitive performance against\n",
            "existing linear and quadratic-complexity VLMs, while mmMamba-hybrid further\n",
            "improves performance significantly, approaching HoVLE's capabilities. At 103K\n",
            "tokens, mmMamba-linear demonstrates 20.6$\\times$ speedup and 75.8% GPU memory\n",
            "reduction compared to HoVLE, while mmMamba-hybrid achieves 13.5$\\times$ speedup\n",
            "and 60.2% memory savings. Code and models are released at\n",
            "https://github.com/hustvl/mmMamba\n",
            "\n",
            "Comment: Code and model are available at https://github.com/hustvl/mmMamba\n",
            "\n",
            "Journal Reference: None\n",
            "\n",
            "DOI: None\n",
            "\n",
            "Primary Category: cs.CV\n",
            "\n",
            "Categories: ['cs.CV']\n",
            "\n",
            "PDF URL: http://arxiv.org/pdf/2502.13145v1\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}