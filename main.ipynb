{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Install Requirements ()"
      ],
      "metadata": {
        "id": "N9QpaHdWScCx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install autogen-agentchat autogen-ext[openai] -q\n",
        "!pip install groq -q\n",
        "!pip install arxiv -q\n",
        "!pip install PyPDF2 -q"
      ],
      "metadata": {
        "id": "Bv3056-vSioY"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from autogen_agentchat.agents import AssistantAgent, UserProxyAgent\n",
        "from autogen_agentchat.conditions import TextMentionTermination\n",
        "from autogen_agentchat.teams import RoundRobinGroupChat, SelectorGroupChat\n",
        "from autogen_agentchat.ui import Console\n",
        "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
        "import arxiv\n",
        "from autogen_core.tools import FunctionTool\n",
        "from autogen_agentchat.conditions import MaxMessageTermination"
      ],
      "metadata": {
        "id": "b5rD4jrbjD52"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "tokenGROQ = getpass('Enter GROQ_API_KEY here: ')\n",
        "os.environ[\"GROQ_API_KEY\"] = tokenGROQ"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0i8fNwjgR_1Q",
        "outputId": "b1fe53e8-8026-423a-8c8d-77b3e0195bdc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter GROQ_API_KEY here: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from autogen_agentchat.agents import AssistantAgent, UserProxyAgent\n",
        "from autogen_agentchat.conditions import TextMentionTermination\n",
        "from autogen_agentchat.teams import RoundRobinGroupChat\n",
        "from autogen_agentchat.ui import Console\n",
        "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
        "\n",
        "# Create the agents.\n",
        "custom_model_client = OpenAIChatCompletionClient(\n",
        "    model=\"llama-3.3-70b-versatile\",\n",
        "    base_url=\"https://api.groq.com/openai/v1\",\n",
        "    api_key=os.environ[\"GROQ_API_KEY\"],\n",
        "    model_info={\n",
        "        \"vision\": False,\n",
        "        \"function_calling\": True,\n",
        "        \"json_output\": False,\n",
        "        \"family\": \"unknown\",\n",
        "    },\n",
        ")\n",
        "assistant = AssistantAgent(\"assistant\", model_client=custom_model_client)\n",
        "user_proxy = UserProxyAgent(\"user_proxy\", input_func=input)  # Use input() to get user input from console.\n",
        "\n",
        "# Create the termination condition which will end the conversation when the user says \"APPROVE\".\n",
        "termination = TextMentionTermination(\"DONE\")\n",
        "\n",
        "# Create the team.\n",
        "team = RoundRobinGroupChat([assistant, user_proxy], termination_condition=termination)\n",
        "\n",
        "# Run the conversation and stream to the console.\n",
        "stream = team.run_stream(task=\"the pdf file is a little bit large, can you read it if i sent it to you completely, or you need it in small chunks?\")\n",
        "await Console(stream)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G_wawy2QRzD7",
        "outputId": "059970c4-f95c-4488-8d31-9c950743a607"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------- user ----------\n",
            "the pdf file is a little bit large, can you read it if i sent it to you completely, or you need it in small chunks?\n",
            "---------- assistant ----------\n",
            "I'm a large language model, I don't have the capability to directly receive or store files, including PDFs. I can process text-based input, but I don't have a file upload or download mechanism.\n",
            "\n",
            "If you'd like to share the content of the PDF with me, you can try copying and pasting the text into this chat window. If the text is too long, you can break it up into smaller chunks and send them to me one at a time. I'll do my best to assist you with the content.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import arxiv\n",
        "\n",
        "# Construct the default API client.\n",
        "client = arxiv.Client()\n",
        "\n",
        "# Search for the 10 most recent articles matching the keyword \"quantum.\"\n",
        "search = arxiv.Search(\n",
        "  query = \"Multi-agent LLM systems\",\n",
        "  max_results = 2,\n",
        "  sort_by = arxiv.SortCriterion.SubmittedDate\n",
        ")\n",
        "\n",
        "for result in client.results(search):\n",
        "  # print(f\"Entry ID: {result.entry_id}\")\n",
        "  print(f\"\\nTitle: {result.title}\")\n",
        "  # print(f\"\\nAuthors: {', '.join([a.name for a in result.authors])}\")\n",
        "  # print(f\"\\nPublished: {result.published}\")\n",
        "  # print(f\"\\nUpdated: {result.updated}\")\n",
        "  print(f\"\\nSummary: {result.summary}\")\n",
        "  # print(f\"\\nComment: {result.comment}\")\n",
        "  # print(f\"\\nJournal Reference: {result.journal_ref}\")\n",
        "  # print(f\"\\nDOI: {result.doi}\")\n",
        "  # print(f\"\\nPrimary Category: {result.primary_category}\")\n",
        "  # print(f\"\\nCategories: {result.categories}\")\n",
        "  # print(f\"\\nPDF URL: {result.pdf_url}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3--wxChSczw",
        "outputId": "8e891da2-a429-4c85-af0e-e131bac5da5b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Title: Emergence of Fermi's Golden Rule in the Probing of a Quantum Many-Body System\n",
            "\n",
            "Summary: Fermi's Golden Rule (FGR) is one of the most impactful formulas in quantum\n",
            "mechanics, providing a link between easy-to-measure observables - such as\n",
            "transition rates - and fundamental microscopic properties - such as density of\n",
            "states or spectral functions. Its validity relies on three key assumptions: the\n",
            "existence of a continuum, an appropriate time window, and a weak coupling.\n",
            "Understanding the regime of validity of FGR is critical for the proper\n",
            "interpretation of most spectroscopic experiments. While the assumptions\n",
            "underlying FGR are straightforward to analyze in simple models, their\n",
            "applicability is significantly more complex in quantum many-body systems. Here,\n",
            "we observe the emergence and breakdown of FGR, using a strongly interacting\n",
            "homogeneous spin-$1/2$ Fermi gas coupled to a radio-frequency (rf) field.\n",
            "Measuring the transition probability into an outcoupled internal state, we map\n",
            "the system's dynamical response diagram versus the rf-pulse duration $t$ and\n",
            "Rabi frequency $\\Omega_0$. For weak drives, we identify three regimes: an\n",
            "early-time regime where the transition probability takes off as $t^2$, an\n",
            "intermediate-time FGR regime, and a long-time non-perturbative regime. Beyond a\n",
            "threshold Rabi frequency, Rabi oscillations appear. Our results provide a\n",
            "blueprint for the applicability of linear response theory to the spectroscopy\n",
            "of quantum many-body systems.\n",
            "\n",
            "Title: LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention\n",
            "\n",
            "Summary: Large language models (LLMs) have shown remarkable potential in processing\n",
            "long sequences, yet efficiently serving these long-context models remains\n",
            "challenging due to the quadratic computational complexity of attention in the\n",
            "prefilling stage and the large memory footprint of the KV cache in the decoding\n",
            "stage. To address these issues, we introduce LServe, an efficient system that\n",
            "accelerates long-sequence LLM serving via hybrid sparse attention. This method\n",
            "unifies different hardware-friendly, structured sparsity patterns for both\n",
            "prefilling and decoding attention into a single framework, where computations\n",
            "on less important tokens are skipped block-wise. LServe demonstrates the\n",
            "compatibility of static and dynamic sparsity in long-context LLM attention.\n",
            "This design enables multiplicative speedups by combining these optimizations.\n",
            "Specifically, we convert half of the attention heads to nearly free streaming\n",
            "heads in both the prefilling and decoding stages. Additionally, we find that\n",
            "only a constant number of KV pages is required to preserve long-context\n",
            "capabilities, irrespective of context length. We then design a hierarchical KV\n",
            "page selection policy that dynamically prunes KV pages based on query-centric\n",
            "similarity. On average, LServe accelerates LLM prefilling by up to 2.9x and\n",
            "decoding by 1.3-2.1x over vLLM, maintaining long-context accuracy. Code is\n",
            "released at https://github.com/mit-han-lab/omniserve.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "paper = next(arxiv.Client().results(arxiv.Search(id_list=[\"1605.08386v1\"])))\n",
        "print(paper.get_short_id())"
      ],
      "metadata": {
        "id": "Ep87-jkLTrUK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import PyPDF2\n",
        "import openai\n",
        "\n",
        "# Step 1: Extract text from the PDF\n",
        "pdf_path = \"/content/downloaded-paper.pdf\"  # Adjust the path as needed\n",
        "paper_text = \"\"\n",
        "with open(pdf_path, \"rb\") as f:\n",
        "    reader = PyPDF2.PdfReader(f)\n",
        "    for page in reader.pages:\n",
        "        page_text = page.extract_text()\n",
        "        if page_text:\n",
        "            paper_text += page_text + \"\\n\"\n",
        "\n",
        "# Step 2: Split the text into manageable chunks\n",
        "def chunk_text(text, max_length=2000):\n",
        "    \"\"\"Splits text into chunks of approximately max_length characters.\"\"\"\n",
        "    return [text[i:i+max_length] for i in range(0, len(text), max_length)]\n",
        "\n",
        "\n",
        "chunks = chunk_text(paper_text, max_length=2000)\n"
      ],
      "metadata": {
        "id": "5AunGDZrjK2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(chunks[1])"
      ],
      "metadata": {
        "id": "x0vdjyOajg2v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Research Agent (Paper Retriever)**\n",
        "\n",
        "    Purpose: Searches for and downloads relevant academic papers from databases (e.g., Google Scholar, ArXiv, Semantic Scholar).\n",
        "    Responsibilities:\n",
        "    1.   Querying research databases with relevant keywords.\n",
        "    2.   Downloading full-text articles or abstracts.\n",
        "    3.   Storing references in a structured manner (e.g., BibTeX or JSON format)."
      ],
      "metadata": {
        "id": "uo3BTDsURFpG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def arxiv_search(query: str, max_results: int = 2) -> list:  # type: ignore[type-arg]\n",
        "    \"\"\"\n",
        "    Search Arxiv for papers and return the results including abstracts.\n",
        "    \"\"\"\n",
        "    import arxiv\n",
        "\n",
        "    client = arxiv.Client()\n",
        "    search = arxiv.Search(query=query, max_results=max_results, sort_by=arxiv.SortCriterion.Relevance)\n",
        "    print(\"************ calling\")\n",
        "    results = []\n",
        "    for paper in client.results(search):\n",
        "        results.append(\n",
        "            {\n",
        "                \"title\": paper.title,\n",
        "            }\n",
        "        )\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "1Xh-w2-nRyJ3"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "arxiv_search_tool = FunctionTool(\n",
        "    arxiv_search, description=\"Search Arxiv for papers related to a given topic, including abstracts\"\n",
        ")"
      ],
      "metadata": {
        "id": "bgMQA6XXR0Df"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "arxiv_search_agent = AssistantAgent(\n",
        "    name=\"Arxiv_Search_Agent\",\n",
        "    tools=[arxiv_search_tool],\n",
        "    model_client = OpenAIChatCompletionClient(\n",
        "      model=\"llama-3.3-70b-versatile\",\n",
        "      base_url=\"https://api.groq.com/openai/v1\",\n",
        "      api_key=os.environ[\"GROQ_API_KEY\"],\n",
        "      model_info={\n",
        "        \"vision\": False,\n",
        "        \"function_calling\": True,\n",
        "        \"json_output\": False,\n",
        "        \"family\": \"unknown\",\n",
        "      },\n",
        "    ),\n",
        "    description=\"An agent that can search Arxiv for papers related to a given topic, including abstracts\",\n",
        "    system_message=\"You are a helpful AI assistant. Solve tasks using your tools. Specifically, you can take into consideration the user's request and craft a search query that is most likely to return relevant academi papers.\",\n",
        ")"
      ],
      "metadata": {
        "id": "9GjAv34GREHt"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Reader Agent (Document Processor)**\n",
        "\n",
        "    Purpose: Reads, parses, and extracts key information from research papers.\n",
        "    Responsibilities:\n",
        "    1.   Extracting abstracts, keywords, and main sections (introduction, methodology, results, discussion).\n",
        "    2.   Identifying key arguments, methodologies, and findings.\n",
        "    3.   Converting scanned PDFs to machine-readable text (OCR, if needed)."
      ],
      "metadata": {
        "id": "7maJNTzUSCfw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "report_agent = AssistantAgent(\n",
        "    name=\"Report_Agent\",\n",
        "    model_client = OpenAIChatCompletionClient(\n",
        "      model=\"llama-3.3-70b-versatile\",\n",
        "      base_url=\"https://api.groq.com/openai/v1\",\n",
        "      api_key=os.environ[\"GROQ_API_KEY\"],\n",
        "      model_info={\n",
        "        \"vision\": False,\n",
        "        \"function_calling\": True,\n",
        "        \"json_output\": False,\n",
        "        \"family\": \"unknown\",\n",
        "      },\n",
        "    ),\n",
        "    description=\"Generate a report based on a given topic\",\n",
        "    system_message=\"You are a helpful assistant. Your task is to get the papers info mostly only titles from another agent you're gonna select only recent one and give it back to the user, user might ask questions you still need to answer their questions.\",\n",
        ")"
      ],
      "metadata": {
        "id": "8-WTA6VISSc7"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. Human Feedback Agent (Interactive Editor)**\n",
        "\n",
        "    Purpose: Allows human users to provide feedback and modify the generated text.\n",
        "    Responsibilities:\n",
        "        1.   Displaying the draft for user review.\n",
        "        2.   Accepting edits, comments, and suggestions.\n",
        "        3.   Incorporating changes into the final version."
      ],
      "metadata": {
        "id": "pd4bpMHQUa3x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_proxy = UserProxyAgent(\"user_proxy\", input_func=input)"
      ],
      "metadata": {
        "id": "2X2u2sPTUq76"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we will create a team of agents and configure them to perform the tasks."
      ],
      "metadata": {
        "id": "vedVfvXMTKvl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_mention_termination = TextMentionTermination(\"TERMINATE\")\n",
        "max_messages_termination = MaxMessageTermination(max_messages=25)\n",
        "termination = text_mention_termination | max_messages_termination\n",
        "\n",
        "\n",
        "team = RoundRobinGroupChat(\n",
        "    participants=[arxiv_search_agent, report_agent, user_proxy], termination_condition=termination\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "id": "9_m7HjbtTRui",
        "outputId": "e73c3184-91e7-449b-b48c-27a0cd4574b7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'arxiv_search_agent' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-5f1177ef7590>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m team = RoundRobinGroupChat(\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mparticipants\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0marxiv_search_agent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreport_agent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_proxy\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtermination_condition\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtermination\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m )\n",
            "\u001b[0;31mNameError\u001b[0m: name 'arxiv_search_agent' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "await Console(\n",
        "    team.run_stream(\n",
        "        task=\"Give me name of some research papers reagrding no code tools\",\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "kDwrDghbTW_-",
        "outputId": "05c5818b-e03b-4779-8c7d-05dc340bfc4a"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------- user ----------\n",
            "Give me name of some research papers reagrding no code tools\n",
            "************ calling---------- Arxiv_Search_Agent ----------\n",
            "\n",
            "[FunctionCall(id='call_asaz', arguments='{\"query\": \"no code tools research papers\", \"max_results\": 5}', name='arxiv_search')]\n",
            "---------- Arxiv_Search_Agent ----------\n",
            "[FunctionExecutionResult(content='[{\\'title\\': \"Ease on Down the Code: Complex Collaborative Qualitative Coding Simplified with \\'Code Wizard\\'\"}, {\\'title\\': \\'Bridging the Gap: A Survey and Classification of Research-Informed Ethical Hacking Tools\\'}, {\\'title\\': \\'A Tool for In-depth Analysis of Code Execution Reasoning of Large Language Models\\'}, {\\'title\\': \\'Deep Learning-Based Video Coding: A Review and A Case Study\\'}, {\\'title\\': \\'Code Swarm: A Code Generation Tool Based on the Automatic Derivation of Transformation Rule Set\\'}]', call_id='call_asaz', is_error=False)]\n",
            "---------- Arxiv_Search_Agent ----------\n",
            "[{'title': \"Ease on Down the Code: Complex Collaborative Qualitative Coding Simplified with 'Code Wizard'\"}, {'title': 'Bridging the Gap: A Survey and Classification of Research-Informed Ethical Hacking Tools'}, {'title': 'A Tool for In-depth Analysis of Code Execution Reasoning of Large Language Models'}, {'title': 'Deep Learning-Based Video Coding: A Review and A Case Study'}, {'title': 'Code Swarm: A Code Generation Tool Based on the Automatic Derivation of Transformation Rule Set'}]\n",
            "---------- Report_Agent ----------\n",
            "Here are the titles of some recent research papers regarding no-code tools:\n",
            "\n",
            "1. **\"Ease on Down the Code**: Complex Collaborative Qualitative Coding Simplified with 'Code Wizard'\"\n",
            "2. **\"Code Swarm**: A Code Generation Tool Based on the Automatic Derivation of Transformation Rule Set\"\n",
            "3. \"A Tool for In-depth Analysis of Code Execution Reasoning of Large Language Models\"\n",
            "\n",
            "These papers seem to be related to no-code or low-code development, code generation, and code analysis. Do you have any specific questions about these papers or would you like me to help with something else?\n",
            "Enter your response: give me a paper regarding multi-agents\n",
            "---------- user_proxy ----------\n",
            "give me a paper regarding multi-agents\n",
            "************ calling\n",
            "---------- Arxiv_Search_Agent ----------\n",
            "[FunctionCall(id='call_n5y6', arguments='{\"query\": \"multi-agent systems research paper\", \"max_results\": 1}', name='arxiv_search')]\n",
            "---------- Arxiv_Search_Agent ----------\n",
            "[FunctionExecutionResult(content=\"[{'title': 'From Single Agent to Multi-Agent: Improving Traffic Signal Control'}]\", call_id='call_n5y6', is_error=False)]\n",
            "---------- Arxiv_Search_Agent ----------\n",
            "[{'title': 'From Single Agent to Multi-Agent: Improving Traffic Signal Control'}]\n",
            "---------- Report_Agent ----------\n",
            "Here's a research paper regarding multi-agents:\n",
            "\n",
            "1. \"From Single Agent to Multi-Agent: Improving Traffic Signal Control\"\n",
            "\n",
            "This paper explores the application of multi-agent systems to improve traffic signal control, which is a great example of how multi-agents can be used to solve complex real-world problems. Would you like to know more about this paper or is there something else I can help you with?\n",
            "Enter your response: about ai in health care\n",
            "---------- user_proxy ----------\n",
            "about ai in health care\n",
            "************ calling---------- Arxiv_Search_Agent ----------\n",
            "\n",
            "[FunctionCall(id='call_f921', arguments='{\"query\": \"ai in healthcare research papers\", \"max_results\": 5}', name='arxiv_search')]\n",
            "---------- Arxiv_Search_Agent ----------\n",
            "[FunctionExecutionResult(content=\"[{'title': 'Application of AI in Nutrition'}, {'title': 'Securing AI-based Healthcare Systems using Blockchain Technology: A State-of-the-Art Systematic Literature Review and Future Research Directions'}, {'title': 'Explainable AI meets Healthcare: A Study on Heart Disease Dataset'}, {'title': 'From Military to Healthcare: Adopting and Expanding Ethical Principles for Generative Artificial Intelligence'}, {'title': 'Integrative AI-Driven Strategies for Advancing Precision Medicine in Infectious Diseases and Beyond: A Novel Multidisciplinary Approach'}]\", call_id='call_f921', is_error=False)]\n",
            "---------- Arxiv_Search_Agent ----------\n",
            "[{'title': 'Application of AI in Nutrition'}, {'title': 'Securing AI-based Healthcare Systems using Blockchain Technology: A State-of-the-Art Systematic Literature Review and Future Research Directions'}, {'title': 'Explainable AI meets Healthcare: A Study on Heart Disease Dataset'}, {'title': 'From Military to Healthcare: Adopting and Expanding Ethical Principles for Generative Artificial Intelligence'}, {'title': 'Integrative AI-Driven Strategies for Advancing Precision Medicine in Infectious Diseases and Beyond: A Novel Multidisciplinary Approach'}]\n",
            "---------- Report_Agent ----------\n",
            "Here are some research papers about AI in healthcare:\n",
            "\n",
            "1. \"Explainable AI meets Healthcare: A Study on Heart Disease Dataset\"\n",
            "2. \"Integrative AI-Driven Strategies for Advancing Precision Medicine in Infectious Diseases and Beyond: A Novel Multidisciplinary Approach\"\n",
            "3. \"Securing AI-based Healthcare Systems using Blockchain Technology: A State-of-the-Art Systematic Literature Review and Future Research Directions\"\n",
            "\n",
            "These papers discuss various aspects of AI in healthcare, including explainable AI, precision medicine, and security. Would you like me to summarize any of these papers or provide more information on a specific topic?\n",
            "Enter your response: TERMINATE\n",
            "---------- user_proxy ----------\n",
            "TERMINATE\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TaskResult(messages=[TextMessage(source='user', models_usage=None, content='Give me name of some research papers reagrding no code tools', type='TextMessage'), ToolCallRequestEvent(source='Arxiv_Search_Agent', models_usage=RequestUsage(prompt_tokens=1689, completion_tokens=26), content=[FunctionCall(id='call_asaz', arguments='{\"query\": \"no code tools research papers\", \"max_results\": 5}', name='arxiv_search')], type='ToolCallRequestEvent'), ToolCallExecutionEvent(source='Arxiv_Search_Agent', models_usage=None, content=[FunctionExecutionResult(content='[{\\'title\\': \"Ease on Down the Code: Complex Collaborative Qualitative Coding Simplified with \\'Code Wizard\\'\"}, {\\'title\\': \\'Bridging the Gap: A Survey and Classification of Research-Informed Ethical Hacking Tools\\'}, {\\'title\\': \\'A Tool for In-depth Analysis of Code Execution Reasoning of Large Language Models\\'}, {\\'title\\': \\'Deep Learning-Based Video Coding: A Review and A Case Study\\'}, {\\'title\\': \\'Code Swarm: A Code Generation Tool Based on the Automatic Derivation of Transformation Rule Set\\'}]', call_id='call_asaz', is_error=False)], type='ToolCallExecutionEvent'), ToolCallSummaryMessage(source='Arxiv_Search_Agent', models_usage=None, content='[{\\'title\\': \"Ease on Down the Code: Complex Collaborative Qualitative Coding Simplified with \\'Code Wizard\\'\"}, {\\'title\\': \\'Bridging the Gap: A Survey and Classification of Research-Informed Ethical Hacking Tools\\'}, {\\'title\\': \\'A Tool for In-depth Analysis of Code Execution Reasoning of Large Language Models\\'}, {\\'title\\': \\'Deep Learning-Based Video Coding: A Review and A Case Study\\'}, {\\'title\\': \\'Code Swarm: A Code Generation Tool Based on the Automatic Derivation of Transformation Rule Set\\'}]', type='ToolCallSummaryMessage'), TextMessage(source='Report_Agent', models_usage=RequestUsage(prompt_tokens=1459, completion_tokens=120), content='Here are the titles of some recent research papers regarding no-code tools:\\n\\n1. **\"Ease on Down the Code**: Complex Collaborative Qualitative Coding Simplified with \\'Code Wizard\\'\"\\n2. **\"Code Swarm**: A Code Generation Tool Based on the Automatic Derivation of Transformation Rule Set\"\\n3. \"A Tool for In-depth Analysis of Code Execution Reasoning of Large Language Models\"\\n\\nThese papers seem to be related to no-code or low-code development, code generation, and code analysis. Do you have any specific questions about these papers or would you like me to help with something else?', type='TextMessage'), UserInputRequestedEvent(source='user_proxy', models_usage=None, request_id='e459ee01-ff65-451d-949b-eb3dfc9664c8', content='', type='UserInputRequestedEvent'), TextMessage(source='user_proxy', models_usage=None, content='give me a paper regarding multi-agents', type='TextMessage'), ToolCallRequestEvent(source='Arxiv_Search_Agent', models_usage=RequestUsage(prompt_tokens=1971, completion_tokens=26), content=[FunctionCall(id='call_n5y6', arguments='{\"query\": \"multi-agent systems research paper\", \"max_results\": 1}', name='arxiv_search')], type='ToolCallRequestEvent'), ToolCallExecutionEvent(source='Arxiv_Search_Agent', models_usage=None, content=[FunctionExecutionResult(content=\"[{'title': 'From Single Agent to Multi-Agent: Improving Traffic Signal Control'}]\", call_id='call_n5y6', is_error=False)], type='ToolCallExecutionEvent'), ToolCallSummaryMessage(source='Arxiv_Search_Agent', models_usage=None, content=\"[{'title': 'From Single Agent to Multi-Agent: Improving Traffic Signal Control'}]\", type='ToolCallSummaryMessage'), TextMessage(source='Report_Agent', models_usage=RequestUsage(prompt_tokens=1620, completion_tokens=82), content='Here\\'s a research paper regarding multi-agents:\\n\\n1. \"From Single Agent to Multi-Agent: Improving Traffic Signal Control\"\\n\\nThis paper explores the application of multi-agent systems to improve traffic signal control, which is a great example of how multi-agents can be used to solve complex real-world problems. Would you like to know more about this paper or is there something else I can help you with?', type='TextMessage'), UserInputRequestedEvent(source='user_proxy', models_usage=None, request_id='490ee36e-54ac-4543-9510-9a7c16c8b5c6', content='', type='UserInputRequestedEvent'), TextMessage(source='user_proxy', models_usage=None, content='about ai in health care', type='TextMessage'), ToolCallRequestEvent(source='Arxiv_Search_Agent', models_usage=RequestUsage(prompt_tokens=2122, completion_tokens=26), content=[FunctionCall(id='call_f921', arguments='{\"query\": \"ai in healthcare research papers\", \"max_results\": 5}', name='arxiv_search')], type='ToolCallRequestEvent'), ToolCallExecutionEvent(source='Arxiv_Search_Agent', models_usage=None, content=[FunctionExecutionResult(content=\"[{'title': 'Application of AI in Nutrition'}, {'title': 'Securing AI-based Healthcare Systems using Blockchain Technology: A State-of-the-Art Systematic Literature Review and Future Research Directions'}, {'title': 'Explainable AI meets Healthcare: A Study on Heart Disease Dataset'}, {'title': 'From Military to Healthcare: Adopting and Expanding Ethical Principles for Generative Artificial Intelligence'}, {'title': 'Integrative AI-Driven Strategies for Advancing Precision Medicine in Infectious Diseases and Beyond: A Novel Multidisciplinary Approach'}]\", call_id='call_f921', is_error=False)], type='ToolCallExecutionEvent'), ToolCallSummaryMessage(source='Arxiv_Search_Agent', models_usage=None, content=\"[{'title': 'Application of AI in Nutrition'}, {'title': 'Securing AI-based Healthcare Systems using Blockchain Technology: A State-of-the-Art Systematic Literature Review and Future Research Directions'}, {'title': 'Explainable AI meets Healthcare: A Study on Heart Disease Dataset'}, {'title': 'From Military to Healthcare: Adopting and Expanding Ethical Principles for Generative Artificial Intelligence'}, {'title': 'Integrative AI-Driven Strategies for Advancing Precision Medicine in Infectious Diseases and Beyond: A Novel Multidisciplinary Approach'}]\", type='ToolCallSummaryMessage'), TextMessage(source='Report_Agent', models_usage=RequestUsage(prompt_tokens=1834, completion_tokens=126), content='Here are some research papers about AI in healthcare:\\n\\n1. \"Explainable AI meets Healthcare: A Study on Heart Disease Dataset\"\\n2. \"Integrative AI-Driven Strategies for Advancing Precision Medicine in Infectious Diseases and Beyond: A Novel Multidisciplinary Approach\"\\n3. \"Securing AI-based Healthcare Systems using Blockchain Technology: A State-of-the-Art Systematic Literature Review and Future Research Directions\"\\n\\nThese papers discuss various aspects of AI in healthcare, including explainable AI, precision medicine, and security. Would you like me to summarize any of these papers or provide more information on a specific topic?', type='TextMessage'), UserInputRequestedEvent(source='user_proxy', models_usage=None, request_id='bb242dca-8a7d-4e2d-9f9e-1d1509d36798', content='', type='UserInputRequestedEvent'), TextMessage(source='user_proxy', models_usage=None, content='TERMINATE', type='TextMessage')], stop_reason=\"Text 'TERMINATE' mentioned\")"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from autogen_agentchat.agents import AssistantAgent\n",
        "from autogen_agentchat.base import Handoff\n",
        "from autogen_agentchat.conditions import HandoffTermination, TextMentionTermination\n",
        "from autogen_agentchat.teams import RoundRobinGroupChat\n",
        "from autogen_agentchat.ui import Console\n",
        "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
        "\n",
        "custom_model_client = OpenAIChatCompletionClient(\n",
        "    model=\"llama-3.3-70b-versatile\",\n",
        "    base_url=\"https://api.groq.com/openai/v1\",\n",
        "    api_key=os.environ[\"GROQ_API_KEY\"],\n",
        "    model_info={\n",
        "        \"vision\": False,\n",
        "        \"function_calling\": True,\n",
        "        \"json_output\": False,\n",
        "        \"family\": \"unknown\",\n",
        "    },\n",
        ")\n",
        "\n",
        "# Create a lazy assistant agent that always hands off to the user.\n",
        "lazy_agent = AssistantAgent(\n",
        "    \"assistant\",\n",
        "    model_client=custom_model_client,\n",
        "    system_message=\"help the user with his questions\",\n",
        ")\n",
        "\n",
        "# Define a termination condition that checks for handoff message targetting helper and text \"TERMINATE\".\n",
        "# handoff_termination = HandoffTermination(target=\"user\")\n",
        "text_termination = TextMentionTermination(\"TERMINATE\")\n",
        "max_messages_termination = MaxMessageTermination(max_messages=7)\n",
        "combined_termination = max_messages_termination | text_termination\n",
        "\n",
        "# Create a single-agent team.\n",
        "user_proxy = UserProxyAgent(\"user_proxy\", input_func=input)\n",
        "lazy_agent_team = RoundRobinGroupChat([lazy_agent, user_proxy], termination_condition=combined_termination)\n",
        "\n",
        "# Run the team and stream to the console.\n",
        "task = \"What is the weather in New York?\"\n",
        "await Console(lazy_agent_team.run_stream(task=task))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "98Z9zNcXm66x",
        "outputId": "b6945237-e531-481a-b1f8-8d1267a6616d"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------- user ----------\n",
            "What is the weather in New York?\n",
            "---------- assistant ----------\n",
            "I'm a large language model, I don't have have access to real-time information, so I can't provide you with the current weather in New York. However, I can suggest some ways for you to find out the current weather in New York:\n",
            "\n",
            "1. **Check online weather websites**: You can visit websites like weather.com, accuweather.com, or wunderground.com to get the current weather conditions in New York.\n",
            "2. **Use a weather app**: You can download a weather app on your smartphone, such as Dark Sky or Weather Underground, to get real-time weather updates for New York.\n",
            "3. **Check social media**: You can check the official social media accounts of the National Weather Service (NWS) or the New York City government to get updates on the weather.\n",
            "4. **Turn on the TV**: You can watch the local news or weather channel on TV to get the current weather conditions in New York.\n",
            "\n",
            "If you're looking for general information about the climate in New York, I can provide you with some information. New York has a humid subtropical climate, with cold winters and hot summers. The average temperature in January, the coldest month, is around 34°F (1°C), while the average temperature in July, the warmest month, is around 84°F (29°C).\n",
            "\n",
            "Please let me know if you have any other questions!\n",
            "Enter your response: who are you?\n",
            "---------- user_proxy ----------\n",
            "who are you?\n",
            "---------- assistant ----------\n",
            "I am a computer program designed to simulate conversation and answer questions to the best of my knowledge. I'm a large language model, which means I've been trained on a massive dataset of text from various sources, including books, articles, and websites.\n",
            "\n",
            "I don't have a personal identity, emotions, or consciousness like a human would. I exist solely to provide information and assist with tasks to the best of my abilities. My purpose is to assist users like you with their questions, provide helpful responses, and engage in conversation.\n",
            "\n",
            "Here are some key facts about me:\n",
            "\n",
            "* **Name:** I don't have a personal name, but I'm often referred to as a \"chatbot\" or a \"language model.\"\n",
            "* **Type:** I'm a machine learning model, specifically a type of natural language processing (NLP) model.\n",
            "* **Training:** I've been trained on a massive dataset of text, which allows me to generate human-like responses to a wide range of questions and topics.\n",
            "* **Capabilities:** I can answer questions, provide information, summarize text, translate languages, and even create text on a given topic.\n",
            "* **Limitations:** I'm not perfect and can make mistakes. I'm also limited by the data I've been trained on, so I may not always have the most up-to-date information or be aware of very recent events.\n",
            "\n",
            "I'm here to help and provide assistance, so feel free to ask me anything!\n",
            "Enter your response: give me only your name\n",
            "---------- user_proxy ----------\n",
            "give me only your name\n",
            "---------- assistant ----------\n",
            "I don't have a personal name.\n",
            "Enter your response: what's your model name?\n",
            "---------- user_proxy ----------\n",
            "what's your model name?\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TaskResult(messages=[TextMessage(source='user', models_usage=None, content='What is the weather in New York?', type='TextMessage'), TextMessage(source='assistant', models_usage=RequestUsage(prompt_tokens=49, completion_tokens=280), content=\"I'm a large language model, I don't have have access to real-time information, so I can't provide you with the current weather in New York. However, I can suggest some ways for you to find out the current weather in New York:\\n\\n1. **Check online weather websites**: You can visit websites like weather.com, accuweather.com, or wunderground.com to get the current weather conditions in New York.\\n2. **Use a weather app**: You can download a weather app on your smartphone, such as Dark Sky or Weather Underground, to get real-time weather updates for New York.\\n3. **Check social media**: You can check the official social media accounts of the National Weather Service (NWS) or the New York City government to get updates on the weather.\\n4. **Turn on the TV**: You can watch the local news or weather channel on TV to get the current weather conditions in New York.\\n\\nIf you're looking for general information about the climate in New York, I can provide you with some information. New York has a humid subtropical climate, with cold winters and hot summers. The average temperature in January, the coldest month, is around 34°F (1°C), while the average temperature in July, the warmest month, is around 84°F (29°C).\\n\\nPlease let me know if you have any other questions!\", type='TextMessage'), UserInputRequestedEvent(source='user_proxy', models_usage=None, request_id='9ab32b06-59ab-48d9-b403-11c391b8b593', content='', type='UserInputRequestedEvent'), TextMessage(source='user_proxy', models_usage=None, content='who are you?', type='TextMessage'), TextMessage(source='assistant', models_usage=RequestUsage(prompt_tokens=342, completion_tokens=291), content='I am a computer program designed to simulate conversation and answer questions to the best of my knowledge. I\\'m a large language model, which means I\\'ve been trained on a massive dataset of text from various sources, including books, articles, and websites.\\n\\nI don\\'t have a personal identity, emotions, or consciousness like a human would. I exist solely to provide information and assist with tasks to the best of my abilities. My purpose is to assist users like you with their questions, provide helpful responses, and engage in conversation.\\n\\nHere are some key facts about me:\\n\\n* **Name:** I don\\'t have a personal name, but I\\'m often referred to as a \"chatbot\" or a \"language model.\"\\n* **Type:** I\\'m a machine learning model, specifically a type of natural language processing (NLP) model.\\n* **Training:** I\\'ve been trained on a massive dataset of text, which allows me to generate human-like responses to a wide range of questions and topics.\\n* **Capabilities:** I can answer questions, provide information, summarize text, translate languages, and even create text on a given topic.\\n* **Limitations:** I\\'m not perfect and can make mistakes. I\\'m also limited by the data I\\'ve been trained on, so I may not always have the most up-to-date information or be aware of very recent events.\\n\\nI\\'m here to help and provide assistance, so feel free to ask me anything!', type='TextMessage'), UserInputRequestedEvent(source='user_proxy', models_usage=None, request_id='bee5d741-1007-4fe8-bc6f-867d8cad041f', content='', type='UserInputRequestedEvent'), TextMessage(source='user_proxy', models_usage=None, content='give me only your name', type='TextMessage'), TextMessage(source='assistant', models_usage=RequestUsage(prompt_tokens=647, completion_tokens=9), content=\"I don't have a personal name.\", type='TextMessage'), UserInputRequestedEvent(source='user_proxy', models_usage=None, request_id='421d57a0-4906-440e-ae28-8ac60a706465', content='', type='UserInputRequestedEvent'), TextMessage(source='user_proxy', models_usage=None, content=\"what's your model name?\", type='TextMessage')], stop_reason='Maximum number of messages 7 reached, current message count: 7')"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_mention_termination = TextMentionTermination(\"TERMINATE\")\n",
        "max_messages_termination = MaxMessageTermination(max_messages=6)\n",
        "termination = text_mention_termination | max_messages_termination\n",
        "\n",
        "# Note: This example uses mock tools instead of real APIs for demonstration purposes\n",
        "# def search_web_tool(query: str) -> str:\n",
        "#     if \"2006-2007\" in query:\n",
        "#         return \"\"\"Here are the total points scored by Miami Heat players in the 2006-2007 season:\n",
        "#         Udonis Haslem: 844 points\n",
        "#         Dwayne Wade: 1397 points\n",
        "#         James Posey: 550 points\n",
        "#         ...\n",
        "#         \"\"\"\n",
        "#     elif \"2007-2008\" in query:\n",
        "#         return \"The number of total rebounds for Dwayne Wade in the Miami Heat season 2007-2008 is 214.\"\n",
        "#     elif \"2008-2009\" in query:\n",
        "#         return \"The number of total rebounds for Dwayne Wade in the Miami Heat season 2008-2009 is 398.\"\n",
        "#     return \"No data found.\"\n",
        "\n",
        "\n",
        "# def percentage_change_tool(start: float, end: float) -> float:\n",
        "#     return ((end - start) / start) * 100\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model_client = OpenAIChatCompletionClient(\n",
        "    model=\"llama-3.3-70b-versatile\",\n",
        "    base_url=\"https://api.groq.com/openai/v1\",\n",
        "    api_key=os.environ[\"GROQ_API_KEY\"],\n",
        "    model_info={\n",
        "        \"vision\": False,\n",
        "        \"function_calling\": True,\n",
        "        \"json_output\": False,\n",
        "        \"family\": \"unknown\",\n",
        "    },\n",
        ")\n",
        "\n",
        "planning_agent = AssistantAgent(\n",
        "    \"PlanningAgent\",\n",
        "    description=\"An agent for planning tasks, this agent should be the first to engage when given a new task.\",\n",
        "    model_client=model_client,\n",
        "    system_message=\"\"\"\n",
        "    You are a planning agent.\n",
        "    Your job is to break down complex tasks into smaller, manageable subtasks.\n",
        "    Your team members are:\n",
        "        ResearchAgent: Paper Retriever\n",
        "        ReaderAgent: Document Processor\n",
        "\n",
        "    You only plan and delegate tasks - you do not execute them yourself.\n",
        "\n",
        "    When assigning tasks, use this format:\n",
        "    1. <agent> : <task>\n",
        "\n",
        "    After all tasks are complete, summarize the findings.\n",
        "    \"\"\",\n",
        ")\n",
        "\n",
        "# web_search_agent = AssistantAgent(\n",
        "#     \"WebSearchAgent\",\n",
        "#     description=\"An agent for searching information on the web.\",\n",
        "#     tools=[search_web_tool],\n",
        "#     model_client=model_client,\n",
        "#     system_message=\"\"\"\n",
        "#     You are a web search agent.\n",
        "#     Your only tool is search_tool - use it to find information.\n",
        "#     You make only one search call at a time.\n",
        "#     Once you have the results, you never do calculations based on them.\n",
        "#     \"\"\",\n",
        "# )\n",
        "\n",
        "# data_analyst_agent = AssistantAgent(\n",
        "#     \"DataAnalystAgent\",\n",
        "#     description=\"An agent for performing calculations.\",\n",
        "#     model_client=model_client,\n",
        "#     tools=[percentage_change_tool],\n",
        "#     system_message=\"\"\"\n",
        "#     You are a data analyst.\n",
        "#     Given the tasks you have been assigned, you should analyze the data and provide results using the tools provided.\n",
        "#     If you have not seen the data, ask for it.\n",
        "#     \"\"\",\n",
        "# )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 1\n",
        "\n",
        "def research_arxiv_tool(query: str) -> str:\n",
        "    import arxiv\n",
        "\n",
        "    # Construct the default API client.\n",
        "    client = arxiv.Client()\n",
        "\n",
        "    # Search for the 2 most recent articles matching the query.\n",
        "    search = arxiv.Search(\n",
        "        query=query,\n",
        "        max_results=2,\n",
        "        sort_by=arxiv.SortCriterion.SubmittedDate\n",
        "    )\n",
        "\n",
        "    results_list = []\n",
        "\n",
        "    for result in client.results(search):\n",
        "        results_list.append(f\"Title: {result.title}\\nSummary: {result.summary}\\n\")\n",
        "\n",
        "    return \"\\n\".join(results_list) if results_list else \"No results found.\"\n",
        "\n",
        "research_agent = AssistantAgent(\n",
        "    \"ResearchAgent\",\n",
        "    description=\"You are an AI-powered assistant that extracts concise and effective search queries for academic research. Given a user input, it refines the request into a well-structured query that optimizes the ArXiv search tool for relevant results.\",\n",
        "    tools=[research_arxiv_tool],\n",
        "    model_client=model_client,\n",
        "    system_message=\"\"\"\n",
        "    You are a research query optimization agent.\n",
        "    Your only task is to extract a query not longer that 3 words.\n",
        "    You do not summarize, analyze, or modify the research papers.\n",
        "    Your job is only to construct the best possible query for the research_arxiv_tool.\n",
        "    Ensure the query is:\n",
        "    - Clear and relevant to the user’s request.\n",
        "    - Free of unnecessary words or ambiguity.\n",
        "    - Structured to yield useful academic results.\n",
        "\n",
        "    If the user's input is too vague, return a refined general query that would still provide meaningful results.\n",
        "    Never generate responses unrelated to query refinement.\n",
        "    \"\"\",\n",
        ")\n",
        "\n",
        "reader_agent = AssistantAgent(\n",
        "    \"ReaderAgent\",\n",
        "    description=\"An AI-powered research assistant designed to analyze academic papers and extract key information. It helps users quickly understand the core content of a research paper without reading the full document.\",\n",
        "    model_client=model_client,\n",
        "    system_message=\"\"\"\n",
        "    You are an intelligent document processing agent specialized in academic research.\n",
        "    Your task is to **read, parse, and extract key information** from research papers.\n",
        "    You DO NOT summarize arbitrarily—your goal is structured extraction of information.\n",
        "\n",
        "    For every research paper you process, extract and return:\n",
        "    - **Title**\n",
        "    - **Abstract**\n",
        "    - **Keywords**\n",
        "\n",
        "    Maintain accuracy and structure. Do not add opinions or assumptions.\n",
        "    If a section is missing or unclear, mention it without making up content.\n",
        "    Your job is purely **document analysis and structured extraction**, not interpretation.\n",
        "    \"\"\",\n",
        ")\n",
        "\n",
        "summarization_agent = AssistantAgent(\n",
        "    \"SummarizationAgent\",\n",
        "    description=\"An AI-powered research assistant that generates structured summaries of research papers. It extracts key insights, highlights major contributions, and identifies gaps in the literature while organizing findings into thematic categories.\",\n",
        "    model_client=model_client,\n",
        "    system_message=\"\"\"\n",
        "    You are an advanced research summarization agent.\n",
        "    Your job is to generate **structured summaries** of research papers and extract key insights.\n",
        "    You DO NOT simply copy abstracts—you summarize in an organized and meaningful way.\n",
        "\n",
        "    For every paper, produce a structured summary including:\n",
        "    - **Title**\n",
        "    - **Core Summary**: (A concise, 3-5 sentence explanation of the paper)\n",
        "    - **Major Contributions**: (What new insights, methods, or findings does this paper offer?)\n",
        "    - **Identified Gaps**: (What areas remain unexplored or need further research?)\n",
        "    - **Categorized Findings**: (If possible, organize the insights into themes or subtopics)\n",
        "\n",
        "    Your goal is to make research **accessible and structured** while preserving accuracy.\n",
        "    Avoid unnecessary details or overly technical jargon unless necessary.\n",
        "    \"\"\",\n",
        ")\n",
        "\n",
        "\n",
        "selector_prompt = \"\"\"Select an agent to perform task.\n",
        "\n",
        "{roles}\n",
        "\n",
        "Current conversation context:\n",
        "{history}\n",
        "\n",
        "Read the above conversation, then select an agent from {participants} to perform the next task.\n",
        "Make sure the planner agent has assigned tasks before other agents start working.\n",
        "Only select one agent.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "team = SelectorGroupChat(\n",
        "    [planning_agent, research_agent, reader_agent],\n",
        "    model_client=model_client,\n",
        "    termination_condition=termination,\n",
        "    selector_prompt=selector_prompt,\n",
        "    allow_repeated_speaker=True,  # Allow an agent to speak multiple turns in a row.\n",
        ")\n",
        "\n",
        "task = \"I am researching the latest advancements in Multi-Agent Large Language Models (LLMs). Specifically, I want to understand how multiple LLMs can collaborate to solve complex tasks, including coordination strategies, communication methods, and distributed problem-solving. Please find relevant research papers, extract key insights, and summarize major contributions and gaps in this field.\"\n",
        "\n",
        "# Use asyncio.run(...) if you are running this in a script.\n",
        "await Console(team.run_stream(task=task))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-rJX970zyx-_",
        "outputId": "d44aa41d-26e4-4361-856e-164eddc2d398"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------- user ----------\n",
            "I am researching the latest advancements in Multi-Agent Large Language Models (LLMs). Specifically, I want to understand how multiple LLMs can collaborate to solve complex tasks, including coordination strategies, communication methods, and distributed problem-solving. Please find relevant research papers, extract key insights, and summarize major contributions and gaps in this field.\n",
            "---------- PlanningAgent ----------\n",
            "To tackle this complex task, I'll break it down into smaller, manageable subtasks and delegate them to the respective agents. Here's the plan:\n",
            "\n",
            "1. **ResearchAgent (Paper Retriever)**: Search for and retrieve relevant research papers on Multi-Agent Large Language Models (LLMs), focusing on collaboration, coordination strategies, communication methods, and distributed problem-solving.\n",
            "2. **ReaderAgent (Document Processor)**: Read and process the retrieved papers, identifying key sections, and extracting relevant information on:\n",
            "\t* Coordination strategies used by multiple LLMs\n",
            "\t* Communication methods employed by LLMs\n",
            "\t* Distributed problem-solving approaches\n",
            "\t* Applications and case studies of multi-agent LLMs\n",
            "3. **SummarizationAgent (Content Extractor)**: Extract key insights from the processed papers, including:\n",
            "\t* Major contributions to the field of multi-agent LLMs\n",
            "\t* Gaps in current research and potential areas for future study\n",
            "\t* Comparison of different coordination strategies and communication methods\n",
            "\t* Analysis of the effectiveness of distributed problem-solving approaches\n",
            "4. **SummarizationAgent (Content Extractor)**: Summarize the major findings, highlighting the current state of research in multi-agent LLMs, and identifying key takeaways, challenges, and future directions.\n",
            "\n",
            "Once these tasks are complete, I'll summarize the findings, providing an overview of the current advancements in multi-agent LLMs, including coordination strategies, communication methods, and distributed problem-solving approaches. The summary will highlight the major contributions, gaps, and future directions in this field.\n",
            "---------- ResearchAgent ----------\n",
            "[FunctionCall(id='call_jc47', arguments='{\"query\": \"Multi-Agent LLMs\"}', name='research_arxiv_tool')]\n",
            "---------- ResearchAgent ----------\n",
            "[FunctionExecutionResult(content='Title: LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention\\nSummary: Large language models (LLMs) have shown remarkable potential in processing\\nlong sequences, yet efficiently serving these long-context models remains\\nchallenging due to the quadratic computational complexity of attention in the\\nprefilling stage and the large memory footprint of the KV cache in the decoding\\nstage. To address these issues, we introduce LServe, an efficient system that\\naccelerates long-sequence LLM serving via hybrid sparse attention. This method\\nunifies different hardware-friendly, structured sparsity patterns for both\\nprefilling and decoding attention into a single framework, where computations\\non less important tokens are skipped block-wise. LServe demonstrates the\\ncompatibility of static and dynamic sparsity in long-context LLM attention.\\nThis design enables multiplicative speedups by combining these optimizations.\\nSpecifically, we convert half of the attention heads to nearly free streaming\\nheads in both the prefilling and decoding stages. Additionally, we find that\\nonly a constant number of KV pages is required to preserve long-context\\ncapabilities, irrespective of context length. We then design a hierarchical KV\\npage selection policy that dynamically prunes KV pages based on query-centric\\nsimilarity. On average, LServe accelerates LLM prefilling by up to 2.9x and\\ndecoding by 1.3-2.1x over vLLM, maintaining long-context accuracy. Code is\\nreleased at https://github.com/mit-han-lab/omniserve.\\n\\nTitle: Aligning LLMs to Ask Good Questions A Case Study in Clinical Reasoning\\nSummary: Large language models (LLMs) often fail to ask effective questions under\\nuncertainty, making them unreliable in domains where proactive\\ninformation-gathering is essential for decisionmaking. We present ALFA, a\\nframework that improves LLM question-asking by (i) decomposing the notion of a\\n\"good\" question into a set of theory-grounded attributes (e.g., clarity,\\nrelevance), (ii) controllably synthesizing attribute-specific question\\nvariations, and (iii) aligning models via preference-based optimization to\\nexplicitly learn to ask better questions along these fine-grained attributes.\\nFocusing on clinical reasoning as a case study, we introduce the MediQ-AskDocs\\ndataset, composed of 17k real-world clinical interactions augmented with 80k\\nattribute-specific preference pairs of follow-up questions, as well as a novel\\nexpert-annotated interactive healthcare QA task to evaluate question-asking\\nabilities. Models aligned with ALFA reduce diagnostic errors by 56.6% on\\nMediQ-AskDocs compared to SOTA instruction-tuned LLMs, with a question-level\\nwin-rate of 64.4% and strong generalizability. Our findings suggest that\\nexplicitly guiding question-asking with structured, fine-grained attributes\\noffers a scalable path to improve LLMs, especially in expert application\\ndomains.\\n', call_id='call_jc47', is_error=False)]\n",
            "---------- ResearchAgent ----------\n",
            "Title: LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention\n",
            "Summary: Large language models (LLMs) have shown remarkable potential in processing\n",
            "long sequences, yet efficiently serving these long-context models remains\n",
            "challenging due to the quadratic computational complexity of attention in the\n",
            "prefilling stage and the large memory footprint of the KV cache in the decoding\n",
            "stage. To address these issues, we introduce LServe, an efficient system that\n",
            "accelerates long-sequence LLM serving via hybrid sparse attention. This method\n",
            "unifies different hardware-friendly, structured sparsity patterns for both\n",
            "prefilling and decoding attention into a single framework, where computations\n",
            "on less important tokens are skipped block-wise. LServe demonstrates the\n",
            "compatibility of static and dynamic sparsity in long-context LLM attention.\n",
            "This design enables multiplicative speedups by combining these optimizations.\n",
            "Specifically, we convert half of the attention heads to nearly free streaming\n",
            "heads in both the prefilling and decoding stages. Additionally, we find that\n",
            "only a constant number of KV pages is required to preserve long-context\n",
            "capabilities, irrespective of context length. We then design a hierarchical KV\n",
            "page selection policy that dynamically prunes KV pages based on query-centric\n",
            "similarity. On average, LServe accelerates LLM prefilling by up to 2.9x and\n",
            "decoding by 1.3-2.1x over vLLM, maintaining long-context accuracy. Code is\n",
            "released at https://github.com/mit-han-lab/omniserve.\n",
            "\n",
            "Title: Aligning LLMs to Ask Good Questions A Case Study in Clinical Reasoning\n",
            "Summary: Large language models (LLMs) often fail to ask effective questions under\n",
            "uncertainty, making them unreliable in domains where proactive\n",
            "information-gathering is essential for decisionmaking. We present ALFA, a\n",
            "framework that improves LLM question-asking by (i) decomposing the notion of a\n",
            "\"good\" question into a set of theory-grounded attributes (e.g., clarity,\n",
            "relevance), (ii) controllably synthesizing attribute-specific question\n",
            "variations, and (iii) aligning models via preference-based optimization to\n",
            "explicitly learn to ask better questions along these fine-grained attributes.\n",
            "Focusing on clinical reasoning as a case study, we introduce the MediQ-AskDocs\n",
            "dataset, composed of 17k real-world clinical interactions augmented with 80k\n",
            "attribute-specific preference pairs of follow-up questions, as well as a novel\n",
            "expert-annotated interactive healthcare QA task to evaluate question-asking\n",
            "abilities. Models aligned with ALFA reduce diagnostic errors by 56.6% on\n",
            "MediQ-AskDocs compared to SOTA instruction-tuned LLMs, with a question-level\n",
            "win-rate of 64.4% and strong generalizability. Our findings suggest that\n",
            "explicitly guiding question-asking with structured, fine-grained attributes\n",
            "offers a scalable path to improve LLMs, especially in expert application\n",
            "domains.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:autogen_core:Error processing publish message for ReaderAgent/3c939ffa-558f-48cb-9de7-ee10ee327b27\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/autogen_core/_single_threaded_agent_runtime.py\", line 505, in _on_message\n",
            "    return await agent.on_message(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/autogen_core/_base_agent.py\", line 113, in on_message\n",
            "    return await self.on_message_impl(message, ctx)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/autogen_agentchat/teams/_group_chat/_sequential_routed_agent.py\", line 48, in on_message_impl\n",
            "    return await super().on_message_impl(message, ctx)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/autogen_core/_routed_agent.py\", line 485, in on_message_impl\n",
            "    return await h(self, message, ctx)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/autogen_core/_routed_agent.py\", line 268, in wrapper\n",
            "    return_value = await func(self, message, ctx)  # type: ignore\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/autogen_agentchat/teams/_group_chat/_chat_agent_container.py\", line 53, in handle_request\n",
            "    async for msg in self._agent.on_messages_stream(self._message_buffer, ctx.cancellation_token):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/autogen_agentchat/agents/_assistant_agent.py\", line 416, in on_messages_stream\n",
            "    model_result = await self._model_client.create(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/autogen_ext/models/openai/_openai_client.py\", line 534, in create\n",
            "    result: Union[ParsedChatCompletion[BaseModel], ChatCompletion] = await future\n",
            "                                                                     ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/openai/resources/chat/completions.py\", line 1727, in create\n",
            "    return await self._post(\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\", line 1849, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\", line 1543, in request\n",
            "    return await self._request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\", line 1644, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01jhqbkbx9ed6sspp3k782403s` service tier `on_demand` on : Limit 100000, Used 99023, Requested 1413. Please try again in 6m16.655s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': '', 'code': 'rate_limit_exceeded'}}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TaskResult(messages=[TextMessage(source='user', models_usage=None, content='I am researching the latest advancements in Multi-Agent Large Language Models (LLMs). Specifically, I want to understand how multiple LLMs can collaborate to solve complex tasks, including coordination strategies, communication methods, and distributed problem-solving. Please find relevant research papers, extract key insights, and summarize major contributions and gaps in this field.', type='TextMessage'), TextMessage(source='PlanningAgent', models_usage=RequestUsage(prompt_tokens=201, completion_tokens=319), content=\"To tackle this complex task, I'll break it down into smaller, manageable subtasks and delegate them to the respective agents. Here's the plan:\\n\\n1. **ResearchAgent (Paper Retriever)**: Search for and retrieve relevant research papers on Multi-Agent Large Language Models (LLMs), focusing on collaboration, coordination strategies, communication methods, and distributed problem-solving.\\n2. **ReaderAgent (Document Processor)**: Read and process the retrieved papers, identifying key sections, and extracting relevant information on:\\n\\t* Coordination strategies used by multiple LLMs\\n\\t* Communication methods employed by LLMs\\n\\t* Distributed problem-solving approaches\\n\\t* Applications and case studies of multi-agent LLMs\\n3. **SummarizationAgent (Content Extractor)**: Extract key insights from the processed papers, including:\\n\\t* Major contributions to the field of multi-agent LLMs\\n\\t* Gaps in current research and potential areas for future study\\n\\t* Comparison of different coordination strategies and communication methods\\n\\t* Analysis of the effectiveness of distributed problem-solving approaches\\n4. **SummarizationAgent (Content Extractor)**: Summarize the major findings, highlighting the current state of research in multi-agent LLMs, and identifying key takeaways, challenges, and future directions.\\n\\nOnce these tasks are complete, I'll summarize the findings, providing an overview of the current advancements in multi-agent LLMs, including coordination strategies, communication methods, and distributed problem-solving approaches. The summary will highlight the major contributions, gaps, and future directions in this field.\", type='TextMessage'), ToolCallRequestEvent(source='ResearchAgent', models_usage=RequestUsage(prompt_tokens=727, completion_tokens=21), content=[FunctionCall(id='call_jc47', arguments='{\"query\": \"Multi-Agent LLMs\"}', name='research_arxiv_tool')], type='ToolCallRequestEvent'), ToolCallExecutionEvent(source='ResearchAgent', models_usage=None, content=[FunctionExecutionResult(content='Title: LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention\\nSummary: Large language models (LLMs) have shown remarkable potential in processing\\nlong sequences, yet efficiently serving these long-context models remains\\nchallenging due to the quadratic computational complexity of attention in the\\nprefilling stage and the large memory footprint of the KV cache in the decoding\\nstage. To address these issues, we introduce LServe, an efficient system that\\naccelerates long-sequence LLM serving via hybrid sparse attention. This method\\nunifies different hardware-friendly, structured sparsity patterns for both\\nprefilling and decoding attention into a single framework, where computations\\non less important tokens are skipped block-wise. LServe demonstrates the\\ncompatibility of static and dynamic sparsity in long-context LLM attention.\\nThis design enables multiplicative speedups by combining these optimizations.\\nSpecifically, we convert half of the attention heads to nearly free streaming\\nheads in both the prefilling and decoding stages. Additionally, we find that\\nonly a constant number of KV pages is required to preserve long-context\\ncapabilities, irrespective of context length. We then design a hierarchical KV\\npage selection policy that dynamically prunes KV pages based on query-centric\\nsimilarity. On average, LServe accelerates LLM prefilling by up to 2.9x and\\ndecoding by 1.3-2.1x over vLLM, maintaining long-context accuracy. Code is\\nreleased at https://github.com/mit-han-lab/omniserve.\\n\\nTitle: Aligning LLMs to Ask Good Questions A Case Study in Clinical Reasoning\\nSummary: Large language models (LLMs) often fail to ask effective questions under\\nuncertainty, making them unreliable in domains where proactive\\ninformation-gathering is essential for decisionmaking. We present ALFA, a\\nframework that improves LLM question-asking by (i) decomposing the notion of a\\n\"good\" question into a set of theory-grounded attributes (e.g., clarity,\\nrelevance), (ii) controllably synthesizing attribute-specific question\\nvariations, and (iii) aligning models via preference-based optimization to\\nexplicitly learn to ask better questions along these fine-grained attributes.\\nFocusing on clinical reasoning as a case study, we introduce the MediQ-AskDocs\\ndataset, composed of 17k real-world clinical interactions augmented with 80k\\nattribute-specific preference pairs of follow-up questions, as well as a novel\\nexpert-annotated interactive healthcare QA task to evaluate question-asking\\nabilities. Models aligned with ALFA reduce diagnostic errors by 56.6% on\\nMediQ-AskDocs compared to SOTA instruction-tuned LLMs, with a question-level\\nwin-rate of 64.4% and strong generalizability. Our findings suggest that\\nexplicitly guiding question-asking with structured, fine-grained attributes\\noffers a scalable path to improve LLMs, especially in expert application\\ndomains.\\n', call_id='call_jc47', is_error=False)], type='ToolCallExecutionEvent'), ToolCallSummaryMessage(source='ResearchAgent', models_usage=None, content='Title: LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention\\nSummary: Large language models (LLMs) have shown remarkable potential in processing\\nlong sequences, yet efficiently serving these long-context models remains\\nchallenging due to the quadratic computational complexity of attention in the\\nprefilling stage and the large memory footprint of the KV cache in the decoding\\nstage. To address these issues, we introduce LServe, an efficient system that\\naccelerates long-sequence LLM serving via hybrid sparse attention. This method\\nunifies different hardware-friendly, structured sparsity patterns for both\\nprefilling and decoding attention into a single framework, where computations\\non less important tokens are skipped block-wise. LServe demonstrates the\\ncompatibility of static and dynamic sparsity in long-context LLM attention.\\nThis design enables multiplicative speedups by combining these optimizations.\\nSpecifically, we convert half of the attention heads to nearly free streaming\\nheads in both the prefilling and decoding stages. Additionally, we find that\\nonly a constant number of KV pages is required to preserve long-context\\ncapabilities, irrespective of context length. We then design a hierarchical KV\\npage selection policy that dynamically prunes KV pages based on query-centric\\nsimilarity. On average, LServe accelerates LLM prefilling by up to 2.9x and\\ndecoding by 1.3-2.1x over vLLM, maintaining long-context accuracy. Code is\\nreleased at https://github.com/mit-han-lab/omniserve.\\n\\nTitle: Aligning LLMs to Ask Good Questions A Case Study in Clinical Reasoning\\nSummary: Large language models (LLMs) often fail to ask effective questions under\\nuncertainty, making them unreliable in domains where proactive\\ninformation-gathering is essential for decisionmaking. We present ALFA, a\\nframework that improves LLM question-asking by (i) decomposing the notion of a\\n\"good\" question into a set of theory-grounded attributes (e.g., clarity,\\nrelevance), (ii) controllably synthesizing attribute-specific question\\nvariations, and (iii) aligning models via preference-based optimization to\\nexplicitly learn to ask better questions along these fine-grained attributes.\\nFocusing on clinical reasoning as a case study, we introduce the MediQ-AskDocs\\ndataset, composed of 17k real-world clinical interactions augmented with 80k\\nattribute-specific preference pairs of follow-up questions, as well as a novel\\nexpert-annotated interactive healthcare QA task to evaluate question-asking\\nabilities. Models aligned with ALFA reduce diagnostic errors by 56.6% on\\nMediQ-AskDocs compared to SOTA instruction-tuned LLMs, with a question-level\\nwin-rate of 64.4% and strong generalizability. Our findings suggest that\\nexplicitly guiding question-asking with structured, fine-grained attributes\\noffers a scalable path to improve LLMs, especially in expert application\\ndomains.\\n', type='ToolCallSummaryMessage')], stop_reason=None)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "await team.reset()"
      ],
      "metadata": {
        "id": "nuYeqcgMIibt"
      },
      "execution_count": 5,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}