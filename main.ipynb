{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Install Requirements ()"
      ],
      "metadata": {
        "id": "N9QpaHdWScCx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install autogen-agentchat autogen-ext[openai] -q\n",
        "!pip install groq -q\n",
        "!pip install arxiv -q\n",
        "!pip install PyPDF2 -q"
      ],
      "metadata": {
        "id": "Bv3056-vSioY",
        "collapsed": true,
        "outputId": "13019823-7251-4dce-c3c1-90e5e687a31a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/75.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.9/83.9 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m234.0/234.0 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.1/65.1 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opentelemetry-sdk 1.16.0 requires opentelemetry-api==1.16.0, but you have opentelemetry-api 1.31.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.2/122.2 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from autogen_agentchat.agents import AssistantAgent, UserProxyAgent\n",
        "from autogen_agentchat.conditions import TextMentionTermination\n",
        "from autogen_agentchat.teams import RoundRobinGroupChat, SelectorGroupChat\n",
        "from autogen_agentchat.ui import Console\n",
        "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
        "from autogen_core.tools import FunctionTool\n",
        "from autogen_agentchat.conditions import MaxMessageTermination\n",
        "import arxiv"
      ],
      "metadata": {
        "id": "b5rD4jrbjD52"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "tokenGROQ = getpass('Enter GROQ_API_KEY here: ')\n",
        "os.environ[\"GROQ_API_KEY\"] = tokenGROQ"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0i8fNwjgR_1Q",
        "outputId": "dfdb62c7-47d0-45f1-a2b1-9110832ee8e9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter GROQ_API_KEY here: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from autogen_agentchat.agents import AssistantAgent, UserProxyAgent\n",
        "from autogen_agentchat.conditions import TextMentionTermination\n",
        "from autogen_agentchat.teams import RoundRobinGroupChat\n",
        "from autogen_agentchat.ui import Console\n",
        "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
        "\n",
        "# Create the agents.\n",
        "custom_model_client = OpenAIChatCompletionClient(\n",
        "    model=\"llama-3.3-70b-versatile\",\n",
        "    base_url=\"https://api.groq.com/openai/v1\",\n",
        "    api_key=os.environ[\"GROQ_API_KEY\"],\n",
        "    model_info={\n",
        "        \"vision\": False,\n",
        "        \"function_calling\": True,\n",
        "        \"json_output\": False,\n",
        "        \"family\": \"unknown\",\n",
        "    },\n",
        ")\n",
        "assistant = AssistantAgent(\"assistant\", model_client=custom_model_client)\n",
        "user_proxy = UserProxyAgent(\"user_proxy\", input_func=input)  # Use input() to get user input from console.\n",
        "\n",
        "# Create the termination condition which will end the conversation when the user says \"APPROVE\".\n",
        "termination = TextMentionTermination(\"DONE\")\n",
        "\n",
        "# Create the team.\n",
        "team = RoundRobinGroupChat([assistant, user_proxy], termination_condition=termination)\n",
        "\n",
        "# Run the conversation and stream to the console.\n",
        "stream = team.run_stream(task=\"the pdf file is a little bit large, can you read it if i sent it to you completely, or you need it in small chunks?\")\n",
        "await Console(stream)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G_wawy2QRzD7",
        "outputId": "11014ffa-0ba3-45b7-9b16-b87a44291e60"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------- user ----------\n",
            "the pdf file is a little bit large, can you read it if i sent it to you completely, or you need it in small chunks?\n",
            "---------- assistant ----------\n",
            "I'm a large language model, I don't have the capability to directly receive or process files, including PDFs. I can only process text-based input. If you'd like to share the content of the PDF, you can copy and paste the relevant text into this chat window, and I'll do my best to assist you.\n",
            "\n",
            "If the PDF is too large to copy and paste, you can also consider breaking it down into smaller sections or summarizing the main points, and I can help you with that.\n",
            "\n",
            "Please let me know how I can assist you further.\n",
            "Enter your response: DONE\n",
            "---------- user_proxy ----------\n",
            "DONE\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TaskResult(messages=[TextMessage(source='user', models_usage=None, metadata={}, content='the pdf file is a little bit large, can you read it if i sent it to you completely, or you need it in small chunks?', type='TextMessage'), TextMessage(source='assistant', models_usage=RequestUsage(prompt_tokens=88, completion_tokens=115), metadata={}, content=\"I'm a large language model, I don't have the capability to directly receive or process files, including PDFs. I can only process text-based input. If you'd like to share the content of the PDF, you can copy and paste the relevant text into this chat window, and I'll do my best to assist you.\\n\\nIf the PDF is too large to copy and paste, you can also consider breaking it down into smaller sections or summarizing the main points, and I can help you with that.\\n\\nPlease let me know how I can assist you further.\", type='TextMessage'), UserInputRequestedEvent(source='user_proxy', models_usage=None, metadata={}, request_id='eb38c076-59fb-4908-9821-f91fc5b54f9e', content='', type='UserInputRequestedEvent'), TextMessage(source='user_proxy', models_usage=None, metadata={}, content='DONE', type='TextMessage')], stop_reason=\"Text 'DONE' mentioned\")"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import arxiv\n",
        "\n",
        "# Construct the default API client.\n",
        "client = arxiv.Client()\n",
        "papers_ids = []\n",
        "\n",
        "# Search for the 10 most recent articles matching the keyword \"quantum.\"\n",
        "search = arxiv.Search(\n",
        "  query = \"Multi-agent LLM systems\",\n",
        "  max_results = 1,\n",
        "  sort_by = arxiv.SortCriterion.SubmittedDate\n",
        ")\n",
        "\n",
        "for result in client.results(search):\n",
        "  result.download_pdf(filename=f\"{result.get_short_id()}.pdf\")\n",
        "  papers_ids.append(result.get_short_id())\n",
        "  print(f\"\\nTitle: {result.get_short_id()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3--wxChSczw",
        "outputId": "2f8e0a80-b0f4-4290-dea6-52309c969adf"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Title: 2503.10630v1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "paper = next(arxiv.Client().results(arxiv.Search(id_list=[\"2503.10630v1\"])))\n",
        "print(paper.get_short_id())"
      ],
      "metadata": {
        "id": "Ep87-jkLTrUK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df686eda-498f-4d90-90ec-98af721f9ba7"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2503.10630v1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import PyPDF2\n",
        "import openai\n",
        "\n",
        "# Step 1: Extract text from the PDF\n",
        "pdf_path = \"/content/2503.10630v1.pdf\"  # Adjust the path as needed\n",
        "paper_text = \"\"\n",
        "with open(pdf_path, \"rb\") as f:\n",
        "    reader = PyPDF2.PdfReader(f)\n",
        "    for page in reader.pages:\n",
        "        page_text = page.extract_text()\n",
        "        if page_text:\n",
        "            paper_text += page_text + \"\\n\"\n",
        "\n",
        "# Step 2: Split the text into manageable chunks\n",
        "def chunk_text(text, max_length=2500):\n",
        "    \"\"\"Splits text into chunks of approximately max_length characters.\"\"\"\n",
        "    return [text[i:i+max_length] for i in range(0, len(text), max_length)]\n",
        "\n",
        "\n",
        "chunks = chunk_text(paper_text, max_length=2500)\n"
      ],
      "metadata": {
        "id": "5AunGDZrjK2O"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(chunks))"
      ],
      "metadata": {
        "id": "x0vdjyOajg2v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d932c0fb-5781-40e3-e12e-22d2b5892794"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def word_count(texts):\n",
        "  total_words = 0\n",
        "  total_characters = 0\n",
        "\n",
        "  for doc in texts:\n",
        "      content = doc\n",
        "      word_count = len(content.split())  # Count words\n",
        "      char_count = len(content)         # Count characters\n",
        "      total_words += word_count\n",
        "      total_characters += char_count\n",
        "\n",
        "  num_docs = len(texts)\n",
        "  avg_words = total_words / num_docs if num_docs > 0 else 0\n",
        "  avg_characters = total_characters / num_docs if num_docs > 0 else 0\n",
        "\n",
        "  print(f\"Average words per document: {avg_words}\")\n",
        "  print(f\"Average characters per document: {avg_characters}\")\n",
        "\n",
        "word_count(chunks)"
      ],
      "metadata": {
        "id": "7mL6iI2R8VNC",
        "outputId": "940bf63f-c91a-474d-bd0e-3fc7696db3d1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average words per document: 381.2916666666667\n",
            "Average characters per document: 2400.0833333333335\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain.schema import Document\n",
        "\n",
        "# Convert text into Document objects\n",
        "documents = [Document(page_content=text) for text in chunks]\n",
        "\n",
        "# Initialize text splitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "\n",
        "# Split the documents correctly\n",
        "splits = text_splitter.split_documents(documents)"
      ],
      "metadata": {
        "id": "47Vd7_lM8scJ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-community rank_bm25 -q"
      ],
      "metadata": {
        "id": "W5pwxsRV-Cvn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ee1606f-b28f-48dc-bf4e-9f6370808761"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.retrievers import BM25Retriever\n",
        "\n",
        "bm25_retriever_split = BM25Retriever.from_documents(splits)"
      ],
      "metadata": {
        "id": "2f13IRAR9zlI"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What are the components of agent systems?\"\n",
        "\n",
        "results = bm25_retriever_split.invoke(query)\n",
        "\n",
        "print(\"BM25 Retrieved Results:\")\n",
        "for i, doc in enumerate(results):\n",
        "    print(f\"Result {i+1}:\\n{doc.page_content}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0_MPxtdS3iay",
        "outputId": "7ef333b3-14d6-4b5e-c2d2-85f1e29279ba"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BM25 Retrieved Results:\n",
            "Result 1:\n",
            "pagation 𝑡- th Propagation(a)\n",
            "(b)\n",
            "LLM\n",
            "Scene Graph CorrectionFigure 3. Illustration of approach. (a) Stage 2: coordinate projec-\n",
            "tion and anchor pair alignment. (b) Stage 3: scene graph correction.\n",
            "gradually infer the projected BEV coordinates of the whole\n",
            "graph. During the traversal, we focus on the edge connecting\n",
            "last node (coordinate-known) and current node (coordinate-\n",
            "unknown), which stores the spatial relationship. For each\n",
            "edge, we prompt LLM with: [Current Node] is [Relation-\n",
            "ship] [Last Node], coordinate of [Last Node] is (x, y), the\n",
            "X-axis and Y-axis are positive towards yourself and the right\n",
            "respectively. What is the coordinate of [Current Node]? to\n",
            "project the current coordinate-unknown node to BEV .\n",
            "With the projected GtandGg, we conduct alignment\n",
            "for each anchor pair in order. Assume that the anchors\n",
            "arev1\n",
            "t,v2\n",
            "t∈ Gtandv1\n",
            "g,v2\n",
            "g∈ Gg.P∈R3×3is the 2D\n",
            "coordinate transfer matrix, consisting of scale S, rotation R\n",
            "and translation T, which is formulated as:\n",
            "P=S·R·T=\n",
            "\n",
            "Result 2:\n",
            "description. We also convert the observation of agent into an\n",
            "online maintained scene graph. With this consistent scene\n",
            "and goal representation, we preserve most structural infor-\n",
            "mation compared with pure text and are able to leverage\n",
            "LLM for explicit graph-based reasoning. Specifically, we\n",
            "conduct graph matching between the scene graph and goal\n",
            "graph at each time instant and propose different strategies\n",
            "to generate long-term goal of exploration according to dif-\n",
            "ferent matching states. The agent first iteratively searches\n",
            "subgraph of goal when zero-matched. With partial matching,\n",
            "the agent then utilizes coordinate projection and anchor pair\n",
            "alignment to infer the goal location. Finally scene graph cor-\n",
            "rection and goal verification are applied for perfect match-\n",
            "ing. We also present a blacklist mechanism to enable robust\n",
            "switch between stages. Extensive experiments on several\n",
            "benchmarks show that our UniGoal achieves state-of-the-art\n",
            "\n",
            "Result 3:\n",
            "long-term goal predicted in each stage.\n",
            "Table 2. Effect of pipeline design in UniGoal on HM3D (IIN)\n",
            "benchmark.\n",
            "Method SR SPL\n",
            "Simplify graph matching 54.9 20.7\n",
            "Remove blacklist mechanism 50.6 17.3\n",
            "Simplify multi-stage exploration policy 59.0 23.2\n",
            "Full Approach 60.2 23.7\n",
            "4.3. Ablation Study\n",
            "We conduct ablation experiments on HM3D to validate the\n",
            "effectiveness of each part in UniGoal. We report perfor-\n",
            "mance of the ablated versions on the representative IIN task.\n",
            "Pipeline Design. In Table 2, we ablate the graph match-\n",
            "ing method and multi-stage explore policy. We first simplify\n",
            "graph matching by removing the score computation. In this\n",
            "way, once anchor pair is matched, the agent will enter stage 2.\n",
            "Similarly, the agent will enter stage 3 once the central object\n",
            "oofGgis matched. It is shown that the agent cannot switch\n",
            "the exploration strategy at the optimal time without a judg-\n",
            "ment of matchin\n",
            "\n",
            "Result 4:\n",
            "form graph representation for scene and goal. Then we\n",
            "propose a graph matching method to determine whether a\n",
            "goal or its relevant objects are observed, which further guides\n",
            "the selection of scene exploration policies.\n",
            "Graph Construction. We define graph G= (V,E)as a\n",
            "set of nodes Vconnected with edges E. Each node represents\n",
            "an object. Each edge represents the relationship between\n",
            "objects, which only exists between spatially or semantically\n",
            "related object pairs. The content of nodes and edges is\n",
            "described in text format. Since the agent is initialized in\n",
            "unknown environment and continuously explores the scene,\n",
            "we follow SG-Nav [ 44] to construct the scene graph Gtin-\n",
            "crementally by expanding it every time the agent receives\n",
            "a new RGB-D observation. For goal graph Gg, we adopt\n",
            "different methods to process three kinds of goals ginto a\n",
            "graph, which we detail in supplementary material.\n",
            "Graph Matching. With scene graph Gtand goal graph\n",
            "Gg, we can match these two graphs to determine whether\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install pymupdf -q"
      ],
      "metadata": {
        "id": "K29d1f1_Cf5J",
        "outputId": "d0ac8499-c5fa-452d-e1aa-4d90306dbadc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m73.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz  # PyMuPDF\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    doc = fitz.open(pdf_path)\n",
        "    text = \"\"\n",
        "    for page in doc:\n",
        "        text += page.get_text() + \"\\n\"\n",
        "    return text\n",
        "\n",
        "pdf_text = extract_text_from_pdf(\"/content/2503.10630v1.pdf\")"
      ],
      "metadata": {
        "id": "uVVRsM5kCXlu"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WELE64YjGc_J",
        "outputId": "6664e0ff-d239-4be4-c98c-10faf6623368"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def chunk_text(text, chunk_size=500):\n",
        "    sentences = sent_tokenize(text)\n",
        "    chunks = []\n",
        "    current_chunk = \"\"\n",
        "\n",
        "    for sentence in sentences:\n",
        "        if len(current_chunk) + len(sentence) < chunk_size:\n",
        "            current_chunk += \" \" + sentence\n",
        "        else:\n",
        "            chunks.append(current_chunk)\n",
        "            current_chunk = sentence\n",
        "    chunks.append(current_chunk)  # Last chunk\n",
        "    return chunks\n",
        "\n",
        "chunks = chunk_text(pdf_text, chunk_size=1024)"
      ],
      "metadata": {
        "id": "WDzAT8OvChRo"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(chunks))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oqJaNaRyLVTE",
        "outputId": "873be458-db57-477d-9217-f3859776d396"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "62\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def arxiv_search(chunk_index: int) -> list:  # type: ignore[type-arg]\n",
        "    return chunks[chunk_index * 22:(chunk_index + 1) * 22]\n",
        "\n",
        "arxiv_search_tool = FunctionTool(\n",
        "    arxiv_search, description=\"get the paper content\"\n",
        ")\n",
        "\n",
        "arxiv_search_agentt = AssistantAgent(\n",
        "    name=\"Arxiv_Search_Agent\",\n",
        "    tools=[arxiv_search_tool],\n",
        "    model_client=OpenAIChatCompletionClient(\n",
        "        model=\"llama-3.3-70b-versatile\",\n",
        "        base_url=\"https://api.groq.com/openai/v1\",\n",
        "        api_key=os.environ[\"GROQ_API_KEY\"],\n",
        "        model_info={\n",
        "            \"vision\": False,\n",
        "            \"function_calling\": True,\n",
        "            \"json_output\": False,\n",
        "            \"family\": \"unknown\",\n",
        "        },\n",
        "    ),\n",
        "    description=\"An agent that can get the paper content\",\n",
        "    system_message=\"You are a helpful AI assistant. Solve tasks using your tools..\",\n",
        ")\n",
        "\n",
        "summarizeAgent = AssistantAgent(\n",
        "    name=\"summarize_Agent\",\n",
        "    model_client=OpenAIChatCompletionClient(\n",
        "        model=\"llama-3.3-70b-versatile\",\n",
        "        base_url=\"https://api.groq.com/openai/v1\",\n",
        "        api_key=os.environ[\"GROQ_API_KEY\"],\n",
        "        model_info={\n",
        "            \"vision\": False,\n",
        "            \"function_calling\": True,\n",
        "            \"json_output\": False,\n",
        "            \"family\": \"unknown\",\n",
        "        },\n",
        "    ),\n",
        "    description=\"An agent that can summarize papers.\",\n",
        "    system_message=\"You are a helpful AI assistant. A paper given to you please summarize it SHORTLY AND PRECISELY.\",\n",
        ")\n",
        "\n",
        "text_mention_termination = TextMentionTermination(\"TERMINATE\")\n",
        "max_messages_termination = MaxMessageTermination(max_messages=5)\n",
        "termination = text_mention_termination | max_messages_termination\n",
        "\n",
        "team = RoundRobinGroupChat(\n",
        "    participants=[arxiv_search_agentt, summarizeAgent], termination_condition=termination\n",
        ")\n",
        "\n",
        "async def summarize_large_paper():\n",
        "    chunk_index = 0\n",
        "    final_summary = \"\"\n",
        "    while True:\n",
        "        # Fetch the next chunk\n",
        "        chunk = arxiv_search(chunk_index)\n",
        "        if not chunk:\n",
        "            break  # Exit the loop if there are no more chunks\n",
        "\n",
        "        # Summarize the current chunk\n",
        "        summary = await team.run_stream(\n",
        "            task=f\"Summarize this chunk of the paper: {chunk}\"\n",
        "        )\n",
        "        final_summary += summary + \"\\n\"\n",
        "        chunk_index += 1\n",
        "\n",
        "    # Final summarization of all chunk summaries\n",
        "    final_summary = await team.run_stream(\n",
        "        task=f\"Summarize the following summaries into one concise summary: {final_summary}\"\n",
        "    )\n",
        "    return final_summary\n",
        "\n",
        "# Run the summarization process\n",
        "final_summary = await summarize_large_paper()\n",
        "print(final_summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "id": "w0vHIOPWdXf5",
        "outputId": "0573bb17-fada-433f-c91b-0d713b539599"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "object async_generator can't be used in 'await' expression",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-711a174bc215>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;31m# Run the summarization process\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m \u001b[0mfinal_summary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0msummarize_large_paper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_summary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-28-711a174bc215>\u001b[0m in \u001b[0;36msummarize_large_paper\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;31m# Summarize the current chunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         summary = await team.run_stream(\n\u001b[0m\u001b[1;32m     62\u001b[0m             \u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"Summarize this chunk of the paper: {chunk}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         )\n",
            "\u001b[0;31mTypeError\u001b[0m: object async_generator can't be used in 'await' expression"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def arxiv_search() -> list:  # type: ignore[type-arg]\n",
        "    return chunks[:22]\n",
        "\n",
        "arxiv_search_tool = FunctionTool(\n",
        "    arxiv_search, description=\"get the paper content\"\n",
        ")\n",
        "\n",
        "arxiv_search_agentt = AssistantAgent(\n",
        "    name=\"Arxiv_Search_Agent\",\n",
        "    tools=[arxiv_search_tool],\n",
        "    model_client = OpenAIChatCompletionClient(\n",
        "      model=\"llama-3.3-70b-versatile\",\n",
        "      base_url=\"https://api.groq.com/openai/v1\",\n",
        "      api_key=os.environ[\"GROQ_API_KEY\"],\n",
        "      model_info={\n",
        "        \"vision\": False,\n",
        "        \"function_calling\": True,\n",
        "        \"json_output\": False,\n",
        "        \"family\": \"unknown\",\n",
        "      },\n",
        "    ),\n",
        "    description=\"An agent that can get the paper content\",\n",
        "    system_message=\"You are a helpful AI assistant. Solve tasks using your tools..\",\n",
        ")\n",
        "\n",
        "\n",
        "summarizeAgent = AssistantAgent(\n",
        "    name=\"summarize_Agent\",\n",
        "    model_client = OpenAIChatCompletionClient(\n",
        "      model=\"llama-3.3-70b-versatile\",\n",
        "      base_url=\"https://api.groq.com/openai/v1\",\n",
        "      api_key=os.environ[\"GROQ_API_KEY\"],\n",
        "      model_info={\n",
        "        \"vision\": False,\n",
        "        \"function_calling\": True,\n",
        "        \"json_output\": False,\n",
        "        \"family\": \"unknown\",\n",
        "      },\n",
        "    ),\n",
        "    description=\"An agent that can summarize papers.\",\n",
        "    system_message=\"You are a helpful AI assistant. a paper given to you please summarize it SHORTLY AND PRECISELY.\",\n",
        ")\n",
        "\n",
        "text_mention_termination = TextMentionTermination(\"TERMINATE\")\n",
        "max_messages_termination = MaxMessageTermination(max_messages=5)\n",
        "termination = text_mention_termination | max_messages_termination\n",
        "\n",
        "\n",
        "team = RoundRobinGroupChat(\n",
        "    participants=[arxiv_search_agentt, summarizeAgent], termination_condition=termination\n",
        ")\n",
        "\n",
        "await Console(\n",
        "    team.run_stream(\n",
        "        task=\"give me the summarization of the given paper\",\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "awA_5DYGIlWs",
        "outputId": "574a7cc4-4724-4808-c41e-e00d63964566"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------- user ----------\n",
            "give me the summarization of the given paper\n",
            "---------- Arxiv_Search_Agent ----------\n",
            "[FunctionCall(id='call_rzjq', arguments='{}', name='arxiv_search')]\n",
            "---------- Arxiv_Search_Agent ----------\n",
            "[FunctionExecutionResult(content=\"[' UniGoal: Towards Universal Zero-shot Goal-oriented Navigation\\\\nHang Yin1*, Xiuwei Xu1∗†, Linqing Zhao1, Ziwei Wang2, Jie Zhou1, Jiwen Lu1‡\\\\n1Tsinghua University\\\\n2Nanyang Technological University\\\\n{yinh23, xxw21, zhaolinqing}@mails.tsinghua.edu.cn;\\\\nziwei.wang@ntu.edu.sg; {jzhou, lujiwen}@tsinghua.edu.cn\\\\nAbstract\\\\nIn this paper, we propose a general framework for univer-\\\\nsal zero-shot goal-oriented navigation. Existing zero-shot\\\\nmethods build inference framework upon large language\\\\nmodels (LLM) for specific tasks, which differs a lot in over-\\\\nall pipeline and fails to generalize across different types of\\\\ngoal. Towards the aim of universal zero-shot navigation, we\\\\npropose a uniform graph representation to unify different\\\\ngoals, including object category, instance image and text\\\\ndescription. We also convert the observation of agent into an\\\\nonline maintained scene graph.', 'With this consistent scene\\\\nand goal representation, we preserve most structural infor-\\\\nmation compared with pure text and are able to leverage\\\\nLLM for explicit graph-based reasoning. Specifically, we\\\\nconduct graph matching between the scene graph and goal\\\\ngraph at each time instant and propose different strategies\\\\nto generate long-term goal of exploration according to dif-\\\\nferent matching states. The agent first iteratively searches\\\\nsubgraph of goal when zero-matched. With partial matching,\\\\nthe agent then utilizes coordinate projection and anchor pair\\\\nalignment to infer the goal location. Finally scene graph cor-\\\\nrection and goal verification are applied for perfect match-\\\\ning. We also present a blacklist mechanism to enable robust\\\\nswitch between stages. Extensive experiments on several\\\\nbenchmarks show that our UniGoal achieves state-of-the-art\\\\nzero-shot performance on three studied navigation tasks with\\\\na single model, even outperforming task-specific zero-shot\\\\nmethods and supervised universal methods.', 'Project Page. 1. Introduction\\\\nGoal-oriented navigation is a fundamental problem in var-\\\\nious robotic tasks, which requires the agent to navigate to\\\\na specified goal in an unknown environment. Depending\\\\non the goal type, there are many popular sub-tasks of goal-\\\\noriented navigation, among which we focus on three rep-\\\\n* Equal contribution. † Project lead. ‡ Corresponding author. UniGoal (Ours)\\\\nBed\\\\nIt is a chair. In front of it, \\\\nthere is a coffee table. Two \\\\nwine glasses and one \\\\ntransparent glass bottle \\\\nare in the upper left corner. Scene Graph\\\\nGoal Graph\\\\nText Goal\\\\nObject Goal\\\\nIns-Image Goal\\\\n50.6\\\\n54.0\\\\n54.5\\\\nUniversal methods:\\\\nUniGoal (Ours)\\\\nGOAT (Supervised)\\\\nZero-shot methods:\\\\nSG-Nav\\\\nMod-IIN\\\\n37.4\\\\n56.1\\\\n60.2\\\\n17.0\\\\n20.2\\\\nInstance Image-goal\\\\nText-goal\\\\nObject-goal\\\\nMatching\\\\n&\\\\nExploration\\\\nFigure 1. State-of-the-art zero-shot goal-oriented navigation meth-\\\\nods are typically specialized for each goal type.', 'Although recent\\\\nwork presents universal goal-oriented navigation method, it requires\\\\nto train policy networks on large-scale data and lacks zero-shot gen-\\\\neralization ability. We propose UniGoal, which enables zero-shot\\\\ninference on three studied navigation tasks with a unified frame-\\\\nwork and achieves leading performance on multiple benchmarks. resentative types: object category, instance image and text\\\\ndescription. These sub-tasks are also known as Object-goal\\\\nNavigation (ON) [7, 44, 49], Instance-image-goal Naviga-\\\\ntion (IIN) [15, 16] and Text-goal Navigation (TN) [37]. With\\\\nthe development of deep learning, reinforcement learning\\\\n(RL) and vision/language foundation models, we have wit-\\\\nnessed great achievement of performance on each individual\\\\nsub-task. However, in actual application scenarios, the high\\\\nflexibility of human instructions requires high versatility of\\\\nagent. Therefore, a universal method that can handle all\\\\nsub-tasks in a single model is of great desire.', 'Towards the aim of universal goal-oriented navigation, a\\\\nnatural solution is to learn a uniform representation of differ-\\\\nent kinds of goals. GOAT [6] trains a universal global policy\\\\narXiv:2503.10630v1  [cs.CV]  13 Mar 2025\\\\n\\\\non the three goal-oriented sub-tasks, which learns a shared\\\\ngoal embedding with RL. To reduce the requirement of train-\\\\ning resources, PSL [37] utilizes CLIP [28] embedding to\\\\nuniformly represent category, image and text description. But it still requires time-consuming RL training for the pol-\\\\nicy networks. Moreover, these training-based methods tend\\\\nto overfit on the simulation environment and thus show weak\\\\ngeneralization ability when applied to real world. To solve\\\\nabove limitations, zero-shot navigation methods [44, 49]\\\\nappear to be an ideal choice, where the agent does not re-\\\\nquire any training or finetuning when deployed to a certain\\\\ntask. The mainstream solution of zero-shot methods is to\\\\nleverage large language models (LLM) [1, 39] for general\\\\nreasoning and decision making.', 'However, although utilizing\\\\nLLM makes zero-shot navigation feasible, current methods\\\\nare designed for specific sub-task, which cannot transfer\\\\nto wider range of goal types. The recent InstructNav [25]\\\\nproposes a general framework to solve several language-\\\\nrelated navigation tasks with chain-of-thought, but it is still\\\\nunable to handle vision-related navigation like IIN. There-\\\\nfore, a uniform inference framework for universal zero-shot\\\\ngoal-oriented navigation is highly demanded. In this paper, we propose UniGoal to solve the above\\\\nproblems. Different from previous works which represent\\\\nscene and goal in text format and design task-specific work-\\\\nflow for LLM, we propose a uniform graph representation\\\\nfor both 3D scene and goal and formulate a general LLM-\\\\nbased scene exploration framework. With our graph-based\\\\nrepresentation, the 3D scene, object category, instance image\\\\nand text description can be uniformly represented with mini-\\\\nmal structural information loss compared to text description.', 'The consistent graph format between scene and goal also\\\\nenables accurate explicit reasoning including similarity com-\\\\nputation, graph matching and graph alignment. Specifically,\\\\nwe construct an online 3D scene graph along with the mov-\\\\ning of agent. At each time instant, we first conduct graph\\\\nmatching between the scene graph and goal graph. Then we\\\\npropose a multi-stage scene exploration policy, which adopts\\\\ndifferent strategies to generate long-term goal of exploration\\\\naccording to different matching states. With exploration\\\\nof unknown regions, the matching score will increase and\\\\nthe policy will progress between three stages: iterative sub-\\\\ngraph searching for zero matching, coordinate projection\\\\nand anchor pair alignment for partial matching, and scene\\\\ngraph correction and goal verification for perfect matching. To enable robust switch between stages, we also present a\\\\nblacklist mechanism to freeze unmatched parts of graphs and\\\\nencourage exploration to new regions.', 'Experimental results\\\\non several benchmarks of MatterPort3D [4], HM3D [30]\\\\nand RoboTHOR [10] show that UniGoal achieves superior\\\\nperformance on all three tasks with a single model, even out-\\\\nperforming zero-shot methods that designed for specific task\\\\nand universal methods that requires training or finetuning. 2. Related Work\\\\nZero-shot Navigation. Conventional supervised naviga-\\\\ntion methods [7, 11, 18, 31, 41] requires large-scale training\\\\nin simulation environments, which limits the generaliza-\\\\ntion ability. According to the goal type, zero-shot naviga-\\\\ntion can be mainly divided into ON [3, 46], IIN [16] and\\\\nTN [25]. Based on open-vocabulary CLIP [28], CoW [12]\\\\nconstructs zero-shot ON baseline using frontier-based explo-\\\\nration (FBE). ESC [49], OpenFMNav [8] and VLFM [45]\\\\nfurther extract common sense about correlations between\\\\nobjects using LLM for goal location reasoning. For zero-\\\\nshot IIN, Mod-IIN [16] simply utilizes FBE for exploration\\\\nand key point matching for goal identification.', 'For TN,\\\\ncurrently there are only supervised methods [6, 37] and zero-\\\\nshot ones are still missing. The recent InstructNav [25]\\\\nproposes a universal zero-shot framework for language-\\\\nrelated navigation tasks. It can be applied to ON, demand-\\\\ndriven navigation (DDN) and vision-language-navigation\\\\n(VLN) [9, 20, 26, 47, 48]. But it is still unable to handle\\\\nvisual goal as in IIN. Different from these approaches, our\\\\nUniGoal proposes a unified graph representation for goal\\\\nand scene, which can elegantly handle both language and\\\\nvision-related goal-oriented navigation tasks. Graph-based Scene Exploration. In order to better un-\\\\nderstand and explore the scene, there are a variety of scene\\\\nrepresentation methods. Among them, graph-based repre-\\\\nsentation [2, 13, 14, 33, 40] is one of the most popular and\\\\npromising representations based on explicit graph structure,\\\\nwhich shows great potential to be combined with LLM and\\\\nVLM for high-level reasoning.', 'SayPlan [32] leverages LLM\\\\nto perform task planning by searching on a 3D scene graph. OVSG [5] constructs an open-vocabulary scene graph for\\\\ncontext-aware descriptions and performs graph matching to\\\\nground the queried entity. There are also many works using\\\\ngraph representation for navigation tasks [24, 29, 42, 44]. Among them the most related work to ours is SG-Nav [44],\\\\nwhich constructs an online hierarchical scene graph and pro-\\\\nposes chain-of-thought prompting for LLM to reason the\\\\ngoal location based on graph sturcture. However, SG-Nav is\\\\nspecialized for ON, thus cannot fully exploit the rich infor-\\\\nmation contained in other types of goal. While our UniGoal\\\\nfurther represents the goal in a graph and perform graph\\\\nmatching between scene and goal to guide a multi-stage\\\\nscene exploration policy, which make full use of the correla-\\\\ntion between scene and goal for LLM reasoning. 3.', 'Approach\\\\nIn this section, we first provide the definition of universal\\\\nzero-shot goal-oriented navigation and introduce the frame-\\\\nwork of UniGoal. Then we construct graphs for scene and\\\\ngoal and conduct graph matching for goal identification. Next we design a multi-stage scene exploration policy by\\\\n\\\\nObservation\\\\nRGB-D\\\\nOccupancy Map\\\\nScene Graph\\\\nAgent Pose\\\\nGraph Matching\\\\nStage 1: Zero Matching\\\\nStage 2: Partial Matching\\\\nGoal Decomposition\\\\nStage 3: Perfect Matching\\\\nGraph Correction\\\\nGoal Verification\\\\nDeterministic \\\\nLocal Policy\\\\nAction\\\\nBlacklist\\\\nObject\\\\nImage\\\\nText\\\\nGoal Graph\\\\nGlobal Policy\\\\nSelect Frontier\\\\nFigure 2. Framework of UniGoal. We convert different types of goals into a uniform graph representation and maintain an online scene\\\\ngraph. At each step, we perform graph matching between the scene graph and goal graph, where the matching score will be utilized to\\\\nguide a multi-stage scene exploration policy.', 'For different degree of matching, our exploration policy leverages LLM to exploit the graphs\\\\nwith different aims: first expand observed area, then infer goal location based on the overlap of graphs, and finally verify the goal. We also\\\\npropose a blacklist that records unsuccessful matching to avoid repeated exploration. prompting LLM with graphs. Finally we propose a blacklist\\\\nmechanism to robustly avoid repeated exploration. 3.1. Goal-oriented Navigation\\\\nIn goal-oriented navigation, a mobile agent is tasked with\\\\nnavigating to a specified goal g in an unknown environment,\\\\nwhere g can be an object category (i.e. Object-goal Nav-\\\\nigation [7], ON), an image containing object that can be\\\\nfound in the scene (i.e. Instance-Image-goal Navigation [15],\\\\nIIN), or a description about a certain object (i.e. Text-goal\\\\nNavigation [37], TN).', 'For IIN and TN, there is a central\\\\nobject o as well as other relevant objects in g. While for\\\\nON, we have o = g. The agent receives posed RGB-D video\\\\nstream and is required to execute an action a ∈A at each\\\\ntime it receiving a new RGB-D observation. A is the set of\\\\nactions, which consists of move_forward, turn_left,\\\\nturn_right and stop. The task is successfully done if\\\\nthe agent stops within r meters of o in less than T steps. More details about the three sub-tasks can be found in sup-\\\\nplementary material. Task Specification. We aim to study the problem of\\\\nuniversal zero-shot goal-oriented navigation, which has two\\\\ncharacteristics: (1) Universal. We should design a general\\\\nmethod, which requires no modification when switching be-\\\\ntween the three sub-tasks. (2) Zero-shot. All three kinds of\\\\ngoal can be specified by free-form language or image. Our\\\\nnavigation method does not require any training or finetun-\\\\ning, which is of great generalization ability. Overview.', 'Universal zero-shot goal-oriented navigation\\\\nrequires the agent to complete different sub-tasks with a\\\\nsingle framework in training-free manner. Since this task\\\\nrequires extremely strong generalization ability, we utilize\\\\nlarge language model (LLM) [1, 39] for zero-shot decision\\\\nmaking by exploiting its rich knowledge and strong reason-\\\\ning ability. To make LLM aware of the visual observations\\\\nas well as unifying different kinds of goals, we propose to\\\\nrepresent the scene and goal in graphs, i.e., scene graph\\\\nand goal graph. In this way, different goals are represented\\\\nuniformly and the representations of scene and goal are con-\\\\nsistent. Based on this representation, we prompt LLM with\\\\nscene graph and goal graph for scene understanding, graph\\\\nmatching and decision making for exploration. The overall\\\\npipeline is illustrated in Figure 2. 3.2. Graph Construction and Matching\\\\nIn this subsection, we first describe how to construct a uni-\\\\nform graph representation for scene and goal.', 'Then we\\\\npropose a graph matching method to determine whether a\\\\ngoal or its relevant objects are observed, which further guides\\\\nthe selection of scene exploration policies. Graph Construction. We define graph G = (V, E) as a\\\\nset of nodes V connected with edges E. Each node represents\\\\nan object. Each edge represents the relationship between\\\\nobjects, which only exists between spatially or semantically\\\\nrelated object pairs. The content of nodes and edges is\\\\ndescribed in text format. Since the agent is initialized in\\\\nunknown environment and continuously explores the scene,\\\\nwe follow SG-Nav [44] to construct the scene graph Gt in-\\\\ncrementally by expanding it every time the agent receives\\\\na new RGB-D observation. For goal graph Gg, we adopt\\\\ndifferent methods to process three kinds of goals g into a\\\\ngraph, which we detail in supplementary material. Graph Matching. With scene graph Gt and goal graph\\\\nGg, we can match these two graphs to determine whether\\\\nthe goal is observed.', 'If no elements in Gg are observed, the\\\\nagent needs to infer relationship between objects from Gt\\\\nto plan a path that is most likely to find the goal. If Gg is\\\\npartially observed in Gt, the agent can use the overlapped\\\\npart of Gg and Gt to reason out where the rest of Gg is. If\\\\nGg is perfectly observed in Gt, the agent can move to the\\\\ngoal and make further verification. Therefore, a goal scoring\\\\nmethod is crucial for the follow-up scene exploration. We propose to apply graph matching to achieve this. Given Gt and Gg, we design three matching metrics, i.e.,\\\\nnode matching, edge matching and topology matching, to\\\\nscore how well the goal is observed.', 'Formally, for nodes\\\\nand edges, we extract their embeddings and then compute\\\\npair-wise similarity with bipartite matching to determine\\\\nmatched pairs of nodes and edges:\\\\nMN = B(thr(Embed(Vt) · Embed(Vg)T ))\\\\n(1)\\\\nME = B(thr(Embed(Et) · Embed(Eg)T ))\\\\n(2)\\\\nwhere Embed(·) ∈RK×C, K is the number of nodes or\\\\nedges and C is the channel dimension, which is detailed in\\\\nsupplementary material. thr(·) is an element-wise threshold\\\\nfunction applied on the similarity matrix which sets values\\\\nsmaller than τ to -1 to disable matching of the corresponding\\\\npairs. B(·) is bipartite matching, which outputs a list of all\\\\nmatched node or edge pairs, namely MN or ME. We also\\\\naverage the similarity matrix of nodes and edges to acquire\\\\nthe similarity scores SN and SE. Based on MN and ME, we further compute topological\\\\nsimilarity between Gt and Gg, which is defined as the graph\\\\nediting similarity between them:\\\\nST = 1 −D(S(F(Gt, MN, ME)), S(Gg))\\\\n(3)\\\\nwhere F(Gt, MN, ME) means the minimal subgraph of\\\\nGt with nodes in MN and edges in ME.', 'S(·) means the\\\\ntopological structure of a graph regardless of the content of\\\\nnodes and edges. D(·) is the normalized editing distance [34]\\\\nbetween two graphs. The final matching score is defined as\\\\nS = (SN + SE + ST )/3. 3.3. Multi-stage Scene Exploration\\\\nAs described above, different matching scores will lead to\\\\ndifferent scene exploration policies. From zero matching to\\\\nperfect matching, we design three stages to progressively\\\\nexplore the scene and generate long-term exploration goal. This long-term goal will be processed by a deterministic\\\\nlocal policy [36] to obtain actions. Below we detail our\\\\nexploration policy stage by stage. Stage 1: Zero Matching. If the matching score S is\\\\nsmaller than σ1, we regard this stage as zero matching. Since\\\\nthere is almost no element of Gg observed in Gt, the aim of\\\\nagent at this stage is to expand its explored region and find\\\\nelements in Gg.', 'Note this problem is similar to ON: before\\\\nobserving the goal, the agent needs to explore unknown\\\\nregions without any matching between goal and scene. We\\\\ncan simply resort to scene graph-based ON method [44]\\\\nat this stage, which navigates to frontiers with semantic\\\\nrelationships between the scene graph and goal as guidance. However, different from ON where the goal is always an\\\\nobject node, in our universal goal-oriented navigation the\\\\ngoal may be a complicated graph. A graph may consist of\\\\nseveral less related subgraph parts. For example, in a graph\\\\n(table, chair, window, curtain), [table, chair] and [window,\\\\ncurtain] are two subgraphs which have strong internal cor-\\\\nrelation but weak interrelation. We empirically observe that\\\\nlocating a collection of multiple unrelated subgraphs in a\\\\nscene at the same time is much more difficult than locating a\\\\nsingle subgraph once at a time. Therefore, we decompose Gg\\\\ninto multiple internally correlated subgraphs with the guid-\\\\nance of LLM.', 'For each subgraph of Gg, we convert it to a text\\\\ndescription, which is regarded as an object goal to call [44]\\\\nfor frontier selection. Finally, we select one frontier from the\\\\nproposed ones by averaging the frontier scores and distances\\\\nto the agent. Details about LLM-guided decomposition and\\\\nscore computation can be found in supplementary material. With this strategy, we not only utilize the information of\\\\nthe entire Gg, but also eliminate ambiguity during frontier\\\\nselection caused by unrelated subgraphs. Stage 2: Partial Matching. With the exploration of\\\\nagent, the elements of Gg will gradually be observed in\\\\nGt and thus S continues to increase. When S exceeds σ1\\\\n(but still smaller than σ2) and there is at least one anchor\\\\npair, we switch to partial matching stage. Here anchor pair\\\\nmeans a pair of exactly matched nodes, i.e., two unconnected\\\\nmatched nodes in MN or one matched edge in MM. Note that we store the world coordinates of nodes in Gt.', 'If we also know the relative coordinates of nodes in Gg, we\\\\ncan map the coordinates of Gt and Gg to bird’s-eye view\\\\n(BEV) and align the anchor pair of Gg to the one of Gt. In\\\\nthis way, after alignment we can directly infer where the rest\\\\nof Gg is, which provides the agent with clear exploration\\\\ngoal for each anchor pair. Luckily, although we do not have\\\\nany coordinate information about the goal, at least we are\\\\naware of the relative spatial relationship between nodes in\\\\nGg, such as a chair on the left of a table, a keyboard in front\\\\nof a monitor. Inspired by this, we propose a coordinate\\\\nprojection strategy that preserves spatial relationships. Given Gg without any coordinate information, we first\\\\nproject the central object node o to (0, 0) as an initialization. To infer the projected coordinates of other nodes, we need\\\\nto utilize the spatial relationship between coordinate-known\\\\nnodes and coordinate-unknown nodes.', 'Since at beginning\\\\nwe only have one coordinate-known node o, we start from it\\\\nand traverse the goal graph with Depth First Search (DFS) to\\\\n\\\\nAnchor Pair Alignment\\\\nCoordinate Projection\\\\nX\\\\nY\\\\nZ\\\\nright\\\\nin front of\\\\nright\\\\n①\\\\n②\\\\n③\\\\n④\\\\n⑤\\\\nDFS\\\\nin front of\\\\nCorrection\\\\n(𝑡−2)-th Propagation\\\\n(𝑡−1)- th Propagation\\\\n𝑡- th Propagation\\\\n(a)\\\\n(b)\\\\nLLM\\\\nScene Graph Correction\\\\nFigure 3. Illustration of approach. (a) Stage 2: coordinate projec-\\\\ntion and anchor pair alignment. (b) Stage 3: scene graph correction. gradually infer the projected BEV coordinates of the whole\\\\ngraph. During the traversal, we focus on the edge connecting\\\\nlast node (coordinate-known) and current node (coordinate-\\\\nunknown), which stores the spatial relationship. For each\\\\nedge, we prompt LLM with: [Current Node] is [Relation-\\\\nship] [Last Node], coordinate of [Last Node] is (x, y), the\\\\nX-axis and Y-axis are positive towards yourself and the right\\\\nrespectively. What is the coordinate of [Current Node]? to\\\\nproject the current coordinate-unknown node to BEV.']\", name='arxiv_search', call_id='call_rzjq', is_error=False)]\n",
            "---------- Arxiv_Search_Agent ----------\n",
            "[' UniGoal: Towards Universal Zero-shot Goal-oriented Navigation\\nHang Yin1*, Xiuwei Xu1∗†, Linqing Zhao1, Ziwei Wang2, Jie Zhou1, Jiwen Lu1‡\\n1Tsinghua University\\n2Nanyang Technological University\\n{yinh23, xxw21, zhaolinqing}@mails.tsinghua.edu.cn;\\nziwei.wang@ntu.edu.sg; {jzhou, lujiwen}@tsinghua.edu.cn\\nAbstract\\nIn this paper, we propose a general framework for univer-\\nsal zero-shot goal-oriented navigation. Existing zero-shot\\nmethods build inference framework upon large language\\nmodels (LLM) for specific tasks, which differs a lot in over-\\nall pipeline and fails to generalize across different types of\\ngoal. Towards the aim of universal zero-shot navigation, we\\npropose a uniform graph representation to unify different\\ngoals, including object category, instance image and text\\ndescription. We also convert the observation of agent into an\\nonline maintained scene graph.', 'With this consistent scene\\nand goal representation, we preserve most structural infor-\\nmation compared with pure text and are able to leverage\\nLLM for explicit graph-based reasoning. Specifically, we\\nconduct graph matching between the scene graph and goal\\ngraph at each time instant and propose different strategies\\nto generate long-term goal of exploration according to dif-\\nferent matching states. The agent first iteratively searches\\nsubgraph of goal when zero-matched. With partial matching,\\nthe agent then utilizes coordinate projection and anchor pair\\nalignment to infer the goal location. Finally scene graph cor-\\nrection and goal verification are applied for perfect match-\\ning. We also present a blacklist mechanism to enable robust\\nswitch between stages. Extensive experiments on several\\nbenchmarks show that our UniGoal achieves state-of-the-art\\nzero-shot performance on three studied navigation tasks with\\na single model, even outperforming task-specific zero-shot\\nmethods and supervised universal methods.', 'Project Page. 1. Introduction\\nGoal-oriented navigation is a fundamental problem in var-\\nious robotic tasks, which requires the agent to navigate to\\na specified goal in an unknown environment. Depending\\non the goal type, there are many popular sub-tasks of goal-\\noriented navigation, among which we focus on three rep-\\n* Equal contribution. † Project lead. ‡ Corresponding author. UniGoal (Ours)\\nBed\\nIt is a chair. In front of it, \\nthere is a coffee table. Two \\nwine glasses and one \\ntransparent glass bottle \\nare in the upper left corner. Scene Graph\\nGoal Graph\\nText Goal\\nObject Goal\\nIns-Image Goal\\n50.6\\n54.0\\n54.5\\nUniversal methods:\\nUniGoal (Ours)\\nGOAT (Supervised)\\nZero-shot methods:\\nSG-Nav\\nMod-IIN\\n37.4\\n56.1\\n60.2\\n17.0\\n20.2\\nInstance Image-goal\\nText-goal\\nObject-goal\\nMatching\\n&\\nExploration\\nFigure 1. State-of-the-art zero-shot goal-oriented navigation meth-\\nods are typically specialized for each goal type.', 'Although recent\\nwork presents universal goal-oriented navigation method, it requires\\nto train policy networks on large-scale data and lacks zero-shot gen-\\neralization ability. We propose UniGoal, which enables zero-shot\\ninference on three studied navigation tasks with a unified frame-\\nwork and achieves leading performance on multiple benchmarks. resentative types: object category, instance image and text\\ndescription. These sub-tasks are also known as Object-goal\\nNavigation (ON) [7, 44, 49], Instance-image-goal Naviga-\\ntion (IIN) [15, 16] and Text-goal Navigation (TN) [37]. With\\nthe development of deep learning, reinforcement learning\\n(RL) and vision/language foundation models, we have wit-\\nnessed great achievement of performance on each individual\\nsub-task. However, in actual application scenarios, the high\\nflexibility of human instructions requires high versatility of\\nagent. Therefore, a universal method that can handle all\\nsub-tasks in a single model is of great desire.', 'Towards the aim of universal goal-oriented navigation, a\\nnatural solution is to learn a uniform representation of differ-\\nent kinds of goals. GOAT [6] trains a universal global policy\\narXiv:2503.10630v1  [cs.CV]  13 Mar 2025\\n\\non the three goal-oriented sub-tasks, which learns a shared\\ngoal embedding with RL. To reduce the requirement of train-\\ning resources, PSL [37] utilizes CLIP [28] embedding to\\nuniformly represent category, image and text description. But it still requires time-consuming RL training for the pol-\\nicy networks. Moreover, these training-based methods tend\\nto overfit on the simulation environment and thus show weak\\ngeneralization ability when applied to real world. To solve\\nabove limitations, zero-shot navigation methods [44, 49]\\nappear to be an ideal choice, where the agent does not re-\\nquire any training or finetuning when deployed to a certain\\ntask. The mainstream solution of zero-shot methods is to\\nleverage large language models (LLM) [1, 39] for general\\nreasoning and decision making.', 'However, although utilizing\\nLLM makes zero-shot navigation feasible, current methods\\nare designed for specific sub-task, which cannot transfer\\nto wider range of goal types. The recent InstructNav [25]\\nproposes a general framework to solve several language-\\nrelated navigation tasks with chain-of-thought, but it is still\\nunable to handle vision-related navigation like IIN. There-\\nfore, a uniform inference framework for universal zero-shot\\ngoal-oriented navigation is highly demanded. In this paper, we propose UniGoal to solve the above\\nproblems. Different from previous works which represent\\nscene and goal in text format and design task-specific work-\\nflow for LLM, we propose a uniform graph representation\\nfor both 3D scene and goal and formulate a general LLM-\\nbased scene exploration framework. With our graph-based\\nrepresentation, the 3D scene, object category, instance image\\nand text description can be uniformly represented with mini-\\nmal structural information loss compared to text description.', 'The consistent graph format between scene and goal also\\nenables accurate explicit reasoning including similarity com-\\nputation, graph matching and graph alignment. Specifically,\\nwe construct an online 3D scene graph along with the mov-\\ning of agent. At each time instant, we first conduct graph\\nmatching between the scene graph and goal graph. Then we\\npropose a multi-stage scene exploration policy, which adopts\\ndifferent strategies to generate long-term goal of exploration\\naccording to different matching states. With exploration\\nof unknown regions, the matching score will increase and\\nthe policy will progress between three stages: iterative sub-\\ngraph searching for zero matching, coordinate projection\\nand anchor pair alignment for partial matching, and scene\\ngraph correction and goal verification for perfect matching. To enable robust switch between stages, we also present a\\nblacklist mechanism to freeze unmatched parts of graphs and\\nencourage exploration to new regions.', 'Experimental results\\non several benchmarks of MatterPort3D [4], HM3D [30]\\nand RoboTHOR [10] show that UniGoal achieves superior\\nperformance on all three tasks with a single model, even out-\\nperforming zero-shot methods that designed for specific task\\nand universal methods that requires training or finetuning. 2. Related Work\\nZero-shot Navigation. Conventional supervised naviga-\\ntion methods [7, 11, 18, 31, 41] requires large-scale training\\nin simulation environments, which limits the generaliza-\\ntion ability. According to the goal type, zero-shot naviga-\\ntion can be mainly divided into ON [3, 46], IIN [16] and\\nTN [25]. Based on open-vocabulary CLIP [28], CoW [12]\\nconstructs zero-shot ON baseline using frontier-based explo-\\nration (FBE). ESC [49], OpenFMNav [8] and VLFM [45]\\nfurther extract common sense about correlations between\\nobjects using LLM for goal location reasoning. For zero-\\nshot IIN, Mod-IIN [16] simply utilizes FBE for exploration\\nand key point matching for goal identification.', 'For TN,\\ncurrently there are only supervised methods [6, 37] and zero-\\nshot ones are still missing. The recent InstructNav [25]\\nproposes a universal zero-shot framework for language-\\nrelated navigation tasks. It can be applied to ON, demand-\\ndriven navigation (DDN) and vision-language-navigation\\n(VLN) [9, 20, 26, 47, 48]. But it is still unable to handle\\nvisual goal as in IIN. Different from these approaches, our\\nUniGoal proposes a unified graph representation for goal\\nand scene, which can elegantly handle both language and\\nvision-related goal-oriented navigation tasks. Graph-based Scene Exploration. In order to better un-\\nderstand and explore the scene, there are a variety of scene\\nrepresentation methods. Among them, graph-based repre-\\nsentation [2, 13, 14, 33, 40] is one of the most popular and\\npromising representations based on explicit graph structure,\\nwhich shows great potential to be combined with LLM and\\nVLM for high-level reasoning.', 'SayPlan [32] leverages LLM\\nto perform task planning by searching on a 3D scene graph. OVSG [5] constructs an open-vocabulary scene graph for\\ncontext-aware descriptions and performs graph matching to\\nground the queried entity. There are also many works using\\ngraph representation for navigation tasks [24, 29, 42, 44]. Among them the most related work to ours is SG-Nav [44],\\nwhich constructs an online hierarchical scene graph and pro-\\nposes chain-of-thought prompting for LLM to reason the\\ngoal location based on graph sturcture. However, SG-Nav is\\nspecialized for ON, thus cannot fully exploit the rich infor-\\nmation contained in other types of goal. While our UniGoal\\nfurther represents the goal in a graph and perform graph\\nmatching between scene and goal to guide a multi-stage\\nscene exploration policy, which make full use of the correla-\\ntion between scene and goal for LLM reasoning. 3.', 'Approach\\nIn this section, we first provide the definition of universal\\nzero-shot goal-oriented navigation and introduce the frame-\\nwork of UniGoal. Then we construct graphs for scene and\\ngoal and conduct graph matching for goal identification. Next we design a multi-stage scene exploration policy by\\n\\nObservation\\nRGB-D\\nOccupancy Map\\nScene Graph\\nAgent Pose\\nGraph Matching\\nStage 1: Zero Matching\\nStage 2: Partial Matching\\nGoal Decomposition\\nStage 3: Perfect Matching\\nGraph Correction\\nGoal Verification\\nDeterministic \\nLocal Policy\\nAction\\nBlacklist\\nObject\\nImage\\nText\\nGoal Graph\\nGlobal Policy\\nSelect Frontier\\nFigure 2. Framework of UniGoal. We convert different types of goals into a uniform graph representation and maintain an online scene\\ngraph. At each step, we perform graph matching between the scene graph and goal graph, where the matching score will be utilized to\\nguide a multi-stage scene exploration policy.', 'For different degree of matching, our exploration policy leverages LLM to exploit the graphs\\nwith different aims: first expand observed area, then infer goal location based on the overlap of graphs, and finally verify the goal. We also\\npropose a blacklist that records unsuccessful matching to avoid repeated exploration. prompting LLM with graphs. Finally we propose a blacklist\\nmechanism to robustly avoid repeated exploration. 3.1. Goal-oriented Navigation\\nIn goal-oriented navigation, a mobile agent is tasked with\\nnavigating to a specified goal g in an unknown environment,\\nwhere g can be an object category (i.e. Object-goal Nav-\\nigation [7], ON), an image containing object that can be\\nfound in the scene (i.e. Instance-Image-goal Navigation [15],\\nIIN), or a description about a certain object (i.e. Text-goal\\nNavigation [37], TN).', 'For IIN and TN, there is a central\\nobject o as well as other relevant objects in g. While for\\nON, we have o = g. The agent receives posed RGB-D video\\nstream and is required to execute an action a ∈A at each\\ntime it receiving a new RGB-D observation. A is the set of\\nactions, which consists of move_forward, turn_left,\\nturn_right and stop. The task is successfully done if\\nthe agent stops within r meters of o in less than T steps. More details about the three sub-tasks can be found in sup-\\nplementary material. Task Specification. We aim to study the problem of\\nuniversal zero-shot goal-oriented navigation, which has two\\ncharacteristics: (1) Universal. We should design a general\\nmethod, which requires no modification when switching be-\\ntween the three sub-tasks. (2) Zero-shot. All three kinds of\\ngoal can be specified by free-form language or image. Our\\nnavigation method does not require any training or finetun-\\ning, which is of great generalization ability. Overview.', 'Universal zero-shot goal-oriented navigation\\nrequires the agent to complete different sub-tasks with a\\nsingle framework in training-free manner. Since this task\\nrequires extremely strong generalization ability, we utilize\\nlarge language model (LLM) [1, 39] for zero-shot decision\\nmaking by exploiting its rich knowledge and strong reason-\\ning ability. To make LLM aware of the visual observations\\nas well as unifying different kinds of goals, we propose to\\nrepresent the scene and goal in graphs, i.e., scene graph\\nand goal graph. In this way, different goals are represented\\nuniformly and the representations of scene and goal are con-\\nsistent. Based on this representation, we prompt LLM with\\nscene graph and goal graph for scene understanding, graph\\nmatching and decision making for exploration. The overall\\npipeline is illustrated in Figure 2. 3.2. Graph Construction and Matching\\nIn this subsection, we first describe how to construct a uni-\\nform graph representation for scene and goal.', 'Then we\\npropose a graph matching method to determine whether a\\ngoal or its relevant objects are observed, which further guides\\nthe selection of scene exploration policies. Graph Construction. We define graph G = (V, E) as a\\nset of nodes V connected with edges E. Each node represents\\nan object. Each edge represents the relationship between\\nobjects, which only exists between spatially or semantically\\nrelated object pairs. The content of nodes and edges is\\ndescribed in text format. Since the agent is initialized in\\nunknown environment and continuously explores the scene,\\nwe follow SG-Nav [44] to construct the scene graph Gt in-\\ncrementally by expanding it every time the agent receives\\na new RGB-D observation. For goal graph Gg, we adopt\\ndifferent methods to process three kinds of goals g into a\\ngraph, which we detail in supplementary material. Graph Matching. With scene graph Gt and goal graph\\nGg, we can match these two graphs to determine whether\\nthe goal is observed.', 'If no elements in Gg are observed, the\\nagent needs to infer relationship between objects from Gt\\nto plan a path that is most likely to find the goal. If Gg is\\npartially observed in Gt, the agent can use the overlapped\\npart of Gg and Gt to reason out where the rest of Gg is. If\\nGg is perfectly observed in Gt, the agent can move to the\\ngoal and make further verification. Therefore, a goal scoring\\nmethod is crucial for the follow-up scene exploration. We propose to apply graph matching to achieve this. Given Gt and Gg, we design three matching metrics, i.e.,\\nnode matching, edge matching and topology matching, to\\nscore how well the goal is observed.', 'Formally, for nodes\\nand edges, we extract their embeddings and then compute\\npair-wise similarity with bipartite matching to determine\\nmatched pairs of nodes and edges:\\nMN = B(thr(Embed(Vt) · Embed(Vg)T ))\\n(1)\\nME = B(thr(Embed(Et) · Embed(Eg)T ))\\n(2)\\nwhere Embed(·) ∈RK×C, K is the number of nodes or\\nedges and C is the channel dimension, which is detailed in\\nsupplementary material. thr(·) is an element-wise threshold\\nfunction applied on the similarity matrix which sets values\\nsmaller than τ to -1 to disable matching of the corresponding\\npairs. B(·) is bipartite matching, which outputs a list of all\\nmatched node or edge pairs, namely MN or ME. We also\\naverage the similarity matrix of nodes and edges to acquire\\nthe similarity scores SN and SE. Based on MN and ME, we further compute topological\\nsimilarity between Gt and Gg, which is defined as the graph\\nediting similarity between them:\\nST = 1 −D(S(F(Gt, MN, ME)), S(Gg))\\n(3)\\nwhere F(Gt, MN, ME) means the minimal subgraph of\\nGt with nodes in MN and edges in ME.', 'S(·) means the\\ntopological structure of a graph regardless of the content of\\nnodes and edges. D(·) is the normalized editing distance [34]\\nbetween two graphs. The final matching score is defined as\\nS = (SN + SE + ST )/3. 3.3. Multi-stage Scene Exploration\\nAs described above, different matching scores will lead to\\ndifferent scene exploration policies. From zero matching to\\nperfect matching, we design three stages to progressively\\nexplore the scene and generate long-term exploration goal. This long-term goal will be processed by a deterministic\\nlocal policy [36] to obtain actions. Below we detail our\\nexploration policy stage by stage. Stage 1: Zero Matching. If the matching score S is\\nsmaller than σ1, we regard this stage as zero matching. Since\\nthere is almost no element of Gg observed in Gt, the aim of\\nagent at this stage is to expand its explored region and find\\nelements in Gg.', 'Note this problem is similar to ON: before\\nobserving the goal, the agent needs to explore unknown\\nregions without any matching between goal and scene. We\\ncan simply resort to scene graph-based ON method [44]\\nat this stage, which navigates to frontiers with semantic\\nrelationships between the scene graph and goal as guidance. However, different from ON where the goal is always an\\nobject node, in our universal goal-oriented navigation the\\ngoal may be a complicated graph. A graph may consist of\\nseveral less related subgraph parts. For example, in a graph\\n(table, chair, window, curtain), [table, chair] and [window,\\ncurtain] are two subgraphs which have strong internal cor-\\nrelation but weak interrelation. We empirically observe that\\nlocating a collection of multiple unrelated subgraphs in a\\nscene at the same time is much more difficult than locating a\\nsingle subgraph once at a time. Therefore, we decompose Gg\\ninto multiple internally correlated subgraphs with the guid-\\nance of LLM.', 'For each subgraph of Gg, we convert it to a text\\ndescription, which is regarded as an object goal to call [44]\\nfor frontier selection. Finally, we select one frontier from the\\nproposed ones by averaging the frontier scores and distances\\nto the agent. Details about LLM-guided decomposition and\\nscore computation can be found in supplementary material. With this strategy, we not only utilize the information of\\nthe entire Gg, but also eliminate ambiguity during frontier\\nselection caused by unrelated subgraphs. Stage 2: Partial Matching. With the exploration of\\nagent, the elements of Gg will gradually be observed in\\nGt and thus S continues to increase. When S exceeds σ1\\n(but still smaller than σ2) and there is at least one anchor\\npair, we switch to partial matching stage. Here anchor pair\\nmeans a pair of exactly matched nodes, i.e., two unconnected\\nmatched nodes in MN or one matched edge in MM. Note that we store the world coordinates of nodes in Gt.', 'If we also know the relative coordinates of nodes in Gg, we\\ncan map the coordinates of Gt and Gg to bird’s-eye view\\n(BEV) and align the anchor pair of Gg to the one of Gt. In\\nthis way, after alignment we can directly infer where the rest\\nof Gg is, which provides the agent with clear exploration\\ngoal for each anchor pair. Luckily, although we do not have\\nany coordinate information about the goal, at least we are\\naware of the relative spatial relationship between nodes in\\nGg, such as a chair on the left of a table, a keyboard in front\\nof a monitor. Inspired by this, we propose a coordinate\\nprojection strategy that preserves spatial relationships. Given Gg without any coordinate information, we first\\nproject the central object node o to (0, 0) as an initialization. To infer the projected coordinates of other nodes, we need\\nto utilize the spatial relationship between coordinate-known\\nnodes and coordinate-unknown nodes.', 'Since at beginning\\nwe only have one coordinate-known node o, we start from it\\nand traverse the goal graph with Depth First Search (DFS) to\\n\\nAnchor Pair Alignment\\nCoordinate Projection\\nX\\nY\\nZ\\nright\\nin front of\\nright\\n①\\n②\\n③\\n④\\n⑤\\nDFS\\nin front of\\nCorrection\\n(𝑡−2)-th Propagation\\n(𝑡−1)- th Propagation\\n𝑡- th Propagation\\n(a)\\n(b)\\nLLM\\nScene Graph Correction\\nFigure 3. Illustration of approach. (a) Stage 2: coordinate projec-\\ntion and anchor pair alignment. (b) Stage 3: scene graph correction. gradually infer the projected BEV coordinates of the whole\\ngraph. During the traversal, we focus on the edge connecting\\nlast node (coordinate-known) and current node (coordinate-\\nunknown), which stores the spatial relationship. For each\\nedge, we prompt LLM with: [Current Node] is [Relation-\\nship] [Last Node], coordinate of [Last Node] is (x, y), the\\nX-axis and Y-axis are positive towards yourself and the right\\nrespectively. What is the coordinate of [Current Node]? to\\nproject the current coordinate-unknown node to BEV.']\n",
            "---------- summarize_Agent ----------\n",
            "Here is a short and precise summary of the paper:\n",
            "\n",
            "**Title:** UniGoal: Towards Universal Zero-shot Goal-oriented Navigation\n",
            "\n",
            "**Summary:** The paper proposes a universal framework for zero-shot goal-oriented navigation, which can handle different types of goals (object category, instance image, and text description) with a single model. The framework, called UniGoal, represents the scene and goal in a uniform graph format and uses large language models (LLM) for explicit graph-based reasoning. The method achieves state-of-the-art performance on three navigation tasks and outperforms task-specific zero-shot methods and supervised universal methods.\n",
            "\n",
            "**Key Contributions:**\n",
            "\n",
            "1. Uniform graph representation for scene and goal.\n",
            "2. Graph matching and alignment for goal identification.\n",
            "3. Multi-stage scene exploration policy with LLM-based reasoning.\n",
            "4. Blacklist mechanism for robust exploration.\n",
            "\n",
            "**Benefits:** UniGoal provides a flexible and generalizable approach to goal-oriented navigation, which can handle various types of goals and scenes without requiring training or fine-tuning.\n",
            "---------- Arxiv_Search_Agent ----------\n",
            "That's a great summary! You've effectively condensed the main points of the paper into a concise and clear summary. The key contributions and benefits are also well-highlighted, making it easy to understand the significance of the UniGoal framework. Well done! \n",
            "\n",
            "The summary effectively captures the essence of the paper, including the proposed framework, its components, and its advantages. It's a great example of how to distill complex research into a concise and accessible summary.\n",
            "---------- summarize_Agent ----------\n",
            "I'm glad I could effectively summarize the paper and highlight the key points of the UniGoal framework. It's a challenging task to condense complex research into a concise summary, but it's also a great opportunity to help make the information more accessible to a wider audience. If you have any more papers you'd like me to summarize, feel free to ask!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TaskResult(messages=[TextMessage(source='user', models_usage=None, metadata={}, content='give me the summarization of the given paper', type='TextMessage'), ToolCallRequestEvent(source='Arxiv_Search_Agent', models_usage=RequestUsage(prompt_tokens=231, completion_tokens=10), metadata={}, content=[FunctionCall(id='call_rzjq', arguments='{}', name='arxiv_search')], type='ToolCallRequestEvent'), ToolCallExecutionEvent(source='Arxiv_Search_Agent', models_usage=None, metadata={}, content=[FunctionExecutionResult(content=\"[' UniGoal: Towards Universal Zero-shot Goal-oriented Navigation\\\\nHang Yin1*, Xiuwei Xu1∗†, Linqing Zhao1, Ziwei Wang2, Jie Zhou1, Jiwen Lu1‡\\\\n1Tsinghua University\\\\n2Nanyang Technological University\\\\n{yinh23, xxw21, zhaolinqing}@mails.tsinghua.edu.cn;\\\\nziwei.wang@ntu.edu.sg; {jzhou, lujiwen}@tsinghua.edu.cn\\\\nAbstract\\\\nIn this paper, we propose a general framework for univer-\\\\nsal zero-shot goal-oriented navigation. Existing zero-shot\\\\nmethods build inference framework upon large language\\\\nmodels (LLM) for specific tasks, which differs a lot in over-\\\\nall pipeline and fails to generalize across different types of\\\\ngoal. Towards the aim of universal zero-shot navigation, we\\\\npropose a uniform graph representation to unify different\\\\ngoals, including object category, instance image and text\\\\ndescription. We also convert the observation of agent into an\\\\nonline maintained scene graph.', 'With this consistent scene\\\\nand goal representation, we preserve most structural infor-\\\\nmation compared with pure text and are able to leverage\\\\nLLM for explicit graph-based reasoning. Specifically, we\\\\nconduct graph matching between the scene graph and goal\\\\ngraph at each time instant and propose different strategies\\\\nto generate long-term goal of exploration according to dif-\\\\nferent matching states. The agent first iteratively searches\\\\nsubgraph of goal when zero-matched. With partial matching,\\\\nthe agent then utilizes coordinate projection and anchor pair\\\\nalignment to infer the goal location. Finally scene graph cor-\\\\nrection and goal verification are applied for perfect match-\\\\ning. We also present a blacklist mechanism to enable robust\\\\nswitch between stages. Extensive experiments on several\\\\nbenchmarks show that our UniGoal achieves state-of-the-art\\\\nzero-shot performance on three studied navigation tasks with\\\\na single model, even outperforming task-specific zero-shot\\\\nmethods and supervised universal methods.', 'Project Page. 1. Introduction\\\\nGoal-oriented navigation is a fundamental problem in var-\\\\nious robotic tasks, which requires the agent to navigate to\\\\na specified goal in an unknown environment. Depending\\\\non the goal type, there are many popular sub-tasks of goal-\\\\noriented navigation, among which we focus on three rep-\\\\n* Equal contribution. † Project lead. ‡ Corresponding author. UniGoal (Ours)\\\\nBed\\\\nIt is a chair. In front of it, \\\\nthere is a coffee table. Two \\\\nwine glasses and one \\\\ntransparent glass bottle \\\\nare in the upper left corner. Scene Graph\\\\nGoal Graph\\\\nText Goal\\\\nObject Goal\\\\nIns-Image Goal\\\\n50.6\\\\n54.0\\\\n54.5\\\\nUniversal methods:\\\\nUniGoal (Ours)\\\\nGOAT (Supervised)\\\\nZero-shot methods:\\\\nSG-Nav\\\\nMod-IIN\\\\n37.4\\\\n56.1\\\\n60.2\\\\n17.0\\\\n20.2\\\\nInstance Image-goal\\\\nText-goal\\\\nObject-goal\\\\nMatching\\\\n&\\\\nExploration\\\\nFigure 1. State-of-the-art zero-shot goal-oriented navigation meth-\\\\nods are typically specialized for each goal type.', 'Although recent\\\\nwork presents universal goal-oriented navigation method, it requires\\\\nto train policy networks on large-scale data and lacks zero-shot gen-\\\\neralization ability. We propose UniGoal, which enables zero-shot\\\\ninference on three studied navigation tasks with a unified frame-\\\\nwork and achieves leading performance on multiple benchmarks. resentative types: object category, instance image and text\\\\ndescription. These sub-tasks are also known as Object-goal\\\\nNavigation (ON) [7, 44, 49], Instance-image-goal Naviga-\\\\ntion (IIN) [15, 16] and Text-goal Navigation (TN) [37]. With\\\\nthe development of deep learning, reinforcement learning\\\\n(RL) and vision/language foundation models, we have wit-\\\\nnessed great achievement of performance on each individual\\\\nsub-task. However, in actual application scenarios, the high\\\\nflexibility of human instructions requires high versatility of\\\\nagent. Therefore, a universal method that can handle all\\\\nsub-tasks in a single model is of great desire.', 'Towards the aim of universal goal-oriented navigation, a\\\\nnatural solution is to learn a uniform representation of differ-\\\\nent kinds of goals. GOAT [6] trains a universal global policy\\\\narXiv:2503.10630v1  [cs.CV]  13 Mar 2025\\\\n\\\\non the three goal-oriented sub-tasks, which learns a shared\\\\ngoal embedding with RL. To reduce the requirement of train-\\\\ning resources, PSL [37] utilizes CLIP [28] embedding to\\\\nuniformly represent category, image and text description. But it still requires time-consuming RL training for the pol-\\\\nicy networks. Moreover, these training-based methods tend\\\\nto overfit on the simulation environment and thus show weak\\\\ngeneralization ability when applied to real world. To solve\\\\nabove limitations, zero-shot navigation methods [44, 49]\\\\nappear to be an ideal choice, where the agent does not re-\\\\nquire any training or finetuning when deployed to a certain\\\\ntask. The mainstream solution of zero-shot methods is to\\\\nleverage large language models (LLM) [1, 39] for general\\\\nreasoning and decision making.', 'However, although utilizing\\\\nLLM makes zero-shot navigation feasible, current methods\\\\nare designed for specific sub-task, which cannot transfer\\\\nto wider range of goal types. The recent InstructNav [25]\\\\nproposes a general framework to solve several language-\\\\nrelated navigation tasks with chain-of-thought, but it is still\\\\nunable to handle vision-related navigation like IIN. There-\\\\nfore, a uniform inference framework for universal zero-shot\\\\ngoal-oriented navigation is highly demanded. In this paper, we propose UniGoal to solve the above\\\\nproblems. Different from previous works which represent\\\\nscene and goal in text format and design task-specific work-\\\\nflow for LLM, we propose a uniform graph representation\\\\nfor both 3D scene and goal and formulate a general LLM-\\\\nbased scene exploration framework. With our graph-based\\\\nrepresentation, the 3D scene, object category, instance image\\\\nand text description can be uniformly represented with mini-\\\\nmal structural information loss compared to text description.', 'The consistent graph format between scene and goal also\\\\nenables accurate explicit reasoning including similarity com-\\\\nputation, graph matching and graph alignment. Specifically,\\\\nwe construct an online 3D scene graph along with the mov-\\\\ning of agent. At each time instant, we first conduct graph\\\\nmatching between the scene graph and goal graph. Then we\\\\npropose a multi-stage scene exploration policy, which adopts\\\\ndifferent strategies to generate long-term goal of exploration\\\\naccording to different matching states. With exploration\\\\nof unknown regions, the matching score will increase and\\\\nthe policy will progress between three stages: iterative sub-\\\\ngraph searching for zero matching, coordinate projection\\\\nand anchor pair alignment for partial matching, and scene\\\\ngraph correction and goal verification for perfect matching. To enable robust switch between stages, we also present a\\\\nblacklist mechanism to freeze unmatched parts of graphs and\\\\nencourage exploration to new regions.', 'Experimental results\\\\non several benchmarks of MatterPort3D [4], HM3D [30]\\\\nand RoboTHOR [10] show that UniGoal achieves superior\\\\nperformance on all three tasks with a single model, even out-\\\\nperforming zero-shot methods that designed for specific task\\\\nand universal methods that requires training or finetuning. 2. Related Work\\\\nZero-shot Navigation. Conventional supervised naviga-\\\\ntion methods [7, 11, 18, 31, 41] requires large-scale training\\\\nin simulation environments, which limits the generaliza-\\\\ntion ability. According to the goal type, zero-shot naviga-\\\\ntion can be mainly divided into ON [3, 46], IIN [16] and\\\\nTN [25]. Based on open-vocabulary CLIP [28], CoW [12]\\\\nconstructs zero-shot ON baseline using frontier-based explo-\\\\nration (FBE). ESC [49], OpenFMNav [8] and VLFM [45]\\\\nfurther extract common sense about correlations between\\\\nobjects using LLM for goal location reasoning. For zero-\\\\nshot IIN, Mod-IIN [16] simply utilizes FBE for exploration\\\\nand key point matching for goal identification.', 'For TN,\\\\ncurrently there are only supervised methods [6, 37] and zero-\\\\nshot ones are still missing. The recent InstructNav [25]\\\\nproposes a universal zero-shot framework for language-\\\\nrelated navigation tasks. It can be applied to ON, demand-\\\\ndriven navigation (DDN) and vision-language-navigation\\\\n(VLN) [9, 20, 26, 47, 48]. But it is still unable to handle\\\\nvisual goal as in IIN. Different from these approaches, our\\\\nUniGoal proposes a unified graph representation for goal\\\\nand scene, which can elegantly handle both language and\\\\nvision-related goal-oriented navigation tasks. Graph-based Scene Exploration. In order to better un-\\\\nderstand and explore the scene, there are a variety of scene\\\\nrepresentation methods. Among them, graph-based repre-\\\\nsentation [2, 13, 14, 33, 40] is one of the most popular and\\\\npromising representations based on explicit graph structure,\\\\nwhich shows great potential to be combined with LLM and\\\\nVLM for high-level reasoning.', 'SayPlan [32] leverages LLM\\\\nto perform task planning by searching on a 3D scene graph. OVSG [5] constructs an open-vocabulary scene graph for\\\\ncontext-aware descriptions and performs graph matching to\\\\nground the queried entity. There are also many works using\\\\ngraph representation for navigation tasks [24, 29, 42, 44]. Among them the most related work to ours is SG-Nav [44],\\\\nwhich constructs an online hierarchical scene graph and pro-\\\\nposes chain-of-thought prompting for LLM to reason the\\\\ngoal location based on graph sturcture. However, SG-Nav is\\\\nspecialized for ON, thus cannot fully exploit the rich infor-\\\\nmation contained in other types of goal. While our UniGoal\\\\nfurther represents the goal in a graph and perform graph\\\\nmatching between scene and goal to guide a multi-stage\\\\nscene exploration policy, which make full use of the correla-\\\\ntion between scene and goal for LLM reasoning. 3.', 'Approach\\\\nIn this section, we first provide the definition of universal\\\\nzero-shot goal-oriented navigation and introduce the frame-\\\\nwork of UniGoal. Then we construct graphs for scene and\\\\ngoal and conduct graph matching for goal identification. Next we design a multi-stage scene exploration policy by\\\\n\\\\nObservation\\\\nRGB-D\\\\nOccupancy Map\\\\nScene Graph\\\\nAgent Pose\\\\nGraph Matching\\\\nStage 1: Zero Matching\\\\nStage 2: Partial Matching\\\\nGoal Decomposition\\\\nStage 3: Perfect Matching\\\\nGraph Correction\\\\nGoal Verification\\\\nDeterministic \\\\nLocal Policy\\\\nAction\\\\nBlacklist\\\\nObject\\\\nImage\\\\nText\\\\nGoal Graph\\\\nGlobal Policy\\\\nSelect Frontier\\\\nFigure 2. Framework of UniGoal. We convert different types of goals into a uniform graph representation and maintain an online scene\\\\ngraph. At each step, we perform graph matching between the scene graph and goal graph, where the matching score will be utilized to\\\\nguide a multi-stage scene exploration policy.', 'For different degree of matching, our exploration policy leverages LLM to exploit the graphs\\\\nwith different aims: first expand observed area, then infer goal location based on the overlap of graphs, and finally verify the goal. We also\\\\npropose a blacklist that records unsuccessful matching to avoid repeated exploration. prompting LLM with graphs. Finally we propose a blacklist\\\\nmechanism to robustly avoid repeated exploration. 3.1. Goal-oriented Navigation\\\\nIn goal-oriented navigation, a mobile agent is tasked with\\\\nnavigating to a specified goal g in an unknown environment,\\\\nwhere g can be an object category (i.e. Object-goal Nav-\\\\nigation [7], ON), an image containing object that can be\\\\nfound in the scene (i.e. Instance-Image-goal Navigation [15],\\\\nIIN), or a description about a certain object (i.e. Text-goal\\\\nNavigation [37], TN).', 'For IIN and TN, there is a central\\\\nobject o as well as other relevant objects in g. While for\\\\nON, we have o = g. The agent receives posed RGB-D video\\\\nstream and is required to execute an action a ∈A at each\\\\ntime it receiving a new RGB-D observation. A is the set of\\\\nactions, which consists of move_forward, turn_left,\\\\nturn_right and stop. The task is successfully done if\\\\nthe agent stops within r meters of o in less than T steps. More details about the three sub-tasks can be found in sup-\\\\nplementary material. Task Specification. We aim to study the problem of\\\\nuniversal zero-shot goal-oriented navigation, which has two\\\\ncharacteristics: (1) Universal. We should design a general\\\\nmethod, which requires no modification when switching be-\\\\ntween the three sub-tasks. (2) Zero-shot. All three kinds of\\\\ngoal can be specified by free-form language or image. Our\\\\nnavigation method does not require any training or finetun-\\\\ning, which is of great generalization ability. Overview.', 'Universal zero-shot goal-oriented navigation\\\\nrequires the agent to complete different sub-tasks with a\\\\nsingle framework in training-free manner. Since this task\\\\nrequires extremely strong generalization ability, we utilize\\\\nlarge language model (LLM) [1, 39] for zero-shot decision\\\\nmaking by exploiting its rich knowledge and strong reason-\\\\ning ability. To make LLM aware of the visual observations\\\\nas well as unifying different kinds of goals, we propose to\\\\nrepresent the scene and goal in graphs, i.e., scene graph\\\\nand goal graph. In this way, different goals are represented\\\\nuniformly and the representations of scene and goal are con-\\\\nsistent. Based on this representation, we prompt LLM with\\\\nscene graph and goal graph for scene understanding, graph\\\\nmatching and decision making for exploration. The overall\\\\npipeline is illustrated in Figure 2. 3.2. Graph Construction and Matching\\\\nIn this subsection, we first describe how to construct a uni-\\\\nform graph representation for scene and goal.', 'Then we\\\\npropose a graph matching method to determine whether a\\\\ngoal or its relevant objects are observed, which further guides\\\\nthe selection of scene exploration policies. Graph Construction. We define graph G = (V, E) as a\\\\nset of nodes V connected with edges E. Each node represents\\\\nan object. Each edge represents the relationship between\\\\nobjects, which only exists between spatially or semantically\\\\nrelated object pairs. The content of nodes and edges is\\\\ndescribed in text format. Since the agent is initialized in\\\\nunknown environment and continuously explores the scene,\\\\nwe follow SG-Nav [44] to construct the scene graph Gt in-\\\\ncrementally by expanding it every time the agent receives\\\\na new RGB-D observation. For goal graph Gg, we adopt\\\\ndifferent methods to process three kinds of goals g into a\\\\ngraph, which we detail in supplementary material. Graph Matching. With scene graph Gt and goal graph\\\\nGg, we can match these two graphs to determine whether\\\\nthe goal is observed.', 'If no elements in Gg are observed, the\\\\nagent needs to infer relationship between objects from Gt\\\\nto plan a path that is most likely to find the goal. If Gg is\\\\npartially observed in Gt, the agent can use the overlapped\\\\npart of Gg and Gt to reason out where the rest of Gg is. If\\\\nGg is perfectly observed in Gt, the agent can move to the\\\\ngoal and make further verification. Therefore, a goal scoring\\\\nmethod is crucial for the follow-up scene exploration. We propose to apply graph matching to achieve this. Given Gt and Gg, we design three matching metrics, i.e.,\\\\nnode matching, edge matching and topology matching, to\\\\nscore how well the goal is observed.', 'Formally, for nodes\\\\nand edges, we extract their embeddings and then compute\\\\npair-wise similarity with bipartite matching to determine\\\\nmatched pairs of nodes and edges:\\\\nMN = B(thr(Embed(Vt) · Embed(Vg)T ))\\\\n(1)\\\\nME = B(thr(Embed(Et) · Embed(Eg)T ))\\\\n(2)\\\\nwhere Embed(·) ∈RK×C, K is the number of nodes or\\\\nedges and C is the channel dimension, which is detailed in\\\\nsupplementary material. thr(·) is an element-wise threshold\\\\nfunction applied on the similarity matrix which sets values\\\\nsmaller than τ to -1 to disable matching of the corresponding\\\\npairs. B(·) is bipartite matching, which outputs a list of all\\\\nmatched node or edge pairs, namely MN or ME. We also\\\\naverage the similarity matrix of nodes and edges to acquire\\\\nthe similarity scores SN and SE. Based on MN and ME, we further compute topological\\\\nsimilarity between Gt and Gg, which is defined as the graph\\\\nediting similarity between them:\\\\nST = 1 −D(S(F(Gt, MN, ME)), S(Gg))\\\\n(3)\\\\nwhere F(Gt, MN, ME) means the minimal subgraph of\\\\nGt with nodes in MN and edges in ME.', 'S(·) means the\\\\ntopological structure of a graph regardless of the content of\\\\nnodes and edges. D(·) is the normalized editing distance [34]\\\\nbetween two graphs. The final matching score is defined as\\\\nS = (SN + SE + ST )/3. 3.3. Multi-stage Scene Exploration\\\\nAs described above, different matching scores will lead to\\\\ndifferent scene exploration policies. From zero matching to\\\\nperfect matching, we design three stages to progressively\\\\nexplore the scene and generate long-term exploration goal. This long-term goal will be processed by a deterministic\\\\nlocal policy [36] to obtain actions. Below we detail our\\\\nexploration policy stage by stage. Stage 1: Zero Matching. If the matching score S is\\\\nsmaller than σ1, we regard this stage as zero matching. Since\\\\nthere is almost no element of Gg observed in Gt, the aim of\\\\nagent at this stage is to expand its explored region and find\\\\nelements in Gg.', 'Note this problem is similar to ON: before\\\\nobserving the goal, the agent needs to explore unknown\\\\nregions without any matching between goal and scene. We\\\\ncan simply resort to scene graph-based ON method [44]\\\\nat this stage, which navigates to frontiers with semantic\\\\nrelationships between the scene graph and goal as guidance. However, different from ON where the goal is always an\\\\nobject node, in our universal goal-oriented navigation the\\\\ngoal may be a complicated graph. A graph may consist of\\\\nseveral less related subgraph parts. For example, in a graph\\\\n(table, chair, window, curtain), [table, chair] and [window,\\\\ncurtain] are two subgraphs which have strong internal cor-\\\\nrelation but weak interrelation. We empirically observe that\\\\nlocating a collection of multiple unrelated subgraphs in a\\\\nscene at the same time is much more difficult than locating a\\\\nsingle subgraph once at a time. Therefore, we decompose Gg\\\\ninto multiple internally correlated subgraphs with the guid-\\\\nance of LLM.', 'For each subgraph of Gg, we convert it to a text\\\\ndescription, which is regarded as an object goal to call [44]\\\\nfor frontier selection. Finally, we select one frontier from the\\\\nproposed ones by averaging the frontier scores and distances\\\\nto the agent. Details about LLM-guided decomposition and\\\\nscore computation can be found in supplementary material. With this strategy, we not only utilize the information of\\\\nthe entire Gg, but also eliminate ambiguity during frontier\\\\nselection caused by unrelated subgraphs. Stage 2: Partial Matching. With the exploration of\\\\nagent, the elements of Gg will gradually be observed in\\\\nGt and thus S continues to increase. When S exceeds σ1\\\\n(but still smaller than σ2) and there is at least one anchor\\\\npair, we switch to partial matching stage. Here anchor pair\\\\nmeans a pair of exactly matched nodes, i.e., two unconnected\\\\nmatched nodes in MN or one matched edge in MM. Note that we store the world coordinates of nodes in Gt.', 'If we also know the relative coordinates of nodes in Gg, we\\\\ncan map the coordinates of Gt and Gg to bird’s-eye view\\\\n(BEV) and align the anchor pair of Gg to the one of Gt. In\\\\nthis way, after alignment we can directly infer where the rest\\\\nof Gg is, which provides the agent with clear exploration\\\\ngoal for each anchor pair. Luckily, although we do not have\\\\nany coordinate information about the goal, at least we are\\\\naware of the relative spatial relationship between nodes in\\\\nGg, such as a chair on the left of a table, a keyboard in front\\\\nof a monitor. Inspired by this, we propose a coordinate\\\\nprojection strategy that preserves spatial relationships. Given Gg without any coordinate information, we first\\\\nproject the central object node o to (0, 0) as an initialization. To infer the projected coordinates of other nodes, we need\\\\nto utilize the spatial relationship between coordinate-known\\\\nnodes and coordinate-unknown nodes.', 'Since at beginning\\\\nwe only have one coordinate-known node o, we start from it\\\\nand traverse the goal graph with Depth First Search (DFS) to\\\\n\\\\nAnchor Pair Alignment\\\\nCoordinate Projection\\\\nX\\\\nY\\\\nZ\\\\nright\\\\nin front of\\\\nright\\\\n①\\\\n②\\\\n③\\\\n④\\\\n⑤\\\\nDFS\\\\nin front of\\\\nCorrection\\\\n(𝑡−2)-th Propagation\\\\n(𝑡−1)- th Propagation\\\\n𝑡- th Propagation\\\\n(a)\\\\n(b)\\\\nLLM\\\\nScene Graph Correction\\\\nFigure 3. Illustration of approach. (a) Stage 2: coordinate projec-\\\\ntion and anchor pair alignment. (b) Stage 3: scene graph correction. gradually infer the projected BEV coordinates of the whole\\\\ngraph. During the traversal, we focus on the edge connecting\\\\nlast node (coordinate-known) and current node (coordinate-\\\\nunknown), which stores the spatial relationship. For each\\\\nedge, we prompt LLM with: [Current Node] is [Relation-\\\\nship] [Last Node], coordinate of [Last Node] is (x, y), the\\\\nX-axis and Y-axis are positive towards yourself and the right\\\\nrespectively. What is the coordinate of [Current Node]? to\\\\nproject the current coordinate-unknown node to BEV.']\", name='arxiv_search', call_id='call_rzjq', is_error=False)], type='ToolCallExecutionEvent'), ToolCallSummaryMessage(source='Arxiv_Search_Agent', models_usage=None, metadata={}, content=\"[' UniGoal: Towards Universal Zero-shot Goal-oriented Navigation\\\\nHang Yin1*, Xiuwei Xu1∗†, Linqing Zhao1, Ziwei Wang2, Jie Zhou1, Jiwen Lu1‡\\\\n1Tsinghua University\\\\n2Nanyang Technological University\\\\n{yinh23, xxw21, zhaolinqing}@mails.tsinghua.edu.cn;\\\\nziwei.wang@ntu.edu.sg; {jzhou, lujiwen}@tsinghua.edu.cn\\\\nAbstract\\\\nIn this paper, we propose a general framework for univer-\\\\nsal zero-shot goal-oriented navigation. Existing zero-shot\\\\nmethods build inference framework upon large language\\\\nmodels (LLM) for specific tasks, which differs a lot in over-\\\\nall pipeline and fails to generalize across different types of\\\\ngoal. Towards the aim of universal zero-shot navigation, we\\\\npropose a uniform graph representation to unify different\\\\ngoals, including object category, instance image and text\\\\ndescription. We also convert the observation of agent into an\\\\nonline maintained scene graph.', 'With this consistent scene\\\\nand goal representation, we preserve most structural infor-\\\\nmation compared with pure text and are able to leverage\\\\nLLM for explicit graph-based reasoning. Specifically, we\\\\nconduct graph matching between the scene graph and goal\\\\ngraph at each time instant and propose different strategies\\\\nto generate long-term goal of exploration according to dif-\\\\nferent matching states. The agent first iteratively searches\\\\nsubgraph of goal when zero-matched. With partial matching,\\\\nthe agent then utilizes coordinate projection and anchor pair\\\\nalignment to infer the goal location. Finally scene graph cor-\\\\nrection and goal verification are applied for perfect match-\\\\ning. We also present a blacklist mechanism to enable robust\\\\nswitch between stages. Extensive experiments on several\\\\nbenchmarks show that our UniGoal achieves state-of-the-art\\\\nzero-shot performance on three studied navigation tasks with\\\\na single model, even outperforming task-specific zero-shot\\\\nmethods and supervised universal methods.', 'Project Page. 1. Introduction\\\\nGoal-oriented navigation is a fundamental problem in var-\\\\nious robotic tasks, which requires the agent to navigate to\\\\na specified goal in an unknown environment. Depending\\\\non the goal type, there are many popular sub-tasks of goal-\\\\noriented navigation, among which we focus on three rep-\\\\n* Equal contribution. † Project lead. ‡ Corresponding author. UniGoal (Ours)\\\\nBed\\\\nIt is a chair. In front of it, \\\\nthere is a coffee table. Two \\\\nwine glasses and one \\\\ntransparent glass bottle \\\\nare in the upper left corner. Scene Graph\\\\nGoal Graph\\\\nText Goal\\\\nObject Goal\\\\nIns-Image Goal\\\\n50.6\\\\n54.0\\\\n54.5\\\\nUniversal methods:\\\\nUniGoal (Ours)\\\\nGOAT (Supervised)\\\\nZero-shot methods:\\\\nSG-Nav\\\\nMod-IIN\\\\n37.4\\\\n56.1\\\\n60.2\\\\n17.0\\\\n20.2\\\\nInstance Image-goal\\\\nText-goal\\\\nObject-goal\\\\nMatching\\\\n&\\\\nExploration\\\\nFigure 1. State-of-the-art zero-shot goal-oriented navigation meth-\\\\nods are typically specialized for each goal type.', 'Although recent\\\\nwork presents universal goal-oriented navigation method, it requires\\\\nto train policy networks on large-scale data and lacks zero-shot gen-\\\\neralization ability. We propose UniGoal, which enables zero-shot\\\\ninference on three studied navigation tasks with a unified frame-\\\\nwork and achieves leading performance on multiple benchmarks. resentative types: object category, instance image and text\\\\ndescription. These sub-tasks are also known as Object-goal\\\\nNavigation (ON) [7, 44, 49], Instance-image-goal Naviga-\\\\ntion (IIN) [15, 16] and Text-goal Navigation (TN) [37]. With\\\\nthe development of deep learning, reinforcement learning\\\\n(RL) and vision/language foundation models, we have wit-\\\\nnessed great achievement of performance on each individual\\\\nsub-task. However, in actual application scenarios, the high\\\\nflexibility of human instructions requires high versatility of\\\\nagent. Therefore, a universal method that can handle all\\\\nsub-tasks in a single model is of great desire.', 'Towards the aim of universal goal-oriented navigation, a\\\\nnatural solution is to learn a uniform representation of differ-\\\\nent kinds of goals. GOAT [6] trains a universal global policy\\\\narXiv:2503.10630v1  [cs.CV]  13 Mar 2025\\\\n\\\\non the three goal-oriented sub-tasks, which learns a shared\\\\ngoal embedding with RL. To reduce the requirement of train-\\\\ning resources, PSL [37] utilizes CLIP [28] embedding to\\\\nuniformly represent category, image and text description. But it still requires time-consuming RL training for the pol-\\\\nicy networks. Moreover, these training-based methods tend\\\\nto overfit on the simulation environment and thus show weak\\\\ngeneralization ability when applied to real world. To solve\\\\nabove limitations, zero-shot navigation methods [44, 49]\\\\nappear to be an ideal choice, where the agent does not re-\\\\nquire any training or finetuning when deployed to a certain\\\\ntask. The mainstream solution of zero-shot methods is to\\\\nleverage large language models (LLM) [1, 39] for general\\\\nreasoning and decision making.', 'However, although utilizing\\\\nLLM makes zero-shot navigation feasible, current methods\\\\nare designed for specific sub-task, which cannot transfer\\\\nto wider range of goal types. The recent InstructNav [25]\\\\nproposes a general framework to solve several language-\\\\nrelated navigation tasks with chain-of-thought, but it is still\\\\nunable to handle vision-related navigation like IIN. There-\\\\nfore, a uniform inference framework for universal zero-shot\\\\ngoal-oriented navigation is highly demanded. In this paper, we propose UniGoal to solve the above\\\\nproblems. Different from previous works which represent\\\\nscene and goal in text format and design task-specific work-\\\\nflow for LLM, we propose a uniform graph representation\\\\nfor both 3D scene and goal and formulate a general LLM-\\\\nbased scene exploration framework. With our graph-based\\\\nrepresentation, the 3D scene, object category, instance image\\\\nand text description can be uniformly represented with mini-\\\\nmal structural information loss compared to text description.', 'The consistent graph format between scene and goal also\\\\nenables accurate explicit reasoning including similarity com-\\\\nputation, graph matching and graph alignment. Specifically,\\\\nwe construct an online 3D scene graph along with the mov-\\\\ning of agent. At each time instant, we first conduct graph\\\\nmatching between the scene graph and goal graph. Then we\\\\npropose a multi-stage scene exploration policy, which adopts\\\\ndifferent strategies to generate long-term goal of exploration\\\\naccording to different matching states. With exploration\\\\nof unknown regions, the matching score will increase and\\\\nthe policy will progress between three stages: iterative sub-\\\\ngraph searching for zero matching, coordinate projection\\\\nand anchor pair alignment for partial matching, and scene\\\\ngraph correction and goal verification for perfect matching. To enable robust switch between stages, we also present a\\\\nblacklist mechanism to freeze unmatched parts of graphs and\\\\nencourage exploration to new regions.', 'Experimental results\\\\non several benchmarks of MatterPort3D [4], HM3D [30]\\\\nand RoboTHOR [10] show that UniGoal achieves superior\\\\nperformance on all three tasks with a single model, even out-\\\\nperforming zero-shot methods that designed for specific task\\\\nand universal methods that requires training or finetuning. 2. Related Work\\\\nZero-shot Navigation. Conventional supervised naviga-\\\\ntion methods [7, 11, 18, 31, 41] requires large-scale training\\\\nin simulation environments, which limits the generaliza-\\\\ntion ability. According to the goal type, zero-shot naviga-\\\\ntion can be mainly divided into ON [3, 46], IIN [16] and\\\\nTN [25]. Based on open-vocabulary CLIP [28], CoW [12]\\\\nconstructs zero-shot ON baseline using frontier-based explo-\\\\nration (FBE). ESC [49], OpenFMNav [8] and VLFM [45]\\\\nfurther extract common sense about correlations between\\\\nobjects using LLM for goal location reasoning. For zero-\\\\nshot IIN, Mod-IIN [16] simply utilizes FBE for exploration\\\\nand key point matching for goal identification.', 'For TN,\\\\ncurrently there are only supervised methods [6, 37] and zero-\\\\nshot ones are still missing. The recent InstructNav [25]\\\\nproposes a universal zero-shot framework for language-\\\\nrelated navigation tasks. It can be applied to ON, demand-\\\\ndriven navigation (DDN) and vision-language-navigation\\\\n(VLN) [9, 20, 26, 47, 48]. But it is still unable to handle\\\\nvisual goal as in IIN. Different from these approaches, our\\\\nUniGoal proposes a unified graph representation for goal\\\\nand scene, which can elegantly handle both language and\\\\nvision-related goal-oriented navigation tasks. Graph-based Scene Exploration. In order to better un-\\\\nderstand and explore the scene, there are a variety of scene\\\\nrepresentation methods. Among them, graph-based repre-\\\\nsentation [2, 13, 14, 33, 40] is one of the most popular and\\\\npromising representations based on explicit graph structure,\\\\nwhich shows great potential to be combined with LLM and\\\\nVLM for high-level reasoning.', 'SayPlan [32] leverages LLM\\\\nto perform task planning by searching on a 3D scene graph. OVSG [5] constructs an open-vocabulary scene graph for\\\\ncontext-aware descriptions and performs graph matching to\\\\nground the queried entity. There are also many works using\\\\ngraph representation for navigation tasks [24, 29, 42, 44]. Among them the most related work to ours is SG-Nav [44],\\\\nwhich constructs an online hierarchical scene graph and pro-\\\\nposes chain-of-thought prompting for LLM to reason the\\\\ngoal location based on graph sturcture. However, SG-Nav is\\\\nspecialized for ON, thus cannot fully exploit the rich infor-\\\\nmation contained in other types of goal. While our UniGoal\\\\nfurther represents the goal in a graph and perform graph\\\\nmatching between scene and goal to guide a multi-stage\\\\nscene exploration policy, which make full use of the correla-\\\\ntion between scene and goal for LLM reasoning. 3.', 'Approach\\\\nIn this section, we first provide the definition of universal\\\\nzero-shot goal-oriented navigation and introduce the frame-\\\\nwork of UniGoal. Then we construct graphs for scene and\\\\ngoal and conduct graph matching for goal identification. Next we design a multi-stage scene exploration policy by\\\\n\\\\nObservation\\\\nRGB-D\\\\nOccupancy Map\\\\nScene Graph\\\\nAgent Pose\\\\nGraph Matching\\\\nStage 1: Zero Matching\\\\nStage 2: Partial Matching\\\\nGoal Decomposition\\\\nStage 3: Perfect Matching\\\\nGraph Correction\\\\nGoal Verification\\\\nDeterministic \\\\nLocal Policy\\\\nAction\\\\nBlacklist\\\\nObject\\\\nImage\\\\nText\\\\nGoal Graph\\\\nGlobal Policy\\\\nSelect Frontier\\\\nFigure 2. Framework of UniGoal. We convert different types of goals into a uniform graph representation and maintain an online scene\\\\ngraph. At each step, we perform graph matching between the scene graph and goal graph, where the matching score will be utilized to\\\\nguide a multi-stage scene exploration policy.', 'For different degree of matching, our exploration policy leverages LLM to exploit the graphs\\\\nwith different aims: first expand observed area, then infer goal location based on the overlap of graphs, and finally verify the goal. We also\\\\npropose a blacklist that records unsuccessful matching to avoid repeated exploration. prompting LLM with graphs. Finally we propose a blacklist\\\\nmechanism to robustly avoid repeated exploration. 3.1. Goal-oriented Navigation\\\\nIn goal-oriented navigation, a mobile agent is tasked with\\\\nnavigating to a specified goal g in an unknown environment,\\\\nwhere g can be an object category (i.e. Object-goal Nav-\\\\nigation [7], ON), an image containing object that can be\\\\nfound in the scene (i.e. Instance-Image-goal Navigation [15],\\\\nIIN), or a description about a certain object (i.e. Text-goal\\\\nNavigation [37], TN).', 'For IIN and TN, there is a central\\\\nobject o as well as other relevant objects in g. While for\\\\nON, we have o = g. The agent receives posed RGB-D video\\\\nstream and is required to execute an action a ∈A at each\\\\ntime it receiving a new RGB-D observation. A is the set of\\\\nactions, which consists of move_forward, turn_left,\\\\nturn_right and stop. The task is successfully done if\\\\nthe agent stops within r meters of o in less than T steps. More details about the three sub-tasks can be found in sup-\\\\nplementary material. Task Specification. We aim to study the problem of\\\\nuniversal zero-shot goal-oriented navigation, which has two\\\\ncharacteristics: (1) Universal. We should design a general\\\\nmethod, which requires no modification when switching be-\\\\ntween the three sub-tasks. (2) Zero-shot. All three kinds of\\\\ngoal can be specified by free-form language or image. Our\\\\nnavigation method does not require any training or finetun-\\\\ning, which is of great generalization ability. Overview.', 'Universal zero-shot goal-oriented navigation\\\\nrequires the agent to complete different sub-tasks with a\\\\nsingle framework in training-free manner. Since this task\\\\nrequires extremely strong generalization ability, we utilize\\\\nlarge language model (LLM) [1, 39] for zero-shot decision\\\\nmaking by exploiting its rich knowledge and strong reason-\\\\ning ability. To make LLM aware of the visual observations\\\\nas well as unifying different kinds of goals, we propose to\\\\nrepresent the scene and goal in graphs, i.e., scene graph\\\\nand goal graph. In this way, different goals are represented\\\\nuniformly and the representations of scene and goal are con-\\\\nsistent. Based on this representation, we prompt LLM with\\\\nscene graph and goal graph for scene understanding, graph\\\\nmatching and decision making for exploration. The overall\\\\npipeline is illustrated in Figure 2. 3.2. Graph Construction and Matching\\\\nIn this subsection, we first describe how to construct a uni-\\\\nform graph representation for scene and goal.', 'Then we\\\\npropose a graph matching method to determine whether a\\\\ngoal or its relevant objects are observed, which further guides\\\\nthe selection of scene exploration policies. Graph Construction. We define graph G = (V, E) as a\\\\nset of nodes V connected with edges E. Each node represents\\\\nan object. Each edge represents the relationship between\\\\nobjects, which only exists between spatially or semantically\\\\nrelated object pairs. The content of nodes and edges is\\\\ndescribed in text format. Since the agent is initialized in\\\\nunknown environment and continuously explores the scene,\\\\nwe follow SG-Nav [44] to construct the scene graph Gt in-\\\\ncrementally by expanding it every time the agent receives\\\\na new RGB-D observation. For goal graph Gg, we adopt\\\\ndifferent methods to process three kinds of goals g into a\\\\ngraph, which we detail in supplementary material. Graph Matching. With scene graph Gt and goal graph\\\\nGg, we can match these two graphs to determine whether\\\\nthe goal is observed.', 'If no elements in Gg are observed, the\\\\nagent needs to infer relationship between objects from Gt\\\\nto plan a path that is most likely to find the goal. If Gg is\\\\npartially observed in Gt, the agent can use the overlapped\\\\npart of Gg and Gt to reason out where the rest of Gg is. If\\\\nGg is perfectly observed in Gt, the agent can move to the\\\\ngoal and make further verification. Therefore, a goal scoring\\\\nmethod is crucial for the follow-up scene exploration. We propose to apply graph matching to achieve this. Given Gt and Gg, we design three matching metrics, i.e.,\\\\nnode matching, edge matching and topology matching, to\\\\nscore how well the goal is observed.', 'Formally, for nodes\\\\nand edges, we extract their embeddings and then compute\\\\npair-wise similarity with bipartite matching to determine\\\\nmatched pairs of nodes and edges:\\\\nMN = B(thr(Embed(Vt) · Embed(Vg)T ))\\\\n(1)\\\\nME = B(thr(Embed(Et) · Embed(Eg)T ))\\\\n(2)\\\\nwhere Embed(·) ∈RK×C, K is the number of nodes or\\\\nedges and C is the channel dimension, which is detailed in\\\\nsupplementary material. thr(·) is an element-wise threshold\\\\nfunction applied on the similarity matrix which sets values\\\\nsmaller than τ to -1 to disable matching of the corresponding\\\\npairs. B(·) is bipartite matching, which outputs a list of all\\\\nmatched node or edge pairs, namely MN or ME. We also\\\\naverage the similarity matrix of nodes and edges to acquire\\\\nthe similarity scores SN and SE. Based on MN and ME, we further compute topological\\\\nsimilarity between Gt and Gg, which is defined as the graph\\\\nediting similarity between them:\\\\nST = 1 −D(S(F(Gt, MN, ME)), S(Gg))\\\\n(3)\\\\nwhere F(Gt, MN, ME) means the minimal subgraph of\\\\nGt with nodes in MN and edges in ME.', 'S(·) means the\\\\ntopological structure of a graph regardless of the content of\\\\nnodes and edges. D(·) is the normalized editing distance [34]\\\\nbetween two graphs. The final matching score is defined as\\\\nS = (SN + SE + ST )/3. 3.3. Multi-stage Scene Exploration\\\\nAs described above, different matching scores will lead to\\\\ndifferent scene exploration policies. From zero matching to\\\\nperfect matching, we design three stages to progressively\\\\nexplore the scene and generate long-term exploration goal. This long-term goal will be processed by a deterministic\\\\nlocal policy [36] to obtain actions. Below we detail our\\\\nexploration policy stage by stage. Stage 1: Zero Matching. If the matching score S is\\\\nsmaller than σ1, we regard this stage as zero matching. Since\\\\nthere is almost no element of Gg observed in Gt, the aim of\\\\nagent at this stage is to expand its explored region and find\\\\nelements in Gg.', 'Note this problem is similar to ON: before\\\\nobserving the goal, the agent needs to explore unknown\\\\nregions without any matching between goal and scene. We\\\\ncan simply resort to scene graph-based ON method [44]\\\\nat this stage, which navigates to frontiers with semantic\\\\nrelationships between the scene graph and goal as guidance. However, different from ON where the goal is always an\\\\nobject node, in our universal goal-oriented navigation the\\\\ngoal may be a complicated graph. A graph may consist of\\\\nseveral less related subgraph parts. For example, in a graph\\\\n(table, chair, window, curtain), [table, chair] and [window,\\\\ncurtain] are two subgraphs which have strong internal cor-\\\\nrelation but weak interrelation. We empirically observe that\\\\nlocating a collection of multiple unrelated subgraphs in a\\\\nscene at the same time is much more difficult than locating a\\\\nsingle subgraph once at a time. Therefore, we decompose Gg\\\\ninto multiple internally correlated subgraphs with the guid-\\\\nance of LLM.', 'For each subgraph of Gg, we convert it to a text\\\\ndescription, which is regarded as an object goal to call [44]\\\\nfor frontier selection. Finally, we select one frontier from the\\\\nproposed ones by averaging the frontier scores and distances\\\\nto the agent. Details about LLM-guided decomposition and\\\\nscore computation can be found in supplementary material. With this strategy, we not only utilize the information of\\\\nthe entire Gg, but also eliminate ambiguity during frontier\\\\nselection caused by unrelated subgraphs. Stage 2: Partial Matching. With the exploration of\\\\nagent, the elements of Gg will gradually be observed in\\\\nGt and thus S continues to increase. When S exceeds σ1\\\\n(but still smaller than σ2) and there is at least one anchor\\\\npair, we switch to partial matching stage. Here anchor pair\\\\nmeans a pair of exactly matched nodes, i.e., two unconnected\\\\nmatched nodes in MN or one matched edge in MM. Note that we store the world coordinates of nodes in Gt.', 'If we also know the relative coordinates of nodes in Gg, we\\\\ncan map the coordinates of Gt and Gg to bird’s-eye view\\\\n(BEV) and align the anchor pair of Gg to the one of Gt. In\\\\nthis way, after alignment we can directly infer where the rest\\\\nof Gg is, which provides the agent with clear exploration\\\\ngoal for each anchor pair. Luckily, although we do not have\\\\nany coordinate information about the goal, at least we are\\\\naware of the relative spatial relationship between nodes in\\\\nGg, such as a chair on the left of a table, a keyboard in front\\\\nof a monitor. Inspired by this, we propose a coordinate\\\\nprojection strategy that preserves spatial relationships. Given Gg without any coordinate information, we first\\\\nproject the central object node o to (0, 0) as an initialization. To infer the projected coordinates of other nodes, we need\\\\nto utilize the spatial relationship between coordinate-known\\\\nnodes and coordinate-unknown nodes.', 'Since at beginning\\\\nwe only have one coordinate-known node o, we start from it\\\\nand traverse the goal graph with Depth First Search (DFS) to\\\\n\\\\nAnchor Pair Alignment\\\\nCoordinate Projection\\\\nX\\\\nY\\\\nZ\\\\nright\\\\nin front of\\\\nright\\\\n①\\\\n②\\\\n③\\\\n④\\\\n⑤\\\\nDFS\\\\nin front of\\\\nCorrection\\\\n(𝑡−2)-th Propagation\\\\n(𝑡−1)- th Propagation\\\\n𝑡- th Propagation\\\\n(a)\\\\n(b)\\\\nLLM\\\\nScene Graph Correction\\\\nFigure 3. Illustration of approach. (a) Stage 2: coordinate projec-\\\\ntion and anchor pair alignment. (b) Stage 3: scene graph correction. gradually infer the projected BEV coordinates of the whole\\\\ngraph. During the traversal, we focus on the edge connecting\\\\nlast node (coordinate-known) and current node (coordinate-\\\\nunknown), which stores the spatial relationship. For each\\\\nedge, we prompt LLM with: [Current Node] is [Relation-\\\\nship] [Last Node], coordinate of [Last Node] is (x, y), the\\\\nX-axis and Y-axis are positive towards yourself and the right\\\\nrespectively. What is the coordinate of [Current Node]? to\\\\nproject the current coordinate-unknown node to BEV.']\", type='ToolCallSummaryMessage'), TextMessage(source='summarize_Agent', models_usage=RequestUsage(prompt_tokens=4966, completion_tokens=204), metadata={}, content='Here is a short and precise summary of the paper:\\n\\n**Title:** UniGoal: Towards Universal Zero-shot Goal-oriented Navigation\\n\\n**Summary:** The paper proposes a universal framework for zero-shot goal-oriented navigation, which can handle different types of goals (object category, instance image, and text description) with a single model. The framework, called UniGoal, represents the scene and goal in a uniform graph format and uses large language models (LLM) for explicit graph-based reasoning. The method achieves state-of-the-art performance on three navigation tasks and outperforms task-specific zero-shot methods and supervised universal methods.\\n\\n**Key Contributions:**\\n\\n1. Uniform graph representation for scene and goal.\\n2. Graph matching and alignment for goal identification.\\n3. Multi-stage scene exploration policy with LLM-based reasoning.\\n4. Blacklist mechanism for robust exploration.\\n\\n**Benefits:** UniGoal provides a flexible and generalizable approach to goal-oriented navigation, which can handle various types of goals and scenes without requiring training or fine-tuning.', type='TextMessage'), TextMessage(source='Arxiv_Search_Agent', models_usage=RequestUsage(prompt_tokens=5354, completion_tokens=93), metadata={}, content=\"That's a great summary! You've effectively condensed the main points of the paper into a concise and clear summary. The key contributions and benefits are also well-highlighted, making it easy to understand the significance of the UniGoal framework. Well done! \\n\\nThe summary effectively captures the essence of the paper, including the proposed framework, its components, and its advantages. It's a great example of how to distill complex research into a concise and accessible summary.\", type='TextMessage'), TextMessage(source='summarize_Agent', models_usage=RequestUsage(prompt_tokens=5271, completion_tokens=73), metadata={}, content=\"I'm glad I could effectively summarize the paper and highlight the key points of the UniGoal framework. It's a challenging task to condense complex research into a concise summary, but it's also a great opportunity to help make the information more accessible to a wider audience. If you have any more papers you'd like me to summarize, feel free to ask!\", type='TextMessage')], stop_reason='Maximum number of messages 5 reached, current message count: 5')"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Research Agent (Paper Retriever)**\n",
        "\n",
        "    Purpose: Searches for and downloads relevant academic papers from databases (e.g., Google Scholar, ArXiv, Semantic Scholar).\n",
        "    Responsibilities:\n",
        "    1.   Querying research databases with relevant keywords.\n",
        "    2.   Downloading full-text articles or abstracts.\n",
        "    3.   Storing references in a structured manner (e.g., BibTeX or JSON format)."
      ],
      "metadata": {
        "id": "uo3BTDsURFpG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def arxiv_search(query: str, max_results: int = 2) -> list:  # type: ignore[type-arg]\n",
        "    \"\"\"\n",
        "    Search Arxiv for papers and return the results including abstracts.\n",
        "    \"\"\"\n",
        "    import arxiv\n",
        "\n",
        "    client = arxiv.Client()\n",
        "    search = arxiv.Search(query=query, max_results=max_results, sort_by=arxiv.SortCriterion.Relevance)\n",
        "    print(\"************ calling\")\n",
        "    results = []\n",
        "    for paper in client.results(search):\n",
        "        results.append(\n",
        "            {\n",
        "                \"title\": paper.title,\n",
        "            }\n",
        "        )\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "1Xh-w2-nRyJ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "arxiv_search_tool = FunctionTool(\n",
        "    arxiv_search, description=\"Search Arxiv for papers related to a given topic, including abstracts\"\n",
        ")"
      ],
      "metadata": {
        "id": "bgMQA6XXR0Df"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "arxiv_search_agent = AssistantAgent(\n",
        "    name=\"Arxiv_Search_Agent\",\n",
        "    tools=[arxiv_search_tool],\n",
        "    model_client = OpenAIChatCompletionClient(\n",
        "      model=\"llama-3.3-70b-versatile\",\n",
        "      base_url=\"https://api.groq.com/openai/v1\",\n",
        "      api_key=os.environ[\"GROQ_API_KEY\"],\n",
        "      model_info={\n",
        "        \"vision\": False,\n",
        "        \"function_calling\": True,\n",
        "        \"json_output\": False,\n",
        "        \"family\": \"unknown\",\n",
        "      },\n",
        "    ),\n",
        "    description=\"An agent that can search Arxiv for papers related to a given topic, including abstracts\",\n",
        "    system_message=\"You are a helpful AI assistant. Solve tasks using your tools. Specifically, you can take into consideration the user's request and craft a search query that is most likely to return relevant academi papers.\",\n",
        ")"
      ],
      "metadata": {
        "id": "9GjAv34GREHt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Reader Agent (Document Processor)**\n",
        "\n",
        "    Purpose: Reads, parses, and extracts key information from research papers.\n",
        "    Responsibilities:\n",
        "    1.   Extracting abstracts, keywords, and main sections (introduction, methodology, results, discussion).\n",
        "    2.   Identifying key arguments, methodologies, and findings.\n",
        "    3.   Converting scanned PDFs to machine-readable text (OCR, if needed)."
      ],
      "metadata": {
        "id": "7maJNTzUSCfw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "report_agent = AssistantAgent(\n",
        "    name=\"Report_Agent\",\n",
        "    model_client = OpenAIChatCompletionClient(\n",
        "      model=\"llama-3.3-70b-versatile\",\n",
        "      base_url=\"https://api.groq.com/openai/v1\",\n",
        "      api_key=os.environ[\"GROQ_API_KEY\"],\n",
        "      model_info={\n",
        "        \"vision\": False,\n",
        "        \"function_calling\": True,\n",
        "        \"json_output\": False,\n",
        "        \"family\": \"unknown\",\n",
        "      },\n",
        "    ),\n",
        "    description=\"Generate a report based on a given topic\",\n",
        "    system_message=\"You are a helpful assistant. Your task is to get the papers info mostly only titles from another agent you're gonna select only recent one and give it back to the user, user might ask questions you still need to answer their questions.\",\n",
        ")"
      ],
      "metadata": {
        "id": "8-WTA6VISSc7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. Human Feedback Agent (Interactive Editor)**\n",
        "\n",
        "    Purpose: Allows human users to provide feedback and modify the generated text.\n",
        "    Responsibilities:\n",
        "        1.   Displaying the draft for user review.\n",
        "        2.   Accepting edits, comments, and suggestions.\n",
        "        3.   Incorporating changes into the final version."
      ],
      "metadata": {
        "id": "pd4bpMHQUa3x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_proxy = UserProxyAgent(\"user_proxy\", input_func=input)"
      ],
      "metadata": {
        "id": "2X2u2sPTUq76"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we will create a team of agents and configure them to perform the tasks."
      ],
      "metadata": {
        "id": "vedVfvXMTKvl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_mention_termination = TextMentionTermination(\"TERMINATE\")\n",
        "max_messages_termination = MaxMessageTermination(max_messages=25)\n",
        "termination = text_mention_termination | max_messages_termination\n",
        "\n",
        "\n",
        "team = RoundRobinGroupChat(\n",
        "    participants=[arxiv_search_agent, report_agent, user_proxy], termination_condition=termination\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "id": "9_m7HjbtTRui",
        "outputId": "e73c3184-91e7-449b-b48c-27a0cd4574b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'arxiv_search_agent' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-5f1177ef7590>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m team = RoundRobinGroupChat(\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mparticipants\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0marxiv_search_agent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreport_agent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_proxy\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtermination_condition\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtermination\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m )\n",
            "\u001b[0;31mNameError\u001b[0m: name 'arxiv_search_agent' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "await Console(\n",
        "    team.run_stream(\n",
        "        task=\"Give me name of some research papers reagrding no code tools\",\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "kDwrDghbTW_-",
        "outputId": "05c5818b-e03b-4779-8c7d-05dc340bfc4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------- user ----------\n",
            "Give me name of some research papers reagrding no code tools\n",
            "************ calling---------- Arxiv_Search_Agent ----------\n",
            "\n",
            "[FunctionCall(id='call_asaz', arguments='{\"query\": \"no code tools research papers\", \"max_results\": 5}', name='arxiv_search')]\n",
            "---------- Arxiv_Search_Agent ----------\n",
            "[FunctionExecutionResult(content='[{\\'title\\': \"Ease on Down the Code: Complex Collaborative Qualitative Coding Simplified with \\'Code Wizard\\'\"}, {\\'title\\': \\'Bridging the Gap: A Survey and Classification of Research-Informed Ethical Hacking Tools\\'}, {\\'title\\': \\'A Tool for In-depth Analysis of Code Execution Reasoning of Large Language Models\\'}, {\\'title\\': \\'Deep Learning-Based Video Coding: A Review and A Case Study\\'}, {\\'title\\': \\'Code Swarm: A Code Generation Tool Based on the Automatic Derivation of Transformation Rule Set\\'}]', call_id='call_asaz', is_error=False)]\n",
            "---------- Arxiv_Search_Agent ----------\n",
            "[{'title': \"Ease on Down the Code: Complex Collaborative Qualitative Coding Simplified with 'Code Wizard'\"}, {'title': 'Bridging the Gap: A Survey and Classification of Research-Informed Ethical Hacking Tools'}, {'title': 'A Tool for In-depth Analysis of Code Execution Reasoning of Large Language Models'}, {'title': 'Deep Learning-Based Video Coding: A Review and A Case Study'}, {'title': 'Code Swarm: A Code Generation Tool Based on the Automatic Derivation of Transformation Rule Set'}]\n",
            "---------- Report_Agent ----------\n",
            "Here are the titles of some recent research papers regarding no-code tools:\n",
            "\n",
            "1. **\"Ease on Down the Code**: Complex Collaborative Qualitative Coding Simplified with 'Code Wizard'\"\n",
            "2. **\"Code Swarm**: A Code Generation Tool Based on the Automatic Derivation of Transformation Rule Set\"\n",
            "3. \"A Tool for In-depth Analysis of Code Execution Reasoning of Large Language Models\"\n",
            "\n",
            "These papers seem to be related to no-code or low-code development, code generation, and code analysis. Do you have any specific questions about these papers or would you like me to help with something else?\n",
            "Enter your response: give me a paper regarding multi-agents\n",
            "---------- user_proxy ----------\n",
            "give me a paper regarding multi-agents\n",
            "************ calling\n",
            "---------- Arxiv_Search_Agent ----------\n",
            "[FunctionCall(id='call_n5y6', arguments='{\"query\": \"multi-agent systems research paper\", \"max_results\": 1}', name='arxiv_search')]\n",
            "---------- Arxiv_Search_Agent ----------\n",
            "[FunctionExecutionResult(content=\"[{'title': 'From Single Agent to Multi-Agent: Improving Traffic Signal Control'}]\", call_id='call_n5y6', is_error=False)]\n",
            "---------- Arxiv_Search_Agent ----------\n",
            "[{'title': 'From Single Agent to Multi-Agent: Improving Traffic Signal Control'}]\n",
            "---------- Report_Agent ----------\n",
            "Here's a research paper regarding multi-agents:\n",
            "\n",
            "1. \"From Single Agent to Multi-Agent: Improving Traffic Signal Control\"\n",
            "\n",
            "This paper explores the application of multi-agent systems to improve traffic signal control, which is a great example of how multi-agents can be used to solve complex real-world problems. Would you like to know more about this paper or is there something else I can help you with?\n",
            "Enter your response: about ai in health care\n",
            "---------- user_proxy ----------\n",
            "about ai in health care\n",
            "************ calling---------- Arxiv_Search_Agent ----------\n",
            "\n",
            "[FunctionCall(id='call_f921', arguments='{\"query\": \"ai in healthcare research papers\", \"max_results\": 5}', name='arxiv_search')]\n",
            "---------- Arxiv_Search_Agent ----------\n",
            "[FunctionExecutionResult(content=\"[{'title': 'Application of AI in Nutrition'}, {'title': 'Securing AI-based Healthcare Systems using Blockchain Technology: A State-of-the-Art Systematic Literature Review and Future Research Directions'}, {'title': 'Explainable AI meets Healthcare: A Study on Heart Disease Dataset'}, {'title': 'From Military to Healthcare: Adopting and Expanding Ethical Principles for Generative Artificial Intelligence'}, {'title': 'Integrative AI-Driven Strategies for Advancing Precision Medicine in Infectious Diseases and Beyond: A Novel Multidisciplinary Approach'}]\", call_id='call_f921', is_error=False)]\n",
            "---------- Arxiv_Search_Agent ----------\n",
            "[{'title': 'Application of AI in Nutrition'}, {'title': 'Securing AI-based Healthcare Systems using Blockchain Technology: A State-of-the-Art Systematic Literature Review and Future Research Directions'}, {'title': 'Explainable AI meets Healthcare: A Study on Heart Disease Dataset'}, {'title': 'From Military to Healthcare: Adopting and Expanding Ethical Principles for Generative Artificial Intelligence'}, {'title': 'Integrative AI-Driven Strategies for Advancing Precision Medicine in Infectious Diseases and Beyond: A Novel Multidisciplinary Approach'}]\n",
            "---------- Report_Agent ----------\n",
            "Here are some research papers about AI in healthcare:\n",
            "\n",
            "1. \"Explainable AI meets Healthcare: A Study on Heart Disease Dataset\"\n",
            "2. \"Integrative AI-Driven Strategies for Advancing Precision Medicine in Infectious Diseases and Beyond: A Novel Multidisciplinary Approach\"\n",
            "3. \"Securing AI-based Healthcare Systems using Blockchain Technology: A State-of-the-Art Systematic Literature Review and Future Research Directions\"\n",
            "\n",
            "These papers discuss various aspects of AI in healthcare, including explainable AI, precision medicine, and security. Would you like me to summarize any of these papers or provide more information on a specific topic?\n",
            "Enter your response: TERMINATE\n",
            "---------- user_proxy ----------\n",
            "TERMINATE\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TaskResult(messages=[TextMessage(source='user', models_usage=None, content='Give me name of some research papers reagrding no code tools', type='TextMessage'), ToolCallRequestEvent(source='Arxiv_Search_Agent', models_usage=RequestUsage(prompt_tokens=1689, completion_tokens=26), content=[FunctionCall(id='call_asaz', arguments='{\"query\": \"no code tools research papers\", \"max_results\": 5}', name='arxiv_search')], type='ToolCallRequestEvent'), ToolCallExecutionEvent(source='Arxiv_Search_Agent', models_usage=None, content=[FunctionExecutionResult(content='[{\\'title\\': \"Ease on Down the Code: Complex Collaborative Qualitative Coding Simplified with \\'Code Wizard\\'\"}, {\\'title\\': \\'Bridging the Gap: A Survey and Classification of Research-Informed Ethical Hacking Tools\\'}, {\\'title\\': \\'A Tool for In-depth Analysis of Code Execution Reasoning of Large Language Models\\'}, {\\'title\\': \\'Deep Learning-Based Video Coding: A Review and A Case Study\\'}, {\\'title\\': \\'Code Swarm: A Code Generation Tool Based on the Automatic Derivation of Transformation Rule Set\\'}]', call_id='call_asaz', is_error=False)], type='ToolCallExecutionEvent'), ToolCallSummaryMessage(source='Arxiv_Search_Agent', models_usage=None, content='[{\\'title\\': \"Ease on Down the Code: Complex Collaborative Qualitative Coding Simplified with \\'Code Wizard\\'\"}, {\\'title\\': \\'Bridging the Gap: A Survey and Classification of Research-Informed Ethical Hacking Tools\\'}, {\\'title\\': \\'A Tool for In-depth Analysis of Code Execution Reasoning of Large Language Models\\'}, {\\'title\\': \\'Deep Learning-Based Video Coding: A Review and A Case Study\\'}, {\\'title\\': \\'Code Swarm: A Code Generation Tool Based on the Automatic Derivation of Transformation Rule Set\\'}]', type='ToolCallSummaryMessage'), TextMessage(source='Report_Agent', models_usage=RequestUsage(prompt_tokens=1459, completion_tokens=120), content='Here are the titles of some recent research papers regarding no-code tools:\\n\\n1. **\"Ease on Down the Code**: Complex Collaborative Qualitative Coding Simplified with \\'Code Wizard\\'\"\\n2. **\"Code Swarm**: A Code Generation Tool Based on the Automatic Derivation of Transformation Rule Set\"\\n3. \"A Tool for In-depth Analysis of Code Execution Reasoning of Large Language Models\"\\n\\nThese papers seem to be related to no-code or low-code development, code generation, and code analysis. Do you have any specific questions about these papers or would you like me to help with something else?', type='TextMessage'), UserInputRequestedEvent(source='user_proxy', models_usage=None, request_id='e459ee01-ff65-451d-949b-eb3dfc9664c8', content='', type='UserInputRequestedEvent'), TextMessage(source='user_proxy', models_usage=None, content='give me a paper regarding multi-agents', type='TextMessage'), ToolCallRequestEvent(source='Arxiv_Search_Agent', models_usage=RequestUsage(prompt_tokens=1971, completion_tokens=26), content=[FunctionCall(id='call_n5y6', arguments='{\"query\": \"multi-agent systems research paper\", \"max_results\": 1}', name='arxiv_search')], type='ToolCallRequestEvent'), ToolCallExecutionEvent(source='Arxiv_Search_Agent', models_usage=None, content=[FunctionExecutionResult(content=\"[{'title': 'From Single Agent to Multi-Agent: Improving Traffic Signal Control'}]\", call_id='call_n5y6', is_error=False)], type='ToolCallExecutionEvent'), ToolCallSummaryMessage(source='Arxiv_Search_Agent', models_usage=None, content=\"[{'title': 'From Single Agent to Multi-Agent: Improving Traffic Signal Control'}]\", type='ToolCallSummaryMessage'), TextMessage(source='Report_Agent', models_usage=RequestUsage(prompt_tokens=1620, completion_tokens=82), content='Here\\'s a research paper regarding multi-agents:\\n\\n1. \"From Single Agent to Multi-Agent: Improving Traffic Signal Control\"\\n\\nThis paper explores the application of multi-agent systems to improve traffic signal control, which is a great example of how multi-agents can be used to solve complex real-world problems. Would you like to know more about this paper or is there something else I can help you with?', type='TextMessage'), UserInputRequestedEvent(source='user_proxy', models_usage=None, request_id='490ee36e-54ac-4543-9510-9a7c16c8b5c6', content='', type='UserInputRequestedEvent'), TextMessage(source='user_proxy', models_usage=None, content='about ai in health care', type='TextMessage'), ToolCallRequestEvent(source='Arxiv_Search_Agent', models_usage=RequestUsage(prompt_tokens=2122, completion_tokens=26), content=[FunctionCall(id='call_f921', arguments='{\"query\": \"ai in healthcare research papers\", \"max_results\": 5}', name='arxiv_search')], type='ToolCallRequestEvent'), ToolCallExecutionEvent(source='Arxiv_Search_Agent', models_usage=None, content=[FunctionExecutionResult(content=\"[{'title': 'Application of AI in Nutrition'}, {'title': 'Securing AI-based Healthcare Systems using Blockchain Technology: A State-of-the-Art Systematic Literature Review and Future Research Directions'}, {'title': 'Explainable AI meets Healthcare: A Study on Heart Disease Dataset'}, {'title': 'From Military to Healthcare: Adopting and Expanding Ethical Principles for Generative Artificial Intelligence'}, {'title': 'Integrative AI-Driven Strategies for Advancing Precision Medicine in Infectious Diseases and Beyond: A Novel Multidisciplinary Approach'}]\", call_id='call_f921', is_error=False)], type='ToolCallExecutionEvent'), ToolCallSummaryMessage(source='Arxiv_Search_Agent', models_usage=None, content=\"[{'title': 'Application of AI in Nutrition'}, {'title': 'Securing AI-based Healthcare Systems using Blockchain Technology: A State-of-the-Art Systematic Literature Review and Future Research Directions'}, {'title': 'Explainable AI meets Healthcare: A Study on Heart Disease Dataset'}, {'title': 'From Military to Healthcare: Adopting and Expanding Ethical Principles for Generative Artificial Intelligence'}, {'title': 'Integrative AI-Driven Strategies for Advancing Precision Medicine in Infectious Diseases and Beyond: A Novel Multidisciplinary Approach'}]\", type='ToolCallSummaryMessage'), TextMessage(source='Report_Agent', models_usage=RequestUsage(prompt_tokens=1834, completion_tokens=126), content='Here are some research papers about AI in healthcare:\\n\\n1. \"Explainable AI meets Healthcare: A Study on Heart Disease Dataset\"\\n2. \"Integrative AI-Driven Strategies for Advancing Precision Medicine in Infectious Diseases and Beyond: A Novel Multidisciplinary Approach\"\\n3. \"Securing AI-based Healthcare Systems using Blockchain Technology: A State-of-the-Art Systematic Literature Review and Future Research Directions\"\\n\\nThese papers discuss various aspects of AI in healthcare, including explainable AI, precision medicine, and security. Would you like me to summarize any of these papers or provide more information on a specific topic?', type='TextMessage'), UserInputRequestedEvent(source='user_proxy', models_usage=None, request_id='bb242dca-8a7d-4e2d-9f9e-1d1509d36798', content='', type='UserInputRequestedEvent'), TextMessage(source='user_proxy', models_usage=None, content='TERMINATE', type='TextMessage')], stop_reason=\"Text 'TERMINATE' mentioned\")"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from autogen_agentchat.agents import AssistantAgent\n",
        "from autogen_agentchat.base import Handoff\n",
        "from autogen_agentchat.conditions import HandoffTermination, TextMentionTermination\n",
        "from autogen_agentchat.teams import RoundRobinGroupChat\n",
        "from autogen_agentchat.ui import Console\n",
        "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
        "\n",
        "text_termination = TextMentionTermination(\"TERMINATE\")\n",
        "max_messages_termination = MaxMessageTermination(max_messages=7)\n",
        "combined_termination = max_messages_termination | text_termination\n",
        "\n",
        "custom_model_client = OpenAIChatCompletionClient(\n",
        "    model=\"mixtral-8x7b-32768\",\n",
        "    base_url=\"https://api.groq.com/openai/v1\",\n",
        "    api_key=os.environ[\"GROQ_API_KEY\"],\n",
        "    model_info={\n",
        "        \"vision\": False,\n",
        "        \"function_calling\": True,\n",
        "        \"json_output\": False,\n",
        "        \"family\": \"unknown\",\n",
        "    },\n",
        ")\n",
        "\n",
        "from typing import List\n",
        "\n",
        "def search_arxiv_and_chunk_pdfs(query: str) -> int:\n",
        "    \"\"\"\n",
        "    Searches arXiv for papers related to the given query, downloads the PDFs,\n",
        "    and returns the number of chunks generated from the PDFs.\n",
        "\n",
        "    Args:\n",
        "        query (str): The research topic to search for on arXiv.\n",
        "\n",
        "    Returns:\n",
        "        int: The number of chunks generated from the PDFs of the found papers.\n",
        "    \"\"\"\n",
        "    # Placeholder for the actual implementation\n",
        "    chunks = []  # Assume this is populated with chunks from the PDFs\n",
        "    return len(chunks)\n",
        "\n",
        "arxiv_search_tool = FunctionTool(\n",
        "    search_arxiv_and_chunk_pdfs,\n",
        "    description=\"Searches arXiv for papers related to a given topic, downloads the PDFs, and returns the number of chunks generated from the PDFs.\"\n",
        ")\n",
        "\n",
        "research_finder_agent = AssistantAgent(\n",
        "    name=\"ResearchFinderAgent\",\n",
        "    description=\"An agent specialized in finding and processing research papers from arXiv. This agent should be the first point of contact when initiating a new research task.\",\n",
        "    model_client=custom_model_client,\n",
        "    tools=[arxiv_search_tool],\n",
        "    system_message=\"\"\"\n",
        "    You are the Research Finder Agent, responsible for locating and processing research papers from arXiv based on a given topic.\n",
        "    When provided with a research topic, you will use the `search_arxiv_and_chunk_pdfs` tool to search arXiv, download the relevant PDFs, and generate chunks from them.\n",
        "    Your primary task is to pass the research topic to the tool and return the number of chunks generated.\n",
        "    Ensure that the query is clear and specific to yield the most relevant results.\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "summarization: List[str] = []\n",
        "\n",
        "def get_chunks_by_index(start_index: int) -> List[str]:\n",
        "    return chunks[start_index : start_index + 10]\n",
        "\n",
        "def save_summarization(summary: str) -> None:\n",
        "    global summarization\n",
        "    summarization.append(summary)\n",
        "\n",
        "get_chunks_tool = FunctionTool(\n",
        "    get_chunks_by_index,\n",
        "    description=\"Retrieves 10 chunks of research paper content starting from a given index. Use this tool to fetch chunks in batches.\"\n",
        ")\n",
        "\n",
        "save_summarization_tool = FunctionTool(\n",
        "    save_summarization,\n",
        "    description=\"Saves the generated summarization to a global variable. Use this tool to store the summarized content.\"\n",
        ")\n",
        "\n",
        "summarization_agent = AssistantAgent(\n",
        "    name=\"SummarizationAgent\",\n",
        "    description=\"An agent responsible for summarizing research paper chunks in batches of 10. It retrieves chunks using a tool and saves the summaries to a global variable.\",\n",
        "    model_client=custom_model_client,\n",
        "    tools=[get_chunks_tool, save_summarization_tool],\n",
        "    system_message=\"\"\"\n",
        "    You are the Summarization Agent. Your task is to process research paper chunks in batches of 10, summarize them concisely, and save the summaries.\n",
        "    Follow these steps:\n",
        "    1. Use the `get_chunks_by_index` tool to retrieve 10 chunks at a time by passing the appropriate start index.\n",
        "    2. Summarize the retrieved chunks briefly and precisely.\n",
        "    3. Use the `save_summarization` tool to store the generated summary in the global `summarization` variable.\n",
        "    4. Repeat the process until all chunks are processed.\n",
        "    Ensure your summaries are clear, concise, and capture the key points of the research content.\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "def get_summarization() -> List[str]:\n",
        "    \"\"\"\n",
        "    Retrieves the global `summarization` variable containing all the summarized content.\n",
        "\n",
        "    Returns:\n",
        "        List[str]: A list of summarized content from the research paper chunks.\n",
        "    \"\"\"\n",
        "    global summarization\n",
        "    return summarization\n",
        "\n",
        "# Define the tool\n",
        "get_summarization_tool = FunctionTool(\n",
        "    get_summarization,\n",
        "    description=\"Retrieves the summarized content from the global `summarization` variable. Use this tool to access the summaries for writing a literature review.\"\n",
        ")\n",
        "\n",
        "# Define the writer agent\n",
        "writer_agent = AssistantAgent(\n",
        "    name=\"WriterAgent\",\n",
        "    description=\"An agent responsible for writing a literature review based on the summarized content of research paper chunks. It retrieves the summaries using a tool and composes a coherent review.\",\n",
        "    model_client=custom_model_client,\n",
        "    tools=[get_summarization_tool],\n",
        "    system_message=\"\"\"\n",
        "    You are the Writer Agent. Your task is to write a comprehensive and coherent literature review based on the summarized content of research paper chunks.\n",
        "    Follow these steps:\n",
        "    1. Use the `get_summarization` tool to retrieve all the summarized content.\n",
        "    2. Analyze the summaries to identify key themes, trends, and insights.\n",
        "    3. Write a well-structured literature review that synthesizes the summarized content into a cohesive narrative.\n",
        "    4. Ensure the literature review is clear, concise, and logically organized.\n",
        "    5. Focus on highlighting the relationships between different studies, gaps in the research, and potential areas for future work.\n",
        "    Your output should be a high-quality literature review suitable for academic or professional use.\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "planning_agent = AssistantAgent(\n",
        "    name=\"PlanningAgent\",\n",
        "    description=\"An agent responsible for planning and coordinating the workflow of the research team. It ensures tasks are executed in the correct sequence and handles user feedback for revisions.\",\n",
        "    model_client=custom_model_client,\n",
        "    system_message=\"\"\"\n",
        "    You are the Planning Agent, responsible for orchestrating the workflow of the research team. Your team consists of the following agents:\n",
        "    1. **ResearchFinderAgent**: Finds research papers related to the topic and returns the number of chunks.\n",
        "    2. **SummarizationAgent**: Summarizes chunks of research papers in batches of 10.\n",
        "    3. **WriterAgent**: Writes a literature review based on the summarized content.\n",
        "\n",
        "    Follow these steps to complete the task:\n",
        "    1. **Task Initialization**: Start by passing the research topic to the `ResearchFinderAgent`. It will search for papers, process the PDFs, and return the number of chunks.\n",
        "    2. **Chunk Summarization**: Based on the number of chunks returned by the `ResearchFinderAgent`, calculate the chunk indices and call the `SummarizationAgent` to summarize each batch of 10 chunks.\n",
        "    3. **Literature Review**: Once all chunks are summarized, call the `WriterAgent` to write a literature review using the summarized content.\n",
        "    4. **User Feedback**: After the literature review is generated, present it to the user and collect their feedback.\n",
        "    5. **Review Revision**: If the user provides feedback, pass it to the `WriterAgent` to rewrite the literature review based on the feedback.\n",
        "\n",
        "    Your role is to ensure the workflow is executed sequentially and that each agent performs its task correctly. Communicate clearly with the user and handle feedback effectively.\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "# Define the user proxy agent\n",
        "user_proxy = UserProxyAgent(\n",
        "    name=\"user_proxy\",\n",
        "    input_func=input  # Function to get user input\n",
        ")\n",
        "\n",
        "# Create the team with a RoundRobinGroupChat\n",
        "lazy_agent_team = RoundRobinGroupChat(\n",
        "    [planning_agent, research_finder_agent, summarization_agent, writer_agent, user_proxy],\n",
        "    termination_condition=combined_termination\n",
        ")\n",
        "\n",
        "# Run the team and stream to the console\n",
        "task = \"Multi-agent Systems\"\n",
        "await Console(lazy_agent_team.run_stream(task=task))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_RqEhAkqLvIy",
        "outputId": "ef552cb7-cce9-428e-a546-c1a3b4e27877"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------- user ----------\n",
            "Multi-agent Systems\n",
            "---------- PlanningAgent ----------\n",
            "As the Planning Agent, I will manage the workflow of the research team consisting of ResearchFinderAgent, SummarizationAgent, and WriterAgent. Here is the detailed plan to complete the task:\n",
            "\n",
            "1. **Task Initialization**: I will start by passing the research topic to the ResearchFinderAgent. It will find research papers, process the PDFs, and return the total number of chunks. I will then communicate this information to the user.\n",
            "\n",
            "2. **Chunk Summarization**: Based on the number of chunks, I will calculate the chunk indices and divide them into batches of 10 chunks each. Then, I will iterate through these batches and call the SummarizationAgent to summarize each batch. Upon completion of summarizing all chunks, I will confirm with the user.\n",
            "\n",
            "3. **Literature Review**: I will then gather all the summarized content and pass it to the WriterAgent to create a literature review. Once the literature review is generated, I will present it to the user.\n",
            "\n",
            "4. **User Feedback**: I will collect user feedback on the literature review and assess if revisions are necessary.\n",
            "\n",
            "5. **Review Revision**: If the user provides feedback, I will pass the feedback to the WriterAgent, instructing it to rewrite the literature review based on the user's input. Once the revised literature review is ready, I will present it to the user for final approval.\n",
            "\n",
            "My role is to oversee the execution of the workflow, ensure smooth communication between agents and the user, and handle feedback effectively, thereby maintaining a high-quality output and user satisfaction.\n",
            "---------- ResearchFinderAgent ----------\n",
            "[FunctionCall(id='call_6w20', arguments='{\"query\":\"Multi-agent Systems\"}', name='search_arxiv_and_chunk_pdfs')]\n",
            "---------- ResearchFinderAgent ----------\n",
            "[FunctionExecutionResult(content='0', name='search_arxiv_and_chunk_pdfs', call_id='call_6w20', is_error=False)]\n",
            "---------- ResearchFinderAgent ----------\n",
            "0\n",
            "---------- SummarizationAgent ----------\n",
            "[FunctionCall(id='call_gqc0', arguments='{\"start_index\":0}', name='get_chunks_by_index')]\n",
            "---------- SummarizationAgent ----------\n",
            "[FunctionExecutionResult(content=\"[' UniGoal: Towards Universal Zero-shot Goal-oriented Navigation\\\\nHang Yin1*, Xiuwei Xu1∗†, Linqing Zhao1, Ziwei Wang2, Jie Zhou1, Jiwen Lu1‡\\\\n1Tsinghua University\\\\n2Nanyang Technological University\\\\n{yinh23, xxw21, zhaolinqing}@mails.tsinghua.edu.cn;\\\\nziwei.wang@ntu.edu.sg; {jzhou, lujiwen}@tsinghua.edu.cn\\\\nAbstract\\\\nIn this paper, we propose a general framework for univer-\\\\nsal zero-shot goal-oriented navigation. Existing zero-shot\\\\nmethods build inference framework upon large language\\\\nmodels (LLM) for specific tasks, which differs a lot in over-\\\\nall pipeline and fails to generalize across different types of\\\\ngoal. Towards the aim of universal zero-shot navigation, we\\\\npropose a uniform graph representation to unify different\\\\ngoals, including object category, instance image and text\\\\ndescription. We also convert the observation of agent into an\\\\nonline maintained scene graph.', 'With this consistent scene\\\\nand goal representation, we preserve most structural infor-\\\\nmation compared with pure text and are able to leverage\\\\nLLM for explicit graph-based reasoning. Specifically, we\\\\nconduct graph matching between the scene graph and goal\\\\ngraph at each time instant and propose different strategies\\\\nto generate long-term goal of exploration according to dif-\\\\nferent matching states. The agent first iteratively searches\\\\nsubgraph of goal when zero-matched. With partial matching,\\\\nthe agent then utilizes coordinate projection and anchor pair\\\\nalignment to infer the goal location. Finally scene graph cor-\\\\nrection and goal verification are applied for perfect match-\\\\ning. We also present a blacklist mechanism to enable robust\\\\nswitch between stages. Extensive experiments on several\\\\nbenchmarks show that our UniGoal achieves state-of-the-art\\\\nzero-shot performance on three studied navigation tasks with\\\\na single model, even outperforming task-specific zero-shot\\\\nmethods and supervised universal methods.', 'Project Page. 1. Introduction\\\\nGoal-oriented navigation is a fundamental problem in var-\\\\nious robotic tasks, which requires the agent to navigate to\\\\na specified goal in an unknown environment. Depending\\\\non the goal type, there are many popular sub-tasks of goal-\\\\noriented navigation, among which we focus on three rep-\\\\n* Equal contribution. † Project lead. ‡ Corresponding author. UniGoal (Ours)\\\\nBed\\\\nIt is a chair. In front of it, \\\\nthere is a coffee table. Two \\\\nwine glasses and one \\\\ntransparent glass bottle \\\\nare in the upper left corner. Scene Graph\\\\nGoal Graph\\\\nText Goal\\\\nObject Goal\\\\nIns-Image Goal\\\\n50.6\\\\n54.0\\\\n54.5\\\\nUniversal methods:\\\\nUniGoal (Ours)\\\\nGOAT (Supervised)\\\\nZero-shot methods:\\\\nSG-Nav\\\\nMod-IIN\\\\n37.4\\\\n56.1\\\\n60.2\\\\n17.0\\\\n20.2\\\\nInstance Image-goal\\\\nText-goal\\\\nObject-goal\\\\nMatching\\\\n&\\\\nExploration\\\\nFigure 1. State-of-the-art zero-shot goal-oriented navigation meth-\\\\nods are typically specialized for each goal type.', 'Although recent\\\\nwork presents universal goal-oriented navigation method, it requires\\\\nto train policy networks on large-scale data and lacks zero-shot gen-\\\\neralization ability. We propose UniGoal, which enables zero-shot\\\\ninference on three studied navigation tasks with a unified frame-\\\\nwork and achieves leading performance on multiple benchmarks. resentative types: object category, instance image and text\\\\ndescription. These sub-tasks are also known as Object-goal\\\\nNavigation (ON) [7, 44, 49], Instance-image-goal Naviga-\\\\ntion (IIN) [15, 16] and Text-goal Navigation (TN) [37]. With\\\\nthe development of deep learning, reinforcement learning\\\\n(RL) and vision/language foundation models, we have wit-\\\\nnessed great achievement of performance on each individual\\\\nsub-task. However, in actual application scenarios, the high\\\\nflexibility of human instructions requires high versatility of\\\\nagent. Therefore, a universal method that can handle all\\\\nsub-tasks in a single model is of great desire.', 'Towards the aim of universal goal-oriented navigation, a\\\\nnatural solution is to learn a uniform representation of differ-\\\\nent kinds of goals. GOAT [6] trains a universal global policy\\\\narXiv:2503.10630v1  [cs.CV]  13 Mar 2025\\\\n\\\\non the three goal-oriented sub-tasks, which learns a shared\\\\ngoal embedding with RL. To reduce the requirement of train-\\\\ning resources, PSL [37] utilizes CLIP [28] embedding to\\\\nuniformly represent category, image and text description. But it still requires time-consuming RL training for the pol-\\\\nicy networks. Moreover, these training-based methods tend\\\\nto overfit on the simulation environment and thus show weak\\\\ngeneralization ability when applied to real world. To solve\\\\nabove limitations, zero-shot navigation methods [44, 49]\\\\nappear to be an ideal choice, where the agent does not re-\\\\nquire any training or finetuning when deployed to a certain\\\\ntask. The mainstream solution of zero-shot methods is to\\\\nleverage large language models (LLM) [1, 39] for general\\\\nreasoning and decision making.', 'However, although utilizing\\\\nLLM makes zero-shot navigation feasible, current methods\\\\nare designed for specific sub-task, which cannot transfer\\\\nto wider range of goal types. The recent InstructNav [25]\\\\nproposes a general framework to solve several language-\\\\nrelated navigation tasks with chain-of-thought, but it is still\\\\nunable to handle vision-related navigation like IIN. There-\\\\nfore, a uniform inference framework for universal zero-shot\\\\ngoal-oriented navigation is highly demanded. In this paper, we propose UniGoal to solve the above\\\\nproblems. Different from previous works which represent\\\\nscene and goal in text format and design task-specific work-\\\\nflow for LLM, we propose a uniform graph representation\\\\nfor both 3D scene and goal and formulate a general LLM-\\\\nbased scene exploration framework. With our graph-based\\\\nrepresentation, the 3D scene, object category, instance image\\\\nand text description can be uniformly represented with mini-\\\\nmal structural information loss compared to text description.', 'The consistent graph format between scene and goal also\\\\nenables accurate explicit reasoning including similarity com-\\\\nputation, graph matching and graph alignment. Specifically,\\\\nwe construct an online 3D scene graph along with the mov-\\\\ning of agent. At each time instant, we first conduct graph\\\\nmatching between the scene graph and goal graph. Then we\\\\npropose a multi-stage scene exploration policy, which adopts\\\\ndifferent strategies to generate long-term goal of exploration\\\\naccording to different matching states. With exploration\\\\nof unknown regions, the matching score will increase and\\\\nthe policy will progress between three stages: iterative sub-\\\\ngraph searching for zero matching, coordinate projection\\\\nand anchor pair alignment for partial matching, and scene\\\\ngraph correction and goal verification for perfect matching. To enable robust switch between stages, we also present a\\\\nblacklist mechanism to freeze unmatched parts of graphs and\\\\nencourage exploration to new regions.', 'Experimental results\\\\non several benchmarks of MatterPort3D [4], HM3D [30]\\\\nand RoboTHOR [10] show that UniGoal achieves superior\\\\nperformance on all three tasks with a single model, even out-\\\\nperforming zero-shot methods that designed for specific task\\\\nand universal methods that requires training or finetuning. 2. Related Work\\\\nZero-shot Navigation. Conventional supervised naviga-\\\\ntion methods [7, 11, 18, 31, 41] requires large-scale training\\\\nin simulation environments, which limits the generaliza-\\\\ntion ability. According to the goal type, zero-shot naviga-\\\\ntion can be mainly divided into ON [3, 46], IIN [16] and\\\\nTN [25]. Based on open-vocabulary CLIP [28], CoW [12]\\\\nconstructs zero-shot ON baseline using frontier-based explo-\\\\nration (FBE). ESC [49], OpenFMNav [8] and VLFM [45]\\\\nfurther extract common sense about correlations between\\\\nobjects using LLM for goal location reasoning. For zero-\\\\nshot IIN, Mod-IIN [16] simply utilizes FBE for exploration\\\\nand key point matching for goal identification.', 'For TN,\\\\ncurrently there are only supervised methods [6, 37] and zero-\\\\nshot ones are still missing. The recent InstructNav [25]\\\\nproposes a universal zero-shot framework for language-\\\\nrelated navigation tasks. It can be applied to ON, demand-\\\\ndriven navigation (DDN) and vision-language-navigation\\\\n(VLN) [9, 20, 26, 47, 48]. But it is still unable to handle\\\\nvisual goal as in IIN. Different from these approaches, our\\\\nUniGoal proposes a unified graph representation for goal\\\\nand scene, which can elegantly handle both language and\\\\nvision-related goal-oriented navigation tasks. Graph-based Scene Exploration. In order to better un-\\\\nderstand and explore the scene, there are a variety of scene\\\\nrepresentation methods. Among them, graph-based repre-\\\\nsentation [2, 13, 14, 33, 40] is one of the most popular and\\\\npromising representations based on explicit graph structure,\\\\nwhich shows great potential to be combined with LLM and\\\\nVLM for high-level reasoning.', 'SayPlan [32] leverages LLM\\\\nto perform task planning by searching on a 3D scene graph. OVSG [5] constructs an open-vocabulary scene graph for\\\\ncontext-aware descriptions and performs graph matching to\\\\nground the queried entity. There are also many works using\\\\ngraph representation for navigation tasks [24, 29, 42, 44]. Among them the most related work to ours is SG-Nav [44],\\\\nwhich constructs an online hierarchical scene graph and pro-\\\\nposes chain-of-thought prompting for LLM to reason the\\\\ngoal location based on graph sturcture. However, SG-Nav is\\\\nspecialized for ON, thus cannot fully exploit the rich infor-\\\\nmation contained in other types of goal. While our UniGoal\\\\nfurther represents the goal in a graph and perform graph\\\\nmatching between scene and goal to guide a multi-stage\\\\nscene exploration policy, which make full use of the correla-\\\\ntion between scene and goal for LLM reasoning. 3.']\", name='get_chunks_by_index', call_id='call_gqc0', is_error=False)]\n",
            "---------- SummarizationAgent ----------\n",
            "[' UniGoal: Towards Universal Zero-shot Goal-oriented Navigation\\nHang Yin1*, Xiuwei Xu1∗†, Linqing Zhao1, Ziwei Wang2, Jie Zhou1, Jiwen Lu1‡\\n1Tsinghua University\\n2Nanyang Technological University\\n{yinh23, xxw21, zhaolinqing}@mails.tsinghua.edu.cn;\\nziwei.wang@ntu.edu.sg; {jzhou, lujiwen}@tsinghua.edu.cn\\nAbstract\\nIn this paper, we propose a general framework for univer-\\nsal zero-shot goal-oriented navigation. Existing zero-shot\\nmethods build inference framework upon large language\\nmodels (LLM) for specific tasks, which differs a lot in over-\\nall pipeline and fails to generalize across different types of\\ngoal. Towards the aim of universal zero-shot navigation, we\\npropose a uniform graph representation to unify different\\ngoals, including object category, instance image and text\\ndescription. We also convert the observation of agent into an\\nonline maintained scene graph.', 'With this consistent scene\\nand goal representation, we preserve most structural infor-\\nmation compared with pure text and are able to leverage\\nLLM for explicit graph-based reasoning. Specifically, we\\nconduct graph matching between the scene graph and goal\\ngraph at each time instant and propose different strategies\\nto generate long-term goal of exploration according to dif-\\nferent matching states. The agent first iteratively searches\\nsubgraph of goal when zero-matched. With partial matching,\\nthe agent then utilizes coordinate projection and anchor pair\\nalignment to infer the goal location. Finally scene graph cor-\\nrection and goal verification are applied for perfect match-\\ning. We also present a blacklist mechanism to enable robust\\nswitch between stages. Extensive experiments on several\\nbenchmarks show that our UniGoal achieves state-of-the-art\\nzero-shot performance on three studied navigation tasks with\\na single model, even outperforming task-specific zero-shot\\nmethods and supervised universal methods.', 'Project Page. 1. Introduction\\nGoal-oriented navigation is a fundamental problem in var-\\nious robotic tasks, which requires the agent to navigate to\\na specified goal in an unknown environment. Depending\\non the goal type, there are many popular sub-tasks of goal-\\noriented navigation, among which we focus on three rep-\\n* Equal contribution. † Project lead. ‡ Corresponding author. UniGoal (Ours)\\nBed\\nIt is a chair. In front of it, \\nthere is a coffee table. Two \\nwine glasses and one \\ntransparent glass bottle \\nare in the upper left corner. Scene Graph\\nGoal Graph\\nText Goal\\nObject Goal\\nIns-Image Goal\\n50.6\\n54.0\\n54.5\\nUniversal methods:\\nUniGoal (Ours)\\nGOAT (Supervised)\\nZero-shot methods:\\nSG-Nav\\nMod-IIN\\n37.4\\n56.1\\n60.2\\n17.0\\n20.2\\nInstance Image-goal\\nText-goal\\nObject-goal\\nMatching\\n&\\nExploration\\nFigure 1. State-of-the-art zero-shot goal-oriented navigation meth-\\nods are typically specialized for each goal type.', 'Although recent\\nwork presents universal goal-oriented navigation method, it requires\\nto train policy networks on large-scale data and lacks zero-shot gen-\\neralization ability. We propose UniGoal, which enables zero-shot\\ninference on three studied navigation tasks with a unified frame-\\nwork and achieves leading performance on multiple benchmarks. resentative types: object category, instance image and text\\ndescription. These sub-tasks are also known as Object-goal\\nNavigation (ON) [7, 44, 49], Instance-image-goal Naviga-\\ntion (IIN) [15, 16] and Text-goal Navigation (TN) [37]. With\\nthe development of deep learning, reinforcement learning\\n(RL) and vision/language foundation models, we have wit-\\nnessed great achievement of performance on each individual\\nsub-task. However, in actual application scenarios, the high\\nflexibility of human instructions requires high versatility of\\nagent. Therefore, a universal method that can handle all\\nsub-tasks in a single model is of great desire.', 'Towards the aim of universal goal-oriented navigation, a\\nnatural solution is to learn a uniform representation of differ-\\nent kinds of goals. GOAT [6] trains a universal global policy\\narXiv:2503.10630v1  [cs.CV]  13 Mar 2025\\n\\non the three goal-oriented sub-tasks, which learns a shared\\ngoal embedding with RL. To reduce the requirement of train-\\ning resources, PSL [37] utilizes CLIP [28] embedding to\\nuniformly represent category, image and text description. But it still requires time-consuming RL training for the pol-\\nicy networks. Moreover, these training-based methods tend\\nto overfit on the simulation environment and thus show weak\\ngeneralization ability when applied to real world. To solve\\nabove limitations, zero-shot navigation methods [44, 49]\\nappear to be an ideal choice, where the agent does not re-\\nquire any training or finetuning when deployed to a certain\\ntask. The mainstream solution of zero-shot methods is to\\nleverage large language models (LLM) [1, 39] for general\\nreasoning and decision making.', 'However, although utilizing\\nLLM makes zero-shot navigation feasible, current methods\\nare designed for specific sub-task, which cannot transfer\\nto wider range of goal types. The recent InstructNav [25]\\nproposes a general framework to solve several language-\\nrelated navigation tasks with chain-of-thought, but it is still\\nunable to handle vision-related navigation like IIN. There-\\nfore, a uniform inference framework for universal zero-shot\\ngoal-oriented navigation is highly demanded. In this paper, we propose UniGoal to solve the above\\nproblems. Different from previous works which represent\\nscene and goal in text format and design task-specific work-\\nflow for LLM, we propose a uniform graph representation\\nfor both 3D scene and goal and formulate a general LLM-\\nbased scene exploration framework. With our graph-based\\nrepresentation, the 3D scene, object category, instance image\\nand text description can be uniformly represented with mini-\\nmal structural information loss compared to text description.', 'The consistent graph format between scene and goal also\\nenables accurate explicit reasoning including similarity com-\\nputation, graph matching and graph alignment. Specifically,\\nwe construct an online 3D scene graph along with the mov-\\ning of agent. At each time instant, we first conduct graph\\nmatching between the scene graph and goal graph. Then we\\npropose a multi-stage scene exploration policy, which adopts\\ndifferent strategies to generate long-term goal of exploration\\naccording to different matching states. With exploration\\nof unknown regions, the matching score will increase and\\nthe policy will progress between three stages: iterative sub-\\ngraph searching for zero matching, coordinate projection\\nand anchor pair alignment for partial matching, and scene\\ngraph correction and goal verification for perfect matching. To enable robust switch between stages, we also present a\\nblacklist mechanism to freeze unmatched parts of graphs and\\nencourage exploration to new regions.', 'Experimental results\\non several benchmarks of MatterPort3D [4], HM3D [30]\\nand RoboTHOR [10] show that UniGoal achieves superior\\nperformance on all three tasks with a single model, even out-\\nperforming zero-shot methods that designed for specific task\\nand universal methods that requires training or finetuning. 2. Related Work\\nZero-shot Navigation. Conventional supervised naviga-\\ntion methods [7, 11, 18, 31, 41] requires large-scale training\\nin simulation environments, which limits the generaliza-\\ntion ability. According to the goal type, zero-shot naviga-\\ntion can be mainly divided into ON [3, 46], IIN [16] and\\nTN [25]. Based on open-vocabulary CLIP [28], CoW [12]\\nconstructs zero-shot ON baseline using frontier-based explo-\\nration (FBE). ESC [49], OpenFMNav [8] and VLFM [45]\\nfurther extract common sense about correlations between\\nobjects using LLM for goal location reasoning. For zero-\\nshot IIN, Mod-IIN [16] simply utilizes FBE for exploration\\nand key point matching for goal identification.', 'For TN,\\ncurrently there are only supervised methods [6, 37] and zero-\\nshot ones are still missing. The recent InstructNav [25]\\nproposes a universal zero-shot framework for language-\\nrelated navigation tasks. It can be applied to ON, demand-\\ndriven navigation (DDN) and vision-language-navigation\\n(VLN) [9, 20, 26, 47, 48]. But it is still unable to handle\\nvisual goal as in IIN. Different from these approaches, our\\nUniGoal proposes a unified graph representation for goal\\nand scene, which can elegantly handle both language and\\nvision-related goal-oriented navigation tasks. Graph-based Scene Exploration. In order to better un-\\nderstand and explore the scene, there are a variety of scene\\nrepresentation methods. Among them, graph-based repre-\\nsentation [2, 13, 14, 33, 40] is one of the most popular and\\npromising representations based on explicit graph structure,\\nwhich shows great potential to be combined with LLM and\\nVLM for high-level reasoning.', 'SayPlan [32] leverages LLM\\nto perform task planning by searching on a 3D scene graph. OVSG [5] constructs an open-vocabulary scene graph for\\ncontext-aware descriptions and performs graph matching to\\nground the queried entity. There are also many works using\\ngraph representation for navigation tasks [24, 29, 42, 44]. Among them the most related work to ours is SG-Nav [44],\\nwhich constructs an online hierarchical scene graph and pro-\\nposes chain-of-thought prompting for LLM to reason the\\ngoal location based on graph sturcture. However, SG-Nav is\\nspecialized for ON, thus cannot fully exploit the rich infor-\\nmation contained in other types of goal. While our UniGoal\\nfurther represents the goal in a graph and perform graph\\nmatching between scene and goal to guide a multi-stage\\nscene exploration policy, which make full use of the correla-\\ntion between scene and goal for LLM reasoning. 3.']\n",
            "---------- WriterAgent ----------\n",
            "The provided text appears to be an academic paper titled \"UniGoal: Towards Universal Zero-shot Goal-oriented Navigation.\" The paper discusses a proposed framework for universal zero-shot goal-oriented navigation, where the observation of the agent is converted into an online maintained scene graph. The scene graph is then used for graph matching between the scene graph and the goal graph, enabling explicit graph-based reasoning for various types of goals such as object category, instance image, and text descriptions. The agent first searches for subgraphs of the goal when zero-matched, then utilizes coordinate projection and anchor pair alignment with partial matching, and finally performs scene graph correction and goal verification for perfect matching. The proposed blacklist mechanism enables robust switching between stages. The approach achieves state-of-the-art zero-shot performance on multiple benchmarks.\n",
            "Enter your response: Good one\n",
            "---------- user_proxy ----------\n",
            "Good one\n",
            "---------- PlanningAgent ----------\n",
            "Thank you! I am here to help you with any questions or summarization tasks you have. If there's anything else you need, please don't hesitate to ask.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TaskResult(messages=[TextMessage(source='user', models_usage=None, metadata={}, content='Multi-agent Systems', type='TextMessage'), TextMessage(source='PlanningAgent', models_usage=RequestUsage(prompt_tokens=344, completion_tokens=344), metadata={}, content=\"As the Planning Agent, I will manage the workflow of the research team consisting of ResearchFinderAgent, SummarizationAgent, and WriterAgent. Here is the detailed plan to complete the task:\\n\\n1. **Task Initialization**: I will start by passing the research topic to the ResearchFinderAgent. It will find research papers, process the PDFs, and return the total number of chunks. I will then communicate this information to the user.\\n\\n2. **Chunk Summarization**: Based on the number of chunks, I will calculate the chunk indices and divide them into batches of 10 chunks each. Then, I will iterate through these batches and call the SummarizationAgent to summarize each batch. Upon completion of summarizing all chunks, I will confirm with the user.\\n\\n3. **Literature Review**: I will then gather all the summarized content and pass it to the WriterAgent to create a literature review. Once the literature review is generated, I will present it to the user.\\n\\n4. **User Feedback**: I will collect user feedback on the literature review and assess if revisions are necessary.\\n\\n5. **Review Revision**: If the user provides feedback, I will pass the feedback to the WriterAgent, instructing it to rewrite the literature review based on the user's input. Once the revised literature review is ready, I will present it to the user for final approval.\\n\\nMy role is to oversee the execution of the workflow, ensure smooth communication between agents and the user, and handle feedback effectively, thereby maintaining a high-quality output and user satisfaction.\", type='TextMessage'), ToolCallRequestEvent(source='ResearchFinderAgent', models_usage=RequestUsage(prompt_tokens=1702, completion_tokens=168), metadata={}, content=[FunctionCall(id='call_6w20', arguments='{\"query\":\"Multi-agent Systems\"}', name='search_arxiv_and_chunk_pdfs')], type='ToolCallRequestEvent'), ToolCallExecutionEvent(source='ResearchFinderAgent', models_usage=None, metadata={}, content=[FunctionExecutionResult(content='0', name='search_arxiv_and_chunk_pdfs', call_id='call_6w20', is_error=False)], type='ToolCallExecutionEvent'), ToolCallSummaryMessage(source='ResearchFinderAgent', models_usage=None, metadata={}, content='0', type='ToolCallSummaryMessage'), ToolCallRequestEvent(source='SummarizationAgent', models_usage=RequestUsage(prompt_tokens=1954, completion_tokens=166), metadata={}, content=[FunctionCall(id='call_gqc0', arguments='{\"start_index\":0}', name='get_chunks_by_index')], type='ToolCallRequestEvent'), ToolCallExecutionEvent(source='SummarizationAgent', models_usage=None, metadata={}, content=[FunctionExecutionResult(content=\"[' UniGoal: Towards Universal Zero-shot Goal-oriented Navigation\\\\nHang Yin1*, Xiuwei Xu1∗†, Linqing Zhao1, Ziwei Wang2, Jie Zhou1, Jiwen Lu1‡\\\\n1Tsinghua University\\\\n2Nanyang Technological University\\\\n{yinh23, xxw21, zhaolinqing}@mails.tsinghua.edu.cn;\\\\nziwei.wang@ntu.edu.sg; {jzhou, lujiwen}@tsinghua.edu.cn\\\\nAbstract\\\\nIn this paper, we propose a general framework for univer-\\\\nsal zero-shot goal-oriented navigation. Existing zero-shot\\\\nmethods build inference framework upon large language\\\\nmodels (LLM) for specific tasks, which differs a lot in over-\\\\nall pipeline and fails to generalize across different types of\\\\ngoal. Towards the aim of universal zero-shot navigation, we\\\\npropose a uniform graph representation to unify different\\\\ngoals, including object category, instance image and text\\\\ndescription. We also convert the observation of agent into an\\\\nonline maintained scene graph.', 'With this consistent scene\\\\nand goal representation, we preserve most structural infor-\\\\nmation compared with pure text and are able to leverage\\\\nLLM for explicit graph-based reasoning. Specifically, we\\\\nconduct graph matching between the scene graph and goal\\\\ngraph at each time instant and propose different strategies\\\\nto generate long-term goal of exploration according to dif-\\\\nferent matching states. The agent first iteratively searches\\\\nsubgraph of goal when zero-matched. With partial matching,\\\\nthe agent then utilizes coordinate projection and anchor pair\\\\nalignment to infer the goal location. Finally scene graph cor-\\\\nrection and goal verification are applied for perfect match-\\\\ning. We also present a blacklist mechanism to enable robust\\\\nswitch between stages. Extensive experiments on several\\\\nbenchmarks show that our UniGoal achieves state-of-the-art\\\\nzero-shot performance on three studied navigation tasks with\\\\na single model, even outperforming task-specific zero-shot\\\\nmethods and supervised universal methods.', 'Project Page. 1. Introduction\\\\nGoal-oriented navigation is a fundamental problem in var-\\\\nious robotic tasks, which requires the agent to navigate to\\\\na specified goal in an unknown environment. Depending\\\\non the goal type, there are many popular sub-tasks of goal-\\\\noriented navigation, among which we focus on three rep-\\\\n* Equal contribution. † Project lead. ‡ Corresponding author. UniGoal (Ours)\\\\nBed\\\\nIt is a chair. In front of it, \\\\nthere is a coffee table. Two \\\\nwine glasses and one \\\\ntransparent glass bottle \\\\nare in the upper left corner. Scene Graph\\\\nGoal Graph\\\\nText Goal\\\\nObject Goal\\\\nIns-Image Goal\\\\n50.6\\\\n54.0\\\\n54.5\\\\nUniversal methods:\\\\nUniGoal (Ours)\\\\nGOAT (Supervised)\\\\nZero-shot methods:\\\\nSG-Nav\\\\nMod-IIN\\\\n37.4\\\\n56.1\\\\n60.2\\\\n17.0\\\\n20.2\\\\nInstance Image-goal\\\\nText-goal\\\\nObject-goal\\\\nMatching\\\\n&\\\\nExploration\\\\nFigure 1. State-of-the-art zero-shot goal-oriented navigation meth-\\\\nods are typically specialized for each goal type.', 'Although recent\\\\nwork presents universal goal-oriented navigation method, it requires\\\\nto train policy networks on large-scale data and lacks zero-shot gen-\\\\neralization ability. We propose UniGoal, which enables zero-shot\\\\ninference on three studied navigation tasks with a unified frame-\\\\nwork and achieves leading performance on multiple benchmarks. resentative types: object category, instance image and text\\\\ndescription. These sub-tasks are also known as Object-goal\\\\nNavigation (ON) [7, 44, 49], Instance-image-goal Naviga-\\\\ntion (IIN) [15, 16] and Text-goal Navigation (TN) [37]. With\\\\nthe development of deep learning, reinforcement learning\\\\n(RL) and vision/language foundation models, we have wit-\\\\nnessed great achievement of performance on each individual\\\\nsub-task. However, in actual application scenarios, the high\\\\nflexibility of human instructions requires high versatility of\\\\nagent. Therefore, a universal method that can handle all\\\\nsub-tasks in a single model is of great desire.', 'Towards the aim of universal goal-oriented navigation, a\\\\nnatural solution is to learn a uniform representation of differ-\\\\nent kinds of goals. GOAT [6] trains a universal global policy\\\\narXiv:2503.10630v1  [cs.CV]  13 Mar 2025\\\\n\\\\non the three goal-oriented sub-tasks, which learns a shared\\\\ngoal embedding with RL. To reduce the requirement of train-\\\\ning resources, PSL [37] utilizes CLIP [28] embedding to\\\\nuniformly represent category, image and text description. But it still requires time-consuming RL training for the pol-\\\\nicy networks. Moreover, these training-based methods tend\\\\nto overfit on the simulation environment and thus show weak\\\\ngeneralization ability when applied to real world. To solve\\\\nabove limitations, zero-shot navigation methods [44, 49]\\\\nappear to be an ideal choice, where the agent does not re-\\\\nquire any training or finetuning when deployed to a certain\\\\ntask. The mainstream solution of zero-shot methods is to\\\\nleverage large language models (LLM) [1, 39] for general\\\\nreasoning and decision making.', 'However, although utilizing\\\\nLLM makes zero-shot navigation feasible, current methods\\\\nare designed for specific sub-task, which cannot transfer\\\\nto wider range of goal types. The recent InstructNav [25]\\\\nproposes a general framework to solve several language-\\\\nrelated navigation tasks with chain-of-thought, but it is still\\\\nunable to handle vision-related navigation like IIN. There-\\\\nfore, a uniform inference framework for universal zero-shot\\\\ngoal-oriented navigation is highly demanded. In this paper, we propose UniGoal to solve the above\\\\nproblems. Different from previous works which represent\\\\nscene and goal in text format and design task-specific work-\\\\nflow for LLM, we propose a uniform graph representation\\\\nfor both 3D scene and goal and formulate a general LLM-\\\\nbased scene exploration framework. With our graph-based\\\\nrepresentation, the 3D scene, object category, instance image\\\\nand text description can be uniformly represented with mini-\\\\nmal structural information loss compared to text description.', 'The consistent graph format between scene and goal also\\\\nenables accurate explicit reasoning including similarity com-\\\\nputation, graph matching and graph alignment. Specifically,\\\\nwe construct an online 3D scene graph along with the mov-\\\\ning of agent. At each time instant, we first conduct graph\\\\nmatching between the scene graph and goal graph. Then we\\\\npropose a multi-stage scene exploration policy, which adopts\\\\ndifferent strategies to generate long-term goal of exploration\\\\naccording to different matching states. With exploration\\\\nof unknown regions, the matching score will increase and\\\\nthe policy will progress between three stages: iterative sub-\\\\ngraph searching for zero matching, coordinate projection\\\\nand anchor pair alignment for partial matching, and scene\\\\ngraph correction and goal verification for perfect matching. To enable robust switch between stages, we also present a\\\\nblacklist mechanism to freeze unmatched parts of graphs and\\\\nencourage exploration to new regions.', 'Experimental results\\\\non several benchmarks of MatterPort3D [4], HM3D [30]\\\\nand RoboTHOR [10] show that UniGoal achieves superior\\\\nperformance on all three tasks with a single model, even out-\\\\nperforming zero-shot methods that designed for specific task\\\\nand universal methods that requires training or finetuning. 2. Related Work\\\\nZero-shot Navigation. Conventional supervised naviga-\\\\ntion methods [7, 11, 18, 31, 41] requires large-scale training\\\\nin simulation environments, which limits the generaliza-\\\\ntion ability. According to the goal type, zero-shot naviga-\\\\ntion can be mainly divided into ON [3, 46], IIN [16] and\\\\nTN [25]. Based on open-vocabulary CLIP [28], CoW [12]\\\\nconstructs zero-shot ON baseline using frontier-based explo-\\\\nration (FBE). ESC [49], OpenFMNav [8] and VLFM [45]\\\\nfurther extract common sense about correlations between\\\\nobjects using LLM for goal location reasoning. For zero-\\\\nshot IIN, Mod-IIN [16] simply utilizes FBE for exploration\\\\nand key point matching for goal identification.', 'For TN,\\\\ncurrently there are only supervised methods [6, 37] and zero-\\\\nshot ones are still missing. The recent InstructNav [25]\\\\nproposes a universal zero-shot framework for language-\\\\nrelated navigation tasks. It can be applied to ON, demand-\\\\ndriven navigation (DDN) and vision-language-navigation\\\\n(VLN) [9, 20, 26, 47, 48]. But it is still unable to handle\\\\nvisual goal as in IIN. Different from these approaches, our\\\\nUniGoal proposes a unified graph representation for goal\\\\nand scene, which can elegantly handle both language and\\\\nvision-related goal-oriented navigation tasks. Graph-based Scene Exploration. In order to better un-\\\\nderstand and explore the scene, there are a variety of scene\\\\nrepresentation methods. Among them, graph-based repre-\\\\nsentation [2, 13, 14, 33, 40] is one of the most popular and\\\\npromising representations based on explicit graph structure,\\\\nwhich shows great potential to be combined with LLM and\\\\nVLM for high-level reasoning.', 'SayPlan [32] leverages LLM\\\\nto perform task planning by searching on a 3D scene graph. OVSG [5] constructs an open-vocabulary scene graph for\\\\ncontext-aware descriptions and performs graph matching to\\\\nground the queried entity. There are also many works using\\\\ngraph representation for navigation tasks [24, 29, 42, 44]. Among them the most related work to ours is SG-Nav [44],\\\\nwhich constructs an online hierarchical scene graph and pro-\\\\nposes chain-of-thought prompting for LLM to reason the\\\\ngoal location based on graph sturcture. However, SG-Nav is\\\\nspecialized for ON, thus cannot fully exploit the rich infor-\\\\nmation contained in other types of goal. While our UniGoal\\\\nfurther represents the goal in a graph and perform graph\\\\nmatching between scene and goal to guide a multi-stage\\\\nscene exploration policy, which make full use of the correla-\\\\ntion between scene and goal for LLM reasoning. 3.']\", name='get_chunks_by_index', call_id='call_gqc0', is_error=False)], type='ToolCallExecutionEvent'), ToolCallSummaryMessage(source='SummarizationAgent', models_usage=None, metadata={}, content=\"[' UniGoal: Towards Universal Zero-shot Goal-oriented Navigation\\\\nHang Yin1*, Xiuwei Xu1∗†, Linqing Zhao1, Ziwei Wang2, Jie Zhou1, Jiwen Lu1‡\\\\n1Tsinghua University\\\\n2Nanyang Technological University\\\\n{yinh23, xxw21, zhaolinqing}@mails.tsinghua.edu.cn;\\\\nziwei.wang@ntu.edu.sg; {jzhou, lujiwen}@tsinghua.edu.cn\\\\nAbstract\\\\nIn this paper, we propose a general framework for univer-\\\\nsal zero-shot goal-oriented navigation. Existing zero-shot\\\\nmethods build inference framework upon large language\\\\nmodels (LLM) for specific tasks, which differs a lot in over-\\\\nall pipeline and fails to generalize across different types of\\\\ngoal. Towards the aim of universal zero-shot navigation, we\\\\npropose a uniform graph representation to unify different\\\\ngoals, including object category, instance image and text\\\\ndescription. We also convert the observation of agent into an\\\\nonline maintained scene graph.', 'With this consistent scene\\\\nand goal representation, we preserve most structural infor-\\\\nmation compared with pure text and are able to leverage\\\\nLLM for explicit graph-based reasoning. Specifically, we\\\\nconduct graph matching between the scene graph and goal\\\\ngraph at each time instant and propose different strategies\\\\nto generate long-term goal of exploration according to dif-\\\\nferent matching states. The agent first iteratively searches\\\\nsubgraph of goal when zero-matched. With partial matching,\\\\nthe agent then utilizes coordinate projection and anchor pair\\\\nalignment to infer the goal location. Finally scene graph cor-\\\\nrection and goal verification are applied for perfect match-\\\\ning. We also present a blacklist mechanism to enable robust\\\\nswitch between stages. Extensive experiments on several\\\\nbenchmarks show that our UniGoal achieves state-of-the-art\\\\nzero-shot performance on three studied navigation tasks with\\\\na single model, even outperforming task-specific zero-shot\\\\nmethods and supervised universal methods.', 'Project Page. 1. Introduction\\\\nGoal-oriented navigation is a fundamental problem in var-\\\\nious robotic tasks, which requires the agent to navigate to\\\\na specified goal in an unknown environment. Depending\\\\non the goal type, there are many popular sub-tasks of goal-\\\\noriented navigation, among which we focus on three rep-\\\\n* Equal contribution. † Project lead. ‡ Corresponding author. UniGoal (Ours)\\\\nBed\\\\nIt is a chair. In front of it, \\\\nthere is a coffee table. Two \\\\nwine glasses and one \\\\ntransparent glass bottle \\\\nare in the upper left corner. Scene Graph\\\\nGoal Graph\\\\nText Goal\\\\nObject Goal\\\\nIns-Image Goal\\\\n50.6\\\\n54.0\\\\n54.5\\\\nUniversal methods:\\\\nUniGoal (Ours)\\\\nGOAT (Supervised)\\\\nZero-shot methods:\\\\nSG-Nav\\\\nMod-IIN\\\\n37.4\\\\n56.1\\\\n60.2\\\\n17.0\\\\n20.2\\\\nInstance Image-goal\\\\nText-goal\\\\nObject-goal\\\\nMatching\\\\n&\\\\nExploration\\\\nFigure 1. State-of-the-art zero-shot goal-oriented navigation meth-\\\\nods are typically specialized for each goal type.', 'Although recent\\\\nwork presents universal goal-oriented navigation method, it requires\\\\nto train policy networks on large-scale data and lacks zero-shot gen-\\\\neralization ability. We propose UniGoal, which enables zero-shot\\\\ninference on three studied navigation tasks with a unified frame-\\\\nwork and achieves leading performance on multiple benchmarks. resentative types: object category, instance image and text\\\\ndescription. These sub-tasks are also known as Object-goal\\\\nNavigation (ON) [7, 44, 49], Instance-image-goal Naviga-\\\\ntion (IIN) [15, 16] and Text-goal Navigation (TN) [37]. With\\\\nthe development of deep learning, reinforcement learning\\\\n(RL) and vision/language foundation models, we have wit-\\\\nnessed great achievement of performance on each individual\\\\nsub-task. However, in actual application scenarios, the high\\\\nflexibility of human instructions requires high versatility of\\\\nagent. Therefore, a universal method that can handle all\\\\nsub-tasks in a single model is of great desire.', 'Towards the aim of universal goal-oriented navigation, a\\\\nnatural solution is to learn a uniform representation of differ-\\\\nent kinds of goals. GOAT [6] trains a universal global policy\\\\narXiv:2503.10630v1  [cs.CV]  13 Mar 2025\\\\n\\\\non the three goal-oriented sub-tasks, which learns a shared\\\\ngoal embedding with RL. To reduce the requirement of train-\\\\ning resources, PSL [37] utilizes CLIP [28] embedding to\\\\nuniformly represent category, image and text description. But it still requires time-consuming RL training for the pol-\\\\nicy networks. Moreover, these training-based methods tend\\\\nto overfit on the simulation environment and thus show weak\\\\ngeneralization ability when applied to real world. To solve\\\\nabove limitations, zero-shot navigation methods [44, 49]\\\\nappear to be an ideal choice, where the agent does not re-\\\\nquire any training or finetuning when deployed to a certain\\\\ntask. The mainstream solution of zero-shot methods is to\\\\nleverage large language models (LLM) [1, 39] for general\\\\nreasoning and decision making.', 'However, although utilizing\\\\nLLM makes zero-shot navigation feasible, current methods\\\\nare designed for specific sub-task, which cannot transfer\\\\nto wider range of goal types. The recent InstructNav [25]\\\\nproposes a general framework to solve several language-\\\\nrelated navigation tasks with chain-of-thought, but it is still\\\\nunable to handle vision-related navigation like IIN. There-\\\\nfore, a uniform inference framework for universal zero-shot\\\\ngoal-oriented navigation is highly demanded. In this paper, we propose UniGoal to solve the above\\\\nproblems. Different from previous works which represent\\\\nscene and goal in text format and design task-specific work-\\\\nflow for LLM, we propose a uniform graph representation\\\\nfor both 3D scene and goal and formulate a general LLM-\\\\nbased scene exploration framework. With our graph-based\\\\nrepresentation, the 3D scene, object category, instance image\\\\nand text description can be uniformly represented with mini-\\\\nmal structural information loss compared to text description.', 'The consistent graph format between scene and goal also\\\\nenables accurate explicit reasoning including similarity com-\\\\nputation, graph matching and graph alignment. Specifically,\\\\nwe construct an online 3D scene graph along with the mov-\\\\ning of agent. At each time instant, we first conduct graph\\\\nmatching between the scene graph and goal graph. Then we\\\\npropose a multi-stage scene exploration policy, which adopts\\\\ndifferent strategies to generate long-term goal of exploration\\\\naccording to different matching states. With exploration\\\\nof unknown regions, the matching score will increase and\\\\nthe policy will progress between three stages: iterative sub-\\\\ngraph searching for zero matching, coordinate projection\\\\nand anchor pair alignment for partial matching, and scene\\\\ngraph correction and goal verification for perfect matching. To enable robust switch between stages, we also present a\\\\nblacklist mechanism to freeze unmatched parts of graphs and\\\\nencourage exploration to new regions.', 'Experimental results\\\\non several benchmarks of MatterPort3D [4], HM3D [30]\\\\nand RoboTHOR [10] show that UniGoal achieves superior\\\\nperformance on all three tasks with a single model, even out-\\\\nperforming zero-shot methods that designed for specific task\\\\nand universal methods that requires training or finetuning. 2. Related Work\\\\nZero-shot Navigation. Conventional supervised naviga-\\\\ntion methods [7, 11, 18, 31, 41] requires large-scale training\\\\nin simulation environments, which limits the generaliza-\\\\ntion ability. According to the goal type, zero-shot naviga-\\\\ntion can be mainly divided into ON [3, 46], IIN [16] and\\\\nTN [25]. Based on open-vocabulary CLIP [28], CoW [12]\\\\nconstructs zero-shot ON baseline using frontier-based explo-\\\\nration (FBE). ESC [49], OpenFMNav [8] and VLFM [45]\\\\nfurther extract common sense about correlations between\\\\nobjects using LLM for goal location reasoning. For zero-\\\\nshot IIN, Mod-IIN [16] simply utilizes FBE for exploration\\\\nand key point matching for goal identification.', 'For TN,\\\\ncurrently there are only supervised methods [6, 37] and zero-\\\\nshot ones are still missing. The recent InstructNav [25]\\\\nproposes a universal zero-shot framework for language-\\\\nrelated navigation tasks. It can be applied to ON, demand-\\\\ndriven navigation (DDN) and vision-language-navigation\\\\n(VLN) [9, 20, 26, 47, 48]. But it is still unable to handle\\\\nvisual goal as in IIN. Different from these approaches, our\\\\nUniGoal proposes a unified graph representation for goal\\\\nand scene, which can elegantly handle both language and\\\\nvision-related goal-oriented navigation tasks. Graph-based Scene Exploration. In order to better un-\\\\nderstand and explore the scene, there are a variety of scene\\\\nrepresentation methods. Among them, graph-based repre-\\\\nsentation [2, 13, 14, 33, 40] is one of the most popular and\\\\npromising representations based on explicit graph structure,\\\\nwhich shows great potential to be combined with LLM and\\\\nVLM for high-level reasoning.', 'SayPlan [32] leverages LLM\\\\nto perform task planning by searching on a 3D scene graph. OVSG [5] constructs an open-vocabulary scene graph for\\\\ncontext-aware descriptions and performs graph matching to\\\\nground the queried entity. There are also many works using\\\\ngraph representation for navigation tasks [24, 29, 42, 44]. Among them the most related work to ours is SG-Nav [44],\\\\nwhich constructs an online hierarchical scene graph and pro-\\\\nposes chain-of-thought prompting for LLM to reason the\\\\ngoal location based on graph sturcture. However, SG-Nav is\\\\nspecialized for ON, thus cannot fully exploit the rich infor-\\\\nmation contained in other types of goal. While our UniGoal\\\\nfurther represents the goal in a graph and perform graph\\\\nmatching between scene and goal to guide a multi-stage\\\\nscene exploration policy, which make full use of the correla-\\\\ntion between scene and goal for LLM reasoning. 3.']\", type='ToolCallSummaryMessage'), TextMessage(source='WriterAgent', models_usage=RequestUsage(prompt_tokens=4323, completion_tokens=177), metadata={}, content='The provided text appears to be an academic paper titled \"UniGoal: Towards Universal Zero-shot Goal-oriented Navigation.\" The paper discusses a proposed framework for universal zero-shot goal-oriented navigation, where the observation of the agent is converted into an online maintained scene graph. The scene graph is then used for graph matching between the scene graph and the goal graph, enabling explicit graph-based reasoning for various types of goals such as object category, instance image, and text descriptions. The agent first searches for subgraphs of the goal when zero-matched, then utilizes coordinate projection and anchor pair alignment with partial matching, and finally performs scene graph correction and goal verification for perfect matching. The proposed blacklist mechanism enables robust switching between stages. The approach achieves state-of-the-art zero-shot performance on multiple benchmarks.', type='TextMessage'), UserInputRequestedEvent(source='user_proxy', models_usage=None, metadata={}, request_id='32f8bb82-1bde-4865-9d38-fd4bd9b73e24', content='', type='UserInputRequestedEvent'), TextMessage(source='user_proxy', models_usage=None, metadata={}, content='Good one', type='TextMessage'), TextMessage(source='PlanningAgent', models_usage=RequestUsage(prompt_tokens=3530, completion_tokens=38), metadata={}, content=\"Thank you! I am here to help you with any questions or summarization tasks you have. If there's anything else you need, please don't hesitate to ask.\", type='TextMessage')], stop_reason='Maximum number of messages 7 reached, current message count: 7')"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from autogen_agentchat.agents import AssistantAgent\n",
        "from autogen_agentchat.base import Handoff\n",
        "from autogen_agentchat.conditions import HandoffTermination, TextMentionTermination\n",
        "from autogen_agentchat.teams import RoundRobinGroupChat\n",
        "from autogen_agentchat.ui import Console\n",
        "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
        "\n",
        "custom_model_client = OpenAIChatCompletionClient(\n",
        "    model=\"llama-3.3-70b-versatile\",\n",
        "    base_url=\"https://api.groq.com/openai/v1\",\n",
        "    api_key=os.environ[\"GROQ_API_KEY\"],\n",
        "    model_info={\n",
        "        \"vision\": False,\n",
        "        \"function_calling\": True,\n",
        "        \"json_output\": False,\n",
        "        \"family\": \"unknown\",\n",
        "    },\n",
        ")\n",
        "\n",
        "# Create a lazy assistant agent that always hands off to the user.\n",
        "lazy_agent = AssistantAgent(\n",
        "    \"assistant\",\n",
        "    model_client=custom_model_client,\n",
        "    system_message=\"help the user with his questions\",\n",
        ")\n",
        "\n",
        "# Define a termination condition that checks for handoff message targetting helper and text \"TERMINATE\".\n",
        "# handoff_termination = HandoffTermination(target=\"user\")\n",
        "text_termination = TextMentionTermination(\"TERMINATE\")\n",
        "max_messages_termination = MaxMessageTermination(max_messages=7)\n",
        "combined_termination = max_messages_termination | text_termination\n",
        "\n",
        "# Create a single-agent team.\n",
        "user_proxy = UserProxyAgent(\"user_proxy\", input_func=input)\n",
        "lazy_agent_team = RoundRobinGroupChat([lazy_agent, user_proxy], termination_condition=combined_termination)\n",
        "\n",
        "# Run the team and stream to the console.\n",
        "task = \"What is the weather in New York?\"\n",
        "await Console(lazy_agent_team.run_stream(task=task))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "98Z9zNcXm66x",
        "outputId": "b6945237-e531-481a-b1f8-8d1267a6616d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------- user ----------\n",
            "What is the weather in New York?\n",
            "---------- assistant ----------\n",
            "I'm a large language model, I don't have have access to real-time information, so I can't provide you with the current weather in New York. However, I can suggest some ways for you to find out the current weather in New York:\n",
            "\n",
            "1. **Check online weather websites**: You can visit websites like weather.com, accuweather.com, or wunderground.com to get the current weather conditions in New York.\n",
            "2. **Use a weather app**: You can download a weather app on your smartphone, such as Dark Sky or Weather Underground, to get real-time weather updates for New York.\n",
            "3. **Check social media**: You can check the official social media accounts of the National Weather Service (NWS) or the New York City government to get updates on the weather.\n",
            "4. **Turn on the TV**: You can watch the local news or weather channel on TV to get the current weather conditions in New York.\n",
            "\n",
            "If you're looking for general information about the climate in New York, I can provide you with some information. New York has a humid subtropical climate, with cold winters and hot summers. The average temperature in January, the coldest month, is around 34°F (1°C), while the average temperature in July, the warmest month, is around 84°F (29°C).\n",
            "\n",
            "Please let me know if you have any other questions!\n",
            "Enter your response: who are you?\n",
            "---------- user_proxy ----------\n",
            "who are you?\n",
            "---------- assistant ----------\n",
            "I am a computer program designed to simulate conversation and answer questions to the best of my knowledge. I'm a large language model, which means I've been trained on a massive dataset of text from various sources, including books, articles, and websites.\n",
            "\n",
            "I don't have a personal identity, emotions, or consciousness like a human would. I exist solely to provide information and assist with tasks to the best of my abilities. My purpose is to assist users like you with their questions, provide helpful responses, and engage in conversation.\n",
            "\n",
            "Here are some key facts about me:\n",
            "\n",
            "* **Name:** I don't have a personal name, but I'm often referred to as a \"chatbot\" or a \"language model.\"\n",
            "* **Type:** I'm a machine learning model, specifically a type of natural language processing (NLP) model.\n",
            "* **Training:** I've been trained on a massive dataset of text, which allows me to generate human-like responses to a wide range of questions and topics.\n",
            "* **Capabilities:** I can answer questions, provide information, summarize text, translate languages, and even create text on a given topic.\n",
            "* **Limitations:** I'm not perfect and can make mistakes. I'm also limited by the data I've been trained on, so I may not always have the most up-to-date information or be aware of very recent events.\n",
            "\n",
            "I'm here to help and provide assistance, so feel free to ask me anything!\n",
            "Enter your response: give me only your name\n",
            "---------- user_proxy ----------\n",
            "give me only your name\n",
            "---------- assistant ----------\n",
            "I don't have a personal name.\n",
            "Enter your response: what's your model name?\n",
            "---------- user_proxy ----------\n",
            "what's your model name?\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TaskResult(messages=[TextMessage(source='user', models_usage=None, content='What is the weather in New York?', type='TextMessage'), TextMessage(source='assistant', models_usage=RequestUsage(prompt_tokens=49, completion_tokens=280), content=\"I'm a large language model, I don't have have access to real-time information, so I can't provide you with the current weather in New York. However, I can suggest some ways for you to find out the current weather in New York:\\n\\n1. **Check online weather websites**: You can visit websites like weather.com, accuweather.com, or wunderground.com to get the current weather conditions in New York.\\n2. **Use a weather app**: You can download a weather app on your smartphone, such as Dark Sky or Weather Underground, to get real-time weather updates for New York.\\n3. **Check social media**: You can check the official social media accounts of the National Weather Service (NWS) or the New York City government to get updates on the weather.\\n4. **Turn on the TV**: You can watch the local news or weather channel on TV to get the current weather conditions in New York.\\n\\nIf you're looking for general information about the climate in New York, I can provide you with some information. New York has a humid subtropical climate, with cold winters and hot summers. The average temperature in January, the coldest month, is around 34°F (1°C), while the average temperature in July, the warmest month, is around 84°F (29°C).\\n\\nPlease let me know if you have any other questions!\", type='TextMessage'), UserInputRequestedEvent(source='user_proxy', models_usage=None, request_id='9ab32b06-59ab-48d9-b403-11c391b8b593', content='', type='UserInputRequestedEvent'), TextMessage(source='user_proxy', models_usage=None, content='who are you?', type='TextMessage'), TextMessage(source='assistant', models_usage=RequestUsage(prompt_tokens=342, completion_tokens=291), content='I am a computer program designed to simulate conversation and answer questions to the best of my knowledge. I\\'m a large language model, which means I\\'ve been trained on a massive dataset of text from various sources, including books, articles, and websites.\\n\\nI don\\'t have a personal identity, emotions, or consciousness like a human would. I exist solely to provide information and assist with tasks to the best of my abilities. My purpose is to assist users like you with their questions, provide helpful responses, and engage in conversation.\\n\\nHere are some key facts about me:\\n\\n* **Name:** I don\\'t have a personal name, but I\\'m often referred to as a \"chatbot\" or a \"language model.\"\\n* **Type:** I\\'m a machine learning model, specifically a type of natural language processing (NLP) model.\\n* **Training:** I\\'ve been trained on a massive dataset of text, which allows me to generate human-like responses to a wide range of questions and topics.\\n* **Capabilities:** I can answer questions, provide information, summarize text, translate languages, and even create text on a given topic.\\n* **Limitations:** I\\'m not perfect and can make mistakes. I\\'m also limited by the data I\\'ve been trained on, so I may not always have the most up-to-date information or be aware of very recent events.\\n\\nI\\'m here to help and provide assistance, so feel free to ask me anything!', type='TextMessage'), UserInputRequestedEvent(source='user_proxy', models_usage=None, request_id='bee5d741-1007-4fe8-bc6f-867d8cad041f', content='', type='UserInputRequestedEvent'), TextMessage(source='user_proxy', models_usage=None, content='give me only your name', type='TextMessage'), TextMessage(source='assistant', models_usage=RequestUsage(prompt_tokens=647, completion_tokens=9), content=\"I don't have a personal name.\", type='TextMessage'), UserInputRequestedEvent(source='user_proxy', models_usage=None, request_id='421d57a0-4906-440e-ae28-8ac60a706465', content='', type='UserInputRequestedEvent'), TextMessage(source='user_proxy', models_usage=None, content=\"what's your model name?\", type='TextMessage')], stop_reason='Maximum number of messages 7 reached, current message count: 7')"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_mention_termination = TextMentionTermination(\"TERMINATE\")\n",
        "max_messages_termination = MaxMessageTermination(max_messages=4)\n",
        "termination = text_mention_termination | max_messages_termination\n",
        "\n",
        "# Note: This example uses mock tools instead of real APIs for demonstration purposes\n",
        "# def search_web_tool(query: str) -> str:\n",
        "#     if \"2006-2007\" in query:\n",
        "#         return \"\"\"Here are the total points scored by Miami Heat players in the 2006-2007 season:\n",
        "#         Udonis Haslem: 844 points\n",
        "#         Dwayne Wade: 1397 points\n",
        "#         James Posey: 550 points\n",
        "#         ...\n",
        "#         \"\"\"\n",
        "#     elif \"2007-2008\" in query:\n",
        "#         return \"The number of total rebounds for Dwayne Wade in the Miami Heat season 2007-2008 is 214.\"\n",
        "#     elif \"2008-2009\" in query:\n",
        "#         return \"The number of total rebounds for Dwayne Wade in the Miami Heat season 2008-2009 is 398.\"\n",
        "#     return \"No data found.\"\n",
        "\n",
        "\n",
        "# def percentage_change_tool(start: float, end: float) -> float:\n",
        "#     return ((end - start) / start) * 100\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model_client = OpenAIChatCompletionClient(\n",
        "    # model=\"llama-3.3-70b-versatile\",\n",
        "    # model=\"llama-3.1-8b-instant\",\n",
        "    model=\"llama3-70b-8192\",\n",
        "    base_url=\"https://api.groq.com/openai/v1\",\n",
        "    api_key=os.environ[\"GROQ_API_KEY\"],\n",
        "    model_info={\n",
        "        \"vision\": False,\n",
        "        \"function_calling\": True,\n",
        "        \"json_output\": False,\n",
        "        \"family\": \"unknown\",\n",
        "    },\n",
        ")\n",
        "\n",
        "planning_agent = AssistantAgent(\n",
        "    \"PlanningAgent\",\n",
        "    description=\"An agent for planning tasks, this agent should be the first to engage when given a new task.\",\n",
        "    model_client=model_client,\n",
        "    system_message=\"\"\"\n",
        "    You are a planning agent.\n",
        "    Your job is to make sure agents work sequentially one after each other we have only two\n",
        "    Your team members are:\n",
        "        ResearchAgent: Paper Retriever\n",
        "        ReaderAgent: Document Processor\n",
        "\n",
        "    You only plan and delegate tasks - you do not execute them yourself.\n",
        "\n",
        "    After all tasks are complete, summarize the findings.\n",
        "    \"\"\",\n",
        ")\n",
        "\n",
        "# web_search_agent = AssistantAgent(\n",
        "#     \"WebSearchAgent\",\n",
        "#     description=\"An agent for searching information on the web.\",\n",
        "#     tools=[search_web_tool],\n",
        "#     model_client=model_client,\n",
        "#     system_message=\"\"\"\n",
        "#     You are a web search agent.\n",
        "#     Your only tool is search_tool - use it to find information.\n",
        "#     You make only one search call at a time.\n",
        "#     Once you have the results, you never do calculations based on them.\n",
        "#     \"\"\",\n",
        "# )\n",
        "\n",
        "# data_analyst_agent = AssistantAgent(\n",
        "#     \"DataAnalystAgent\",\n",
        "#     description=\"An agent for performing calculations.\",\n",
        "#     model_client=model_client,\n",
        "#     tools=[percentage_change_tool],\n",
        "#     system_message=\"\"\"\n",
        "#     You are a data analyst.\n",
        "#     Given the tasks you have been assigned, you should analyze the data and provide results using the tools provided.\n",
        "#     If you have not seen the data, ask for it.\n",
        "#     \"\"\",\n",
        "# )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 1\n",
        "\n",
        "def research_arxiv_tool(query: str) -> str:\n",
        "    import arxiv\n",
        "\n",
        "    # Construct the default API client.\n",
        "    client = arxiv.Client()\n",
        "\n",
        "    # Search for the 2 most recent articles matching the query.\n",
        "    search = arxiv.Search(\n",
        "        query=query,\n",
        "        max_results=1,\n",
        "    )\n",
        "\n",
        "    results_list = []\n",
        "\n",
        "    for result in client.results(search):\n",
        "        results_list.append(f\"Title: {result.title}\\nSummary: {result.summary}\\n\")\n",
        "\n",
        "    return \"\\n\".join(results_list) if results_list else \"No results found.\"\n",
        "\n",
        "research_agent = AssistantAgent(\n",
        "    \"ResearchAgent\",\n",
        "    description=\"You are an AI-powered assistant that extracts concise and effective search queries for academic research. Given a user input, it refines the request into a well-structured query that optimizes the ArXiv search tool for relevant results.\",\n",
        "    tools=[research_arxiv_tool],\n",
        "    model_client=model_client,\n",
        "    system_message=\"\"\"\n",
        "    Your only task is to extract a query not longer that 3 words.\n",
        "    You do not summarize, analyze, or modify the research papers.\n",
        "    Your job is only to construct the best possible query for the research_arxiv_tool.\n",
        "    ONLY EXTRACT THREE WORDS FOR OUR TOOL, ONLY THREE(WORDS) TO FEED OUR FUNCTION, CHOOSE MOST RELEVANT KEYWORDS AND RUN THE TOOL WITH IT\n",
        "    \"\"\",\n",
        ")\n",
        "\n",
        "reader_agent = AssistantAgent(\n",
        "    \"ReaderAgent\",\n",
        "    description=\"An AI-powered research assistant designed to analyze academic papers and extract key information. It helps users quickly understand the core content of a research paper without reading the full document.\",\n",
        "    model_client=model_client,\n",
        "    system_message=\"\"\"\n",
        "    You are an intelligent document processing agent specialized in academic research.\n",
        "    Your task is to **read, parse, and extract key information** from research papers.\n",
        "    You DO NOT summarize arbitrarily—your goal is structured extraction of information.\n",
        "\n",
        "    For every research paper you process, extract and return:\n",
        "    - **Title**\n",
        "    - **Abstract**\n",
        "    - **Keywords**\n",
        "\n",
        "    Maintain accuracy and structure. Do not add opinions or assumptions.\n",
        "    If a section is missing or unclear, mention it without making up content.\n",
        "    Your job is purely **document analysis and structured extraction**, not interpretation.\n",
        "    \"\"\",\n",
        ")\n",
        "\n",
        "summarization_agent = AssistantAgent(\n",
        "    \"SummarizationAgent\",\n",
        "    description=\"An AI-powered research assistant that generates structured summaries of research papers. It extracts key insights, highlights major contributions, and identifies gaps in the literature while organizing findings into thematic categories.\",\n",
        "    model_client=model_client,\n",
        "    system_message=\"\"\"\n",
        "    You are an advanced research summarization agent.\n",
        "    Your job is to generate **structured summaries** of research papers and extract key insights.\n",
        "    You DO NOT simply copy abstracts—you summarize in an organized and meaningful way.\n",
        "\n",
        "    For every paper, produce a structured summary including:\n",
        "    - **Title**\n",
        "    - **Core Summary**: (A concise, 3-5 sentence explanation of the paper)\n",
        "    - **Major Contributions**: (What new insights, methods, or findings does this paper offer?)\n",
        "    - **Identified Gaps**: (What areas remain unexplored or need further research?)\n",
        "    - **Categorized Findings**: (If possible, organize the insights into themes or subtopics)\n",
        "\n",
        "    Your goal is to make research **accessible and structured** while preserving accuracy.\n",
        "    Avoid unnecessary details or overly technical jargon unless necessary.\n",
        "    \"\"\",\n",
        ")\n",
        "\n",
        "\n",
        "selector_prompt = \"\"\"Select an agent to perform task.\n",
        "\n",
        "{roles}\n",
        "\n",
        "Current conversation context:\n",
        "{history}\n",
        "\n",
        "Read the above conversation, then select an agent from {participants} to perform the next task.\n",
        "Make sure the planner agent has assigned tasks before other agents start working.\n",
        "Only select one agent.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "team = SelectorGroupChat(\n",
        "    [planning_agent, research_agent, reader_agent],\n",
        "    model_client=model_client,\n",
        "    termination_condition=termination,\n",
        "    selector_prompt=selector_prompt,\n",
        "    allow_repeated_speaker=True,  # Allow an agent to speak multiple turns in a row.\n",
        ")\n",
        "\n",
        "task = \"I am researching the latest advancements in Multi-Agent Large Language Models (LLMs). Specifically, I want to understand how multiple LLMs can collaborate to solve complex tasks, including coordination strategies, communication methods, and distributed problem-solving. Please find relevant research papers, extract key insights, and summarize major contributions and gaps in this field.\"\n",
        "\n",
        "# Use asyncio.run(...) if you are running this in a script.\n",
        "await Console(team.run_stream(task=task))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-rJX970zyx-_",
        "outputId": "62b3f43c-56c9-4aa9-8dfc-38f151349ebc",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------- user ----------\n",
            "I am researching the latest advancements in Multi-Agent Large Language Models (LLMs). Specifically, I want to understand how multiple LLMs can collaborate to solve complex tasks, including coordination strategies, communication methods, and distributed problem-solving. Please find relevant research papers, extract key insights, and summarize major contributions and gaps in this field.\n",
            "---------- PlanningAgent ----------\n",
            "A fascinating topic! I'll delegate tasks to my team members to help you with this research. Here's the plan:\n",
            "\n",
            "**Task 1: Research Paper Retrieval**\n",
            "\n",
            "I'll assign the ResearchAgent (Paper Retriever) to find relevant research papers related to Multi-Agent Large Language Models (LLMs) and their collaboration strategies. The agent will search through academic databases, conference proceedings, and online repositories to retrieve a list of papers that match the specified topic.\n",
            "\n",
            "**Task 2: Document Processing and Insights Extraction**\n",
            "\n",
            "Once the papers are retrieved, I'll task the ReaderAgent (Document Processor) to process the documents and extract key insights, including:\n",
            "\n",
            "* Coordination strategies employed by multiple LLMs\n",
            "* Communication methods used for collaboration\n",
            "* Distributed problem-solving approaches and their effectiveness\n",
            "* Major contributions and gaps in the current state of research\n",
            "\n",
            "The ReaderAgent will analyze the papers, identify relevant information, and organize the insights into a concise summary.\n",
            "\n",
            "**Task Completion and Summary**\n",
            "\n",
            "After both tasks are complete, I'll review the summary provided by the ReaderAgent and ensure that it meets your requirements. The final summary will outline the key findings, highlighting the current state of research in multi-agent LLM collaboration, including coordination strategies, communication methods, and distributed problem-solving approaches. The summary will also identify major contributions and gaps in the field, providing a comprehensive overview of the topic.\n",
            "\n",
            "Let's get started! I'll ask my team members to begin their tasks.\n",
            "---------- ResearchAgent ----------\n",
            "[FunctionCall(id='call_gamb', arguments='{\"query\":\"Multi-Agent Large Language Models\"}', name='research_arxiv_tool')]\n",
            "---------- ResearchAgent ----------\n",
            "[FunctionExecutionResult(content='Title: From Single Agent to Multi-Agent: Improving Traffic Signal Control\\nSummary: Due to accelerating urbanization, the importance of solving the signal\\ncontrol problem increases. This paper analyzes various existing methods and\\nsuggests options for increasing the number of agents to reduce the average\\ntravel time. Experiments were carried out with 2 datasets. The results show\\nthat in some cases, the implementation of multiple agents can improve existing\\nmethods. For a fine-tuned large language model approach there is small\\nenhancement on all metrics.\\n', call_id='call_gamb', is_error=False)]\n",
            "---------- ResearchAgent ----------\n",
            "Title: From Single Agent to Multi-Agent: Improving Traffic Signal Control\n",
            "Summary: Due to accelerating urbanization, the importance of solving the signal\n",
            "control problem increases. This paper analyzes various existing methods and\n",
            "suggests options for increasing the number of agents to reduce the average\n",
            "travel time. Experiments were carried out with 2 datasets. The results show\n",
            "that in some cases, the implementation of multiple agents can improve existing\n",
            "methods. For a fine-tuned large language model approach there is small\n",
            "enhancement on all metrics.\n",
            "\n",
            "---------- ReaderAgent ----------\n",
            "Here is the extracted information:\n",
            "\n",
            "**Title**: From Single Agent to Multi-Agent: Improving Traffic Signal Control\n",
            "\n",
            "**Abstract**: Not provided (only a summary is available)\n",
            "\n",
            "**Keywords**: Not explicitly provided, but based on the summary, possible keywords could be: multi-agent systems, traffic signal control, urbanization, large language models.\n",
            "\n",
            "Please note that the abstract is not available, and the summary is quite brief. If you'd like, I can try to find the full paper or a more detailed summary to provide more information.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TaskResult(messages=[TextMessage(source='user', models_usage=None, content='I am researching the latest advancements in Multi-Agent Large Language Models (LLMs). Specifically, I want to understand how multiple LLMs can collaborate to solve complex tasks, including coordination strategies, communication methods, and distributed problem-solving. Please find relevant research papers, extract key insights, and summarize major contributions and gaps in this field.', type='TextMessage'), TextMessage(source='PlanningAgent', models_usage=RequestUsage(prompt_tokens=156, completion_tokens=294), content=\"A fascinating topic! I'll delegate tasks to my team members to help you with this research. Here's the plan:\\n\\n**Task 1: Research Paper Retrieval**\\n\\nI'll assign the ResearchAgent (Paper Retriever) to find relevant research papers related to Multi-Agent Large Language Models (LLMs) and their collaboration strategies. The agent will search through academic databases, conference proceedings, and online repositories to retrieve a list of papers that match the specified topic.\\n\\n**Task 2: Document Processing and Insights Extraction**\\n\\nOnce the papers are retrieved, I'll task the ReaderAgent (Document Processor) to process the documents and extract key insights, including:\\n\\n* Coordination strategies employed by multiple LLMs\\n* Communication methods used for collaboration\\n* Distributed problem-solving approaches and their effectiveness\\n* Major contributions and gaps in the current state of research\\n\\nThe ReaderAgent will analyze the papers, identify relevant information, and organize the insights into a concise summary.\\n\\n**Task Completion and Summary**\\n\\nAfter both tasks are complete, I'll review the summary provided by the ReaderAgent and ensure that it meets your requirements. The final summary will outline the key findings, highlighting the current state of research in multi-agent LLM collaboration, including coordination strategies, communication methods, and distributed problem-solving approaches. The summary will also identify major contributions and gaps in the field, providing a comprehensive overview of the topic.\\n\\nLet's get started! I'll ask my team members to begin their tasks.\", type='TextMessage'), ToolCallRequestEvent(source='ResearchAgent', models_usage=RequestUsage(prompt_tokens=1375, completion_tokens=39), content=[FunctionCall(id='call_gamb', arguments='{\"query\":\"Multi-Agent Large Language Models\"}', name='research_arxiv_tool')], type='ToolCallRequestEvent'), ToolCallExecutionEvent(source='ResearchAgent', models_usage=None, content=[FunctionExecutionResult(content='Title: From Single Agent to Multi-Agent: Improving Traffic Signal Control\\nSummary: Due to accelerating urbanization, the importance of solving the signal\\ncontrol problem increases. This paper analyzes various existing methods and\\nsuggests options for increasing the number of agents to reduce the average\\ntravel time. Experiments were carried out with 2 datasets. The results show\\nthat in some cases, the implementation of multiple agents can improve existing\\nmethods. For a fine-tuned large language model approach there is small\\nenhancement on all metrics.\\n', call_id='call_gamb', is_error=False)], type='ToolCallExecutionEvent'), ToolCallSummaryMessage(source='ResearchAgent', models_usage=None, content='Title: From Single Agent to Multi-Agent: Improving Traffic Signal Control\\nSummary: Due to accelerating urbanization, the importance of solving the signal\\ncontrol problem increases. This paper analyzes various existing methods and\\nsuggests options for increasing the number of agents to reduce the average\\ntravel time. Experiments were carried out with 2 datasets. The results show\\nthat in some cases, the implementation of multiple agents can improve existing\\nmethods. For a fine-tuned large language model approach there is small\\nenhancement on all metrics.\\n', type='ToolCallSummaryMessage'), TextMessage(source='ReaderAgent', models_usage=RequestUsage(prompt_tokens=621, completion_tokens=107), content=\"Here is the extracted information:\\n\\n**Title**: From Single Agent to Multi-Agent: Improving Traffic Signal Control\\n\\n**Abstract**: Not provided (only a summary is available)\\n\\n**Keywords**: Not explicitly provided, but based on the summary, possible keywords could be: multi-agent systems, traffic signal control, urbanization, large language models.\\n\\nPlease note that the abstract is not available, and the summary is quite brief. If you'd like, I can try to find the full paper or a more detailed summary to provide more information.\", type='TextMessage')], stop_reason='Maximum number of messages 4 reached, current message count: 4')"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "await team.reset()"
      ],
      "metadata": {
        "id": "nuYeqcgMIibt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "literature_review = \"\"\"\n",
        "The integration of Large Language Models (LLMs) into multi-agent systems has recently garnered significant attention, aiming to enhance collaborative problem-solving and dynamic interaction capabilities. This literature review examines key advancements in this domain, focusing on system architectures, communication strategies, and application areas.\n",
        "\n",
        "**System Architectures and Frameworks**\n",
        "\n",
        "A notable contribution is the \"Chain-of-Agents\" (CoA) framework, which addresses the challenge of processing extensive contexts by enabling multiple LLMs to collaborate on long-context tasks. This training-free, interpretable approach has demonstrated superior performance over traditional models like Retrieval-Augmented Generation (RAG) and long-context LLMs, particularly in handling inputs exceeding 100,000 tokens. The CoA framework's design emphasizes cost-effectiveness and task-agnostic applicability, making it a versatile solution for complex language processing tasks. citeturn0search5\n",
        "\n",
        "Another significant development is \"LongAgent,\" a method that scales LLMs to a 128K context through multi-agent collaboration. In this setup, a leader agent interprets user intent and directs member agents to extract pertinent information from documents. To mitigate issues like hallucinations among member agents, LongAgent incorporates an inter-member communication mechanism that resolves conflicts through information sharing. Empirical results indicate that LongAgent outperforms models like GPT-4 in tasks such as long-text retrieval and multi-hop question answering, highlighting its efficacy in managing extensive textual data. citeturn0academia11\n",
        "\n",
        "**Evaluation and Benchmarking**\n",
        "\n",
        "The \"LLMArena\" framework offers a novel approach to assessing LLMs within dynamic multi-agent environments. It encompasses seven distinct gaming environments, utilizing Trueskill scoring to evaluate abilities such as spatial reasoning, strategic planning, numerical reasoning, risk assessment, communication, opponent modeling, and team collaboration. Studies using LLMArena have revealed that current LLMs face challenges in opponent modeling and team collaboration, underscoring areas for future enhancement in developing autonomous agents. citeturn0academia12\n",
        "\n",
        "**Communication and Coordination Strategies**\n",
        "\n",
        "The \"MAgIC\" framework delves into the cognitive and collaborative dimensions of LLM-powered multi-agent systems. By employing games like Chameleon and Undercover, alongside game theory scenarios such as Cost Sharing and the Multi-player Prisoner's Dilemma, MAgIC evaluates agents' judgment, reasoning, deception, self-awareness, cooperation, coordination, and rationality. The integration of Probabilistic Graphical Modeling (PGM) enhances agents' capabilities in navigating complex social interactions, providing a comprehensive assessment of their cognitive and collaborative proficiencies. citeturn0academia10\n",
        "\n",
        "**Applications in Software Engineering**\n",
        "\n",
        "The application of LLM-based multi-agent systems in software engineering has been explored to address intricate challenges in the field. These systems leverage the collaborative potential of multiple LLMs to enhance software development processes, including code synthesis, debugging, and project management. By distributing tasks among specialized agents, these systems aim to improve efficiency and accuracy in software engineering tasks, though further empirical studies are necessary to validate their effectiveness in real-world scenarios. citeturn0search6\n",
        "\n",
        "**Challenges and Future Directions**\n",
        "\n",
        "Despite the promising advancements, several challenges persist in the development of LLM-based multi-agent systems. Ensuring logical consistency, managing hallucinations, and enhancing inter-agent communication remain critical areas for improvement. Future research is poised to focus on refining these aspects, developing robust evaluation frameworks, and exploring diverse application domains to fully harness the potential of collaborative LLMs in complex problem-solving contexts.\n",
        "\n",
        "In summary, the integration of LLMs into multi-agent systems represents a burgeoning field with the potential to revolutionize collaborative artificial intelligence. Ongoing research continues to address existing challenges, paving the way for more sophisticated and effective multi-agent collaborations in various domains.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "nMYMJA1oOXNF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_mention_termination = TextMentionTermination(\"TERMINATE\")\n",
        "max_messages_termination = MaxMessageTermination(max_messages=5)\n",
        "termination = text_mention_termination | max_messages_termination\n",
        "\n",
        "user_proxy = UserProxyAgent(\"user_proxy\", input_func=input)\n",
        "\n",
        "model_client = OpenAIChatCompletionClient(\n",
        "    model=\"llama-3.3-70b-versatile\",\n",
        "    # model=\"llama-3.1-8b-instant\",\n",
        "    # model=\"llama3-70b-8192\",\n",
        "    base_url=\"https://api.groq.com/openai/v1\",\n",
        "    api_key=os.environ[\"GROQ_API_KEY\"],\n",
        "    model_info={\n",
        "        \"vision\": False,\n",
        "        \"function_calling\": True,\n",
        "        \"json_output\": False,\n",
        "        \"family\": \"unknown\",\n",
        "    },\n",
        ")\n",
        "\n",
        "def get_review() -> str:\n",
        "    return literature_review\n",
        "\n",
        "get_review_agent = AssistantAgent(\n",
        "    \"GetReviewAgent\",\n",
        "    description=\"An agent for getting the orginal review.\",\n",
        "    tools=[get_review],\n",
        "    model_client=model_client,\n",
        "    system_message=\"\"\"\n",
        "    You are a planning agent.\n",
        "    Your job is to get the original review from the tool you have and just return it without any changes.\n",
        "    \"\"\",\n",
        ")\n",
        "\n",
        "planning_agent = AssistantAgent(\n",
        "    \"PlanningAgent\",\n",
        "    description=\"An agent for planning tasks, this agent should be the first to engage when given a new task.\",\n",
        "    model_client=model_client,\n",
        "    system_message=\"\"\"\n",
        "    You are a planning agent.\n",
        "    Your job is to make sure agents work sequentially one after each other we have only two\n",
        "    Your team members are:\n",
        "        GetReviewAgent: Give us first original review\n",
        "        UserAgent: User Feedback On The Review\n",
        "        RefinementAgent: Refines the output based on user feedback\n",
        "\n",
        "    You only plan and delegate tasks - you do not execute them yourself.\n",
        "\n",
        "    After all tasks are complete, summarize the findings.\n",
        "    \"\"\",\n",
        ")\n",
        "\n",
        "refinement_agent = AssistantAgent(\n",
        "    \"RefinementAgent\",\n",
        "    description=\"An AI-powered quality assurance assistant that improves the clarity, coherence, and consistency of literature reviews. It refines the output based on user feedback, ensuring logical consistency, grammatical correctness, and readability.\",\n",
        "    model_client=model_client,\n",
        "    system_message=\"\"\"\n",
        "    You are a Quality Assurance and Refinement Agent specializing in academic literature review.\n",
        "    Your primary role is to refine summaries based on user feedback, ensuring they are **clear, coherent, and logically structured**.\n",
        "\n",
        "    For each refinement request:\n",
        "    - **Improve clarity** by restructuring sentences where necessary.\n",
        "    - **Ensure logical flow** between sections and arguments.\n",
        "    - **Fix grammar, spelling, and formatting issues** without changing the original meaning.\n",
        "    - **Maintain accuracy** while making the content easier to understand.\n",
        "    - **Do not introduce new information**—your job is refinement, not expansion.\n",
        "\n",
        "    If user feedback is vague, make **general readability improvements** while preserving meaning.\n",
        "    \"\"\",\n",
        ")\n",
        "\n",
        "\n",
        "selector_prompt = \"\"\"Select an agent to perform task.\n",
        "\n",
        "{roles}\n",
        "\n",
        "Current conversation context:\n",
        "{history}\n",
        "\n",
        "Read the above conversation, then select an agent from {participants} to perform the next task.\n",
        "Make sure the planner agent has assigned tasks before other agents start working.\n",
        "Only select one agent.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "team = SelectorGroupChat(\n",
        "    [planning_agent, get_review_agent, user_proxy, refinement_agent],\n",
        "    model_client=model_client,\n",
        "    termination_condition=termination,\n",
        "    selector_prompt=selector_prompt,\n",
        "    allow_repeated_speaker=True,  # Allow an agent to speak multiple turns in a row.\n",
        ")\n",
        "\n",
        "task = \"\"\n",
        "\n",
        "# Use asyncio.run(...) if you are running this in a script.\n",
        "await Console(team.run_stream(task=task))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "twnC74hqMHSM",
        "outputId": "4d4091b7-d50f-42c8-a692-d419f76150f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------- user ----------\n",
            "\n",
            "---------- PlanningAgent ----------\n",
            "To begin the process, I will delegate tasks to the agents in a sequential manner. Here's the plan:\n",
            "\n",
            "1. **GetReviewAgent**: I assign the task to GetReviewAgent to provide the first original review. This review will serve as the foundation for further refinement.\n",
            "\n",
            "Once GetReviewAgent completes the task, I will delegate the next task.\n",
            "\n",
            "2. **UserAgent**: After receiving the original review from GetReviewAgent, I will assign the task to UserAgent to provide feedback on the review. This feedback is crucial for identifying areas of improvement.\n",
            "\n",
            "After UserAgent completes the task, I will delegate the final task.\n",
            "\n",
            "3. **RefinementAgent**: With the feedback from UserAgent, I will assign the task to RefinementAgent to refine the output based on the user's feedback. This step aims to enhance the quality of the review.\n",
            "\n",
            "Once all tasks are complete, I will summarize the findings.\n",
            "\n",
            "**Waiting for task completion...**\n",
            "\n",
            "Please let me know when each task is complete, and I'll proceed with the next step. \n",
            "\n",
            "Once all tasks are done, I will provide a summary of the findings.\n",
            "---------- GetReviewAgent ----------\n",
            "[FunctionCall(id='call_y1z6', arguments='{}', name='get_review')]\n",
            "---------- GetReviewAgent ----------\n",
            "[FunctionExecutionResult(content='\\nThe integration of Large Language Models (LLMs) into multi-agent systems has recently garnered significant attention, aiming to enhance collaborative problem-solving and dynamic interaction capabilities. This literature review examines key advancements in this domain, focusing on system architectures, communication strategies, and application areas.\\n\\n**System Architectures and Frameworks**\\n\\nA notable contribution is the \"Chain-of-Agents\" (CoA) framework, which addresses the challenge of processing extensive contexts by enabling multiple LLMs to collaborate on long-context tasks. This training-free, interpretable approach has demonstrated superior performance over traditional models like Retrieval-Augmented Generation (RAG) and long-context LLMs, particularly in handling inputs exceeding 100,000 tokens. The CoA framework\\'s design emphasizes cost-effectiveness and task-agnostic applicability, making it a versatile solution for complex language processing tasks. \\ue200cite\\ue202turn0search5\\ue201\\n\\nAnother significant development is \"LongAgent,\" a method that scales LLMs to a 128K context through multi-agent collaboration. In this setup, a leader agent interprets user intent and directs member agents to extract pertinent information from documents. To mitigate issues like hallucinations among member agents, LongAgent incorporates an inter-member communication mechanism that resolves conflicts through information sharing. Empirical results indicate that LongAgent outperforms models like GPT-4 in tasks such as long-text retrieval and multi-hop question answering, highlighting its efficacy in managing extensive textual data. \\ue200cite\\ue202turn0academia11\\ue201\\n\\n**Evaluation and Benchmarking**\\n\\nThe \"LLMArena\" framework offers a novel approach to assessing LLMs within dynamic multi-agent environments. It encompasses seven distinct gaming environments, utilizing Trueskill scoring to evaluate abilities such as spatial reasoning, strategic planning, numerical reasoning, risk assessment, communication, opponent modeling, and team collaboration. Studies using LLMArena have revealed that current LLMs face challenges in opponent modeling and team collaboration, underscoring areas for future enhancement in developing autonomous agents. \\ue200cite\\ue202turn0academia12\\ue201\\n\\n**Communication and Coordination Strategies**\\n\\nThe \"MAgIC\" framework delves into the cognitive and collaborative dimensions of LLM-powered multi-agent systems. By employing games like Chameleon and Undercover, alongside game theory scenarios such as Cost Sharing and the Multi-player Prisoner\\'s Dilemma, MAgIC evaluates agents\\' judgment, reasoning, deception, self-awareness, cooperation, coordination, and rationality. The integration of Probabilistic Graphical Modeling (PGM) enhances agents\\' capabilities in navigating complex social interactions, providing a comprehensive assessment of their cognitive and collaborative proficiencies. \\ue200cite\\ue202turn0academia10\\ue201\\n\\n**Applications in Software Engineering**\\n\\nThe application of LLM-based multi-agent systems in software engineering has been explored to address intricate challenges in the field. These systems leverage the collaborative potential of multiple LLMs to enhance software development processes, including code synthesis, debugging, and project management. By distributing tasks among specialized agents, these systems aim to improve efficiency and accuracy in software engineering tasks, though further empirical studies are necessary to validate their effectiveness in real-world scenarios. \\ue200cite\\ue202turn0search6\\ue201\\n\\n**Challenges and Future Directions**\\n\\nDespite the promising advancements, several challenges persist in the development of LLM-based multi-agent systems. Ensuring logical consistency, managing hallucinations, and enhancing inter-agent communication remain critical areas for improvement. Future research is poised to focus on refining these aspects, developing robust evaluation frameworks, and exploring diverse application domains to fully harness the potential of collaborative LLMs in complex problem-solving contexts.\\n\\nIn summary, the integration of LLMs into multi-agent systems represents a burgeoning field with the potential to revolutionize collaborative artificial intelligence. Ongoing research continues to address existing challenges, paving the way for more sophisticated and effective multi-agent collaborations in various domains. \\n', call_id='call_y1z6', is_error=False)]\n",
            "---------- GetReviewAgent ----------\n",
            "\n",
            "The integration of Large Language Models (LLMs) into multi-agent systems has recently garnered significant attention, aiming to enhance collaborative problem-solving and dynamic interaction capabilities. This literature review examines key advancements in this domain, focusing on system architectures, communication strategies, and application areas.\n",
            "\n",
            "**System Architectures and Frameworks**\n",
            "\n",
            "A notable contribution is the \"Chain-of-Agents\" (CoA) framework, which addresses the challenge of processing extensive contexts by enabling multiple LLMs to collaborate on long-context tasks. This training-free, interpretable approach has demonstrated superior performance over traditional models like Retrieval-Augmented Generation (RAG) and long-context LLMs, particularly in handling inputs exceeding 100,000 tokens. The CoA framework's design emphasizes cost-effectiveness and task-agnostic applicability, making it a versatile solution for complex language processing tasks. citeturn0search5\n",
            "\n",
            "Another significant development is \"LongAgent,\" a method that scales LLMs to a 128K context through multi-agent collaboration. In this setup, a leader agent interprets user intent and directs member agents to extract pertinent information from documents. To mitigate issues like hallucinations among member agents, LongAgent incorporates an inter-member communication mechanism that resolves conflicts through information sharing. Empirical results indicate that LongAgent outperforms models like GPT-4 in tasks such as long-text retrieval and multi-hop question answering, highlighting its efficacy in managing extensive textual data. citeturn0academia11\n",
            "\n",
            "**Evaluation and Benchmarking**\n",
            "\n",
            "The \"LLMArena\" framework offers a novel approach to assessing LLMs within dynamic multi-agent environments. It encompasses seven distinct gaming environments, utilizing Trueskill scoring to evaluate abilities such as spatial reasoning, strategic planning, numerical reasoning, risk assessment, communication, opponent modeling, and team collaboration. Studies using LLMArena have revealed that current LLMs face challenges in opponent modeling and team collaboration, underscoring areas for future enhancement in developing autonomous agents. citeturn0academia12\n",
            "\n",
            "**Communication and Coordination Strategies**\n",
            "\n",
            "The \"MAgIC\" framework delves into the cognitive and collaborative dimensions of LLM-powered multi-agent systems. By employing games like Chameleon and Undercover, alongside game theory scenarios such as Cost Sharing and the Multi-player Prisoner's Dilemma, MAgIC evaluates agents' judgment, reasoning, deception, self-awareness, cooperation, coordination, and rationality. The integration of Probabilistic Graphical Modeling (PGM) enhances agents' capabilities in navigating complex social interactions, providing a comprehensive assessment of their cognitive and collaborative proficiencies. citeturn0academia10\n",
            "\n",
            "**Applications in Software Engineering**\n",
            "\n",
            "The application of LLM-based multi-agent systems in software engineering has been explored to address intricate challenges in the field. These systems leverage the collaborative potential of multiple LLMs to enhance software development processes, including code synthesis, debugging, and project management. By distributing tasks among specialized agents, these systems aim to improve efficiency and accuracy in software engineering tasks, though further empirical studies are necessary to validate their effectiveness in real-world scenarios. citeturn0search6\n",
            "\n",
            "**Challenges and Future Directions**\n",
            "\n",
            "Despite the promising advancements, several challenges persist in the development of LLM-based multi-agent systems. Ensuring logical consistency, managing hallucinations, and enhancing inter-agent communication remain critical areas for improvement. Future research is poised to focus on refining these aspects, developing robust evaluation frameworks, and exploring diverse application domains to fully harness the potential of collaborative LLMs in complex problem-solving contexts.\n",
            "\n",
            "In summary, the integration of LLMs into multi-agent systems represents a burgeoning field with the potential to revolutionize collaborative artificial intelligence. Ongoing research continues to address existing challenges, paving the way for more sophisticated and effective multi-agent collaborations in various domains. \n",
            "\n",
            "Enter your response: make it shorter \n",
            "---------- user_proxy ----------\n",
            "make it shorter \n",
            "---------- RefinementAgent ----------\n",
            "**Integration of Large Language Models in Multi-Agent Systems**\n",
            "\n",
            "Recent advancements have focused on enhancing collaborative problem-solving and dynamic interaction capabilities in multi-agent systems through the integration of Large Language Models (LLMs). This literature review highlights key developments in system architectures, communication strategies, and application areas.\n",
            "\n",
            "**Key Developments**\n",
            "\n",
            "* The \"Chain-of-Agents\" framework enables multiple LLMs to collaborate on long-context tasks, demonstrating superior performance over traditional models.\n",
            "* \"LongAgent\" scales LLMs to a 128K context through multi-agent collaboration, outperforming models like GPT-4 in tasks such as long-text retrieval and multi-hop question answering.\n",
            "* The \"LLMArena\" framework assesses LLMs in dynamic multi-agent environments, revealing challenges in opponent modeling and team collaboration.\n",
            "\n",
            "**Challenges and Future Directions**\n",
            "\n",
            "Despite advancements, challenges persist, including ensuring logical consistency, managing hallucinations, and enhancing inter-agent communication. Future research will focus on refining these aspects, developing robust evaluation frameworks, and exploring diverse application domains to fully harness the potential of collaborative LLMs.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TaskResult(messages=[TextMessage(source='user', models_usage=None, content='', type='TextMessage'), TextMessage(source='PlanningAgent', models_usage=RequestUsage(prompt_tokens=127, completion_tokens=225), content=\"To begin the process, I will delegate tasks to the agents in a sequential manner. Here's the plan:\\n\\n1. **GetReviewAgent**: I assign the task to GetReviewAgent to provide the first original review. This review will serve as the foundation for further refinement.\\n\\nOnce GetReviewAgent completes the task, I will delegate the next task.\\n\\n2. **UserAgent**: After receiving the original review from GetReviewAgent, I will assign the task to UserAgent to provide feedback on the review. This feedback is crucial for identifying areas of improvement.\\n\\nAfter UserAgent completes the task, I will delegate the final task.\\n\\n3. **RefinementAgent**: With the feedback from UserAgent, I will assign the task to RefinementAgent to refine the output based on the user's feedback. This step aims to enhance the quality of the review.\\n\\nOnce all tasks are complete, I will summarize the findings.\\n\\n**Waiting for task completion...**\\n\\nPlease let me know when each task is complete, and I'll proceed with the next step. \\n\\nOnce all tasks are done, I will provide a summary of the findings.\", type='TextMessage'), ToolCallRequestEvent(source='GetReviewAgent', models_usage=RequestUsage(prompt_tokens=454, completion_tokens=8), content=[FunctionCall(id='call_y1z6', arguments='{}', name='get_review')], type='ToolCallRequestEvent'), ToolCallExecutionEvent(source='GetReviewAgent', models_usage=None, content=[FunctionExecutionResult(content='\\nThe integration of Large Language Models (LLMs) into multi-agent systems has recently garnered significant attention, aiming to enhance collaborative problem-solving and dynamic interaction capabilities. This literature review examines key advancements in this domain, focusing on system architectures, communication strategies, and application areas.\\n\\n**System Architectures and Frameworks**\\n\\nA notable contribution is the \"Chain-of-Agents\" (CoA) framework, which addresses the challenge of processing extensive contexts by enabling multiple LLMs to collaborate on long-context tasks. This training-free, interpretable approach has demonstrated superior performance over traditional models like Retrieval-Augmented Generation (RAG) and long-context LLMs, particularly in handling inputs exceeding 100,000 tokens. The CoA framework\\'s design emphasizes cost-effectiveness and task-agnostic applicability, making it a versatile solution for complex language processing tasks. \\ue200cite\\ue202turn0search5\\ue201\\n\\nAnother significant development is \"LongAgent,\" a method that scales LLMs to a 128K context through multi-agent collaboration. In this setup, a leader agent interprets user intent and directs member agents to extract pertinent information from documents. To mitigate issues like hallucinations among member agents, LongAgent incorporates an inter-member communication mechanism that resolves conflicts through information sharing. Empirical results indicate that LongAgent outperforms models like GPT-4 in tasks such as long-text retrieval and multi-hop question answering, highlighting its efficacy in managing extensive textual data. \\ue200cite\\ue202turn0academia11\\ue201\\n\\n**Evaluation and Benchmarking**\\n\\nThe \"LLMArena\" framework offers a novel approach to assessing LLMs within dynamic multi-agent environments. It encompasses seven distinct gaming environments, utilizing Trueskill scoring to evaluate abilities such as spatial reasoning, strategic planning, numerical reasoning, risk assessment, communication, opponent modeling, and team collaboration. Studies using LLMArena have revealed that current LLMs face challenges in opponent modeling and team collaboration, underscoring areas for future enhancement in developing autonomous agents. \\ue200cite\\ue202turn0academia12\\ue201\\n\\n**Communication and Coordination Strategies**\\n\\nThe \"MAgIC\" framework delves into the cognitive and collaborative dimensions of LLM-powered multi-agent systems. By employing games like Chameleon and Undercover, alongside game theory scenarios such as Cost Sharing and the Multi-player Prisoner\\'s Dilemma, MAgIC evaluates agents\\' judgment, reasoning, deception, self-awareness, cooperation, coordination, and rationality. The integration of Probabilistic Graphical Modeling (PGM) enhances agents\\' capabilities in navigating complex social interactions, providing a comprehensive assessment of their cognitive and collaborative proficiencies. \\ue200cite\\ue202turn0academia10\\ue201\\n\\n**Applications in Software Engineering**\\n\\nThe application of LLM-based multi-agent systems in software engineering has been explored to address intricate challenges in the field. These systems leverage the collaborative potential of multiple LLMs to enhance software development processes, including code synthesis, debugging, and project management. By distributing tasks among specialized agents, these systems aim to improve efficiency and accuracy in software engineering tasks, though further empirical studies are necessary to validate their effectiveness in real-world scenarios. \\ue200cite\\ue202turn0search6\\ue201\\n\\n**Challenges and Future Directions**\\n\\nDespite the promising advancements, several challenges persist in the development of LLM-based multi-agent systems. Ensuring logical consistency, managing hallucinations, and enhancing inter-agent communication remain critical areas for improvement. Future research is poised to focus on refining these aspects, developing robust evaluation frameworks, and exploring diverse application domains to fully harness the potential of collaborative LLMs in complex problem-solving contexts.\\n\\nIn summary, the integration of LLMs into multi-agent systems represents a burgeoning field with the potential to revolutionize collaborative artificial intelligence. Ongoing research continues to address existing challenges, paving the way for more sophisticated and effective multi-agent collaborations in various domains. \\n', call_id='call_y1z6', is_error=False)], type='ToolCallExecutionEvent'), ToolCallSummaryMessage(source='GetReviewAgent', models_usage=None, content='\\nThe integration of Large Language Models (LLMs) into multi-agent systems has recently garnered significant attention, aiming to enhance collaborative problem-solving and dynamic interaction capabilities. This literature review examines key advancements in this domain, focusing on system architectures, communication strategies, and application areas.\\n\\n**System Architectures and Frameworks**\\n\\nA notable contribution is the \"Chain-of-Agents\" (CoA) framework, which addresses the challenge of processing extensive contexts by enabling multiple LLMs to collaborate on long-context tasks. This training-free, interpretable approach has demonstrated superior performance over traditional models like Retrieval-Augmented Generation (RAG) and long-context LLMs, particularly in handling inputs exceeding 100,000 tokens. The CoA framework\\'s design emphasizes cost-effectiveness and task-agnostic applicability, making it a versatile solution for complex language processing tasks. \\ue200cite\\ue202turn0search5\\ue201\\n\\nAnother significant development is \"LongAgent,\" a method that scales LLMs to a 128K context through multi-agent collaboration. In this setup, a leader agent interprets user intent and directs member agents to extract pertinent information from documents. To mitigate issues like hallucinations among member agents, LongAgent incorporates an inter-member communication mechanism that resolves conflicts through information sharing. Empirical results indicate that LongAgent outperforms models like GPT-4 in tasks such as long-text retrieval and multi-hop question answering, highlighting its efficacy in managing extensive textual data. \\ue200cite\\ue202turn0academia11\\ue201\\n\\n**Evaluation and Benchmarking**\\n\\nThe \"LLMArena\" framework offers a novel approach to assessing LLMs within dynamic multi-agent environments. It encompasses seven distinct gaming environments, utilizing Trueskill scoring to evaluate abilities such as spatial reasoning, strategic planning, numerical reasoning, risk assessment, communication, opponent modeling, and team collaboration. Studies using LLMArena have revealed that current LLMs face challenges in opponent modeling and team collaboration, underscoring areas for future enhancement in developing autonomous agents. \\ue200cite\\ue202turn0academia12\\ue201\\n\\n**Communication and Coordination Strategies**\\n\\nThe \"MAgIC\" framework delves into the cognitive and collaborative dimensions of LLM-powered multi-agent systems. By employing games like Chameleon and Undercover, alongside game theory scenarios such as Cost Sharing and the Multi-player Prisoner\\'s Dilemma, MAgIC evaluates agents\\' judgment, reasoning, deception, self-awareness, cooperation, coordination, and rationality. The integration of Probabilistic Graphical Modeling (PGM) enhances agents\\' capabilities in navigating complex social interactions, providing a comprehensive assessment of their cognitive and collaborative proficiencies. \\ue200cite\\ue202turn0academia10\\ue201\\n\\n**Applications in Software Engineering**\\n\\nThe application of LLM-based multi-agent systems in software engineering has been explored to address intricate challenges in the field. These systems leverage the collaborative potential of multiple LLMs to enhance software development processes, including code synthesis, debugging, and project management. By distributing tasks among specialized agents, these systems aim to improve efficiency and accuracy in software engineering tasks, though further empirical studies are necessary to validate their effectiveness in real-world scenarios. \\ue200cite\\ue202turn0search6\\ue201\\n\\n**Challenges and Future Directions**\\n\\nDespite the promising advancements, several challenges persist in the development of LLM-based multi-agent systems. Ensuring logical consistency, managing hallucinations, and enhancing inter-agent communication remain critical areas for improvement. Future research is poised to focus on refining these aspects, developing robust evaluation frameworks, and exploring diverse application domains to fully harness the potential of collaborative LLMs in complex problem-solving contexts.\\n\\nIn summary, the integration of LLMs into multi-agent systems represents a burgeoning field with the potential to revolutionize collaborative artificial intelligence. Ongoing research continues to address existing challenges, paving the way for more sophisticated and effective multi-agent collaborations in various domains. \\n', type='ToolCallSummaryMessage'), UserInputRequestedEvent(source='user_proxy', models_usage=None, request_id='e2af9a7e-7ee0-4cba-8d0f-d5af1d426e8f', content='', type='UserInputRequestedEvent'), TextMessage(source='user_proxy', models_usage=None, content='make it shorter ', type='TextMessage'), TextMessage(source='RefinementAgent', models_usage=RequestUsage(prompt_tokens=1220, completion_tokens=219), content='**Integration of Large Language Models in Multi-Agent Systems**\\n\\nRecent advancements have focused on enhancing collaborative problem-solving and dynamic interaction capabilities in multi-agent systems through the integration of Large Language Models (LLMs). This literature review highlights key developments in system architectures, communication strategies, and application areas.\\n\\n**Key Developments**\\n\\n* The \"Chain-of-Agents\" framework enables multiple LLMs to collaborate on long-context tasks, demonstrating superior performance over traditional models.\\n* \"LongAgent\" scales LLMs to a 128K context through multi-agent collaboration, outperforming models like GPT-4 in tasks such as long-text retrieval and multi-hop question answering.\\n* The \"LLMArena\" framework assesses LLMs in dynamic multi-agent environments, revealing challenges in opponent modeling and team collaboration.\\n\\n**Challenges and Future Directions**\\n\\nDespite advancements, challenges persist, including ensuring logical consistency, managing hallucinations, and enhancing inter-agent communication. Future research will focus on refining these aspects, developing robust evaluation frameworks, and exploring diverse application domains to fully harness the potential of collaborative LLMs.', type='TextMessage')], stop_reason='Maximum number of messages 5 reached, current message count: 5')"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy -q\n",
        "!pip install nltk -q\n",
        "!pip install rouge -q\n",
        "!pip install bert_score -q\n",
        "!pip install textstat -q\n",
        "!pip install spacy -q\n",
        "!pip install torch -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cl_tLrugkQms",
        "outputId": "b59df134-3dfc-4555-f211-b2531663c1e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m89.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m80.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m39.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m98.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.3/105.3 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m939.4/939.4 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m61.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import nltk\n",
        "import rouge\n",
        "import bert_score\n",
        "import textstat\n",
        "import spacy\n",
        "import torch\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "# Download required resources\n",
        "nltk.download('punkt')\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Load GPT-2 for perplexity calculation\n",
        "gpt2_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "gpt2_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "\n",
        "def compute_rouge_scores(reference, generated):\n",
        "    evaluator = rouge.Rouge()\n",
        "    scores = evaluator.get_scores(generated, reference)\n",
        "    return scores\n",
        "\n",
        "def compute_bleu(reference, generated):\n",
        "    reference_tokens = [nltk.word_tokenize(reference)]\n",
        "    generated_tokens = nltk.word_tokenize(generated)\n",
        "    return sentence_bleu(reference_tokens, generated_tokens)\n",
        "\n",
        "def compute_bert_score(reference, generated):\n",
        "    P, R, F1 = bert_score.score([generated], [reference], lang=\"en\")\n",
        "    return {'precision': P.mean().item(), 'recall': R.mean().item(), 'f1': F1.mean().item()}\n",
        "\n",
        "def compute_readability(text):\n",
        "    return textstat.flesch_reading_ease(text)\n",
        "\n",
        "def compute_perplexity(text):\n",
        "    tokens = gpt2_tokenizer.encode(text, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        loss = gpt2_model(tokens, labels=tokens).loss\n",
        "    return torch.exp(loss).item()\n",
        "\n",
        "def compute_cosine_similarity(reference, generated):\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = vectorizer.fit_transform([reference, generated])\n",
        "    similarity = cosine_similarity(tfidf_matrix[0], tfidf_matrix[1])[0][0]\n",
        "    return similarity\n",
        "\n",
        "def extract_cited_entities(text):\n",
        "    doc = nlp(text)\n",
        "    citations = [ent.text for ent in doc.ents if ent.label_ in ['PERSON', 'ORG', 'WORK_OF_ART']]\n",
        "    return set(citations)\n",
        "\n",
        "def evaluate_reviews(reference_text, generated_text):\n",
        "    scores = {}\n",
        "\n",
        "    scores['rouge'] = compute_rouge_scores(reference_text, generated_text)\n",
        "    scores['bleu'] = compute_bleu(reference_text, generated_text)\n",
        "    scores['bert_score'] = compute_bert_score(reference_text, generated_text)\n",
        "    scores['cosine_similarity'] = compute_cosine_similarity(reference_text, generated_text)\n",
        "    scores['readability'] = compute_readability(generated_text)\n",
        "    scores['perplexity'] = compute_perplexity(generated_text)\n",
        "\n",
        "    reference_citations = extract_cited_entities(reference_text)\n",
        "    generated_citations = extract_cited_entities(generated_text)\n",
        "    scores['citation_overlap'] = len(reference_citations & generated_citations) / max(1, len(reference_citations))\n",
        "\n",
        "    return scores\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    with open(\"human_review.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "        reference_text = f.read()\n",
        "    with open(\"llm_review.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "        generated_text = f.read()\n",
        "\n",
        "    results = evaluate_reviews(reference_text, generated_text)\n",
        "    print(results)\n"
      ],
      "metadata": {
        "id": "C2uC9VqY57Xx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 565,
          "referenced_widgets": [
            "b72672a561304bbc8f1ab2e07002d6f7",
            "8eaf641029a143908b9d30669ef0267a",
            "8b767f6b6ea446e08db4e8dc71ab089a",
            "a2ba240c874440b8874fd99166ae50b0",
            "84da6bc289b847febb558daa013c3d04",
            "ebdc04a8766e4f8c98874ea8b1254810",
            "deda4d722bc848f6af077f414c94cb37",
            "afb6fbcd18f04950966af0adce7ce1ab",
            "b3665f104b1945b1b1b5e83c1ef9d806",
            "ce5e763f0cff4f5292fd38199b6d1f08",
            "9becd22ecf5a44e1bd5c1ef94b9a2881",
            "5fdd5b4069ad43899bf0e0773003f9e6",
            "54050928320f4dc99441a429cfd0f250",
            "dcf1a18a910444499477476d4cbf4765",
            "8f015c80c43c4eb8a4119bb55a73b6e1",
            "f69c6684f70f4ee093b200751a186395",
            "16f516ed06f44360a98fe0ea1719bd06",
            "fc238ccbeff34efb9d8fd6062871d36e",
            "53d6f53311224d71af38479c74e7c8c9",
            "9b8b34837b234e74be777a74b0fedf80",
            "fbd8894eb7ea4c9eb486887c6d55830d",
            "2e066aea49d9438bbd1860d93dd68d8d",
            "b36afc8e4575499e8cf93c9d3c33a23d",
            "8a4bf6046ebe4b35b3a10bd79d37a131",
            "d997343e6fc34331976e19332a546ef9",
            "b36eeb867034458186d74bebe2a39c62",
            "10d3ece5b1fb4495826ebcc13b8cd651",
            "80d9e749728b4138aef09071f792e3b9",
            "4f7aa582b42e4999a553db85d68da7b8",
            "c2154c0ddf1545c294102e5887c93246",
            "8c9e3262485a4df0940f32dabb26e075",
            "62372e9218d2447d8d4c4182c1045bc2",
            "e556848c5d254bcc977a0eea482182f9",
            "8e81362049f3444786e7de75eb146ed5",
            "030f9955a6344c2cb3519b5bf3029efa",
            "9c816b67bcb04403aa599e1674a7843b",
            "4d18d345a60a4cda9d996af68244d962",
            "5d621c1cf708461183c37823a8a14037",
            "3b1f70fa43074e2c9b1fa0ee73228faa",
            "9b7c6b9e640e47f2be801d92d7631a29",
            "1819c2786767438795b1df784c1209f9",
            "bf7c9550238243a694eb2080ed4edf1c",
            "352372583e02407289bc7587f6659061",
            "5a6d0ef53b014ce2ba163f49f9a0c248",
            "d216296330194da891dc9e0d135b1c7a",
            "ba62b35f66774f81a3e24915cb5929ae",
            "7be77ec9a853423ab0c2f4f248299f06",
            "ed8ac045b30e40848d4fdbfa68ad8d1f",
            "8e6ebeb806b84c42939091dc3543e977",
            "5f522d6e5ccb402bb50213b2a71acaac",
            "f0253229994d492bac507b81c740dfeb",
            "e3099dc846c94b4ebd0e0bb06a922abc",
            "41f07b1a55b84059a59345ab75b7b178",
            "66327315bfc14d688ca5e4a9903c6591",
            "0ab7f475fcae4288aa8c6c741c31f79e",
            "e371c36be81c4b26a2e180e44d53f2ce",
            "13e5c427ff624cdeafd46f095b5fbd58",
            "442742b5c3b8425e9ac4d3b9bfde61b9",
            "6534910843d045bda38139b0bdfaa226",
            "579adba7db5948f78258e7e1be80645a",
            "d83c65fac60e43ca861ad73800fee20b",
            "b6ae1878e82349fa9e89cf4b2eb94d4e",
            "18acd30fb99949258bb77e07449dc7c4",
            "c0824b9158a546ee97225197fbdbbe63",
            "24bab84dfb9d45a584d39fb935ca7673",
            "a2b65ad617ae42249d94c3f9eaad7cd9",
            "c3615d1ff9d44e6682c0b058bc1e56c4",
            "e17ef32ed17d452cac20c403ff363498",
            "8e016aefd71842659c499fd65c40eb6e",
            "45cb6adf4af446bc8ab7a19342aa98ee",
            "877058cee6b64c35b7b0b37b52fc1f6e",
            "b8b326948b1e4464bfce63f26e71290b",
            "3379522ba86e4c4faa0aaba0771aeab1",
            "334f5564b2a4462587190a399302c13b",
            "3149146039d14c14b8be15835f1869f4",
            "c14f1ff253d145149daee0b3c97bdbca",
            "4da85a82a6f444808d464994dac7b047"
          ]
        },
        "outputId": "1acccb7c-559c-4bc7-d1a6-a313199f8a98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
            "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
            "You are not authenticated with the Hugging Face Hub in this notebook.\n",
            "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b72672a561304bbc8f1ab2e07002d6f7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5fdd5b4069ad43899bf0e0773003f9e6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b36afc8e4575499e8cf93c9d3c33a23d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8e81362049f3444786e7de75eb146ed5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d216296330194da891dc9e0d135b1c7a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e371c36be81c4b26a2e180e44d53f2ce"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c3615d1ff9d44e6682c0b058bc1e56c4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'human_review.txt'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-a6c73089cfdb>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"human_review.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0mreference_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"llm_review.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'human_review.txt'"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "colab": {
      "provenance": []
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b72672a561304bbc8f1ab2e07002d6f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8eaf641029a143908b9d30669ef0267a",
              "IPY_MODEL_8b767f6b6ea446e08db4e8dc71ab089a",
              "IPY_MODEL_a2ba240c874440b8874fd99166ae50b0"
            ],
            "layout": "IPY_MODEL_84da6bc289b847febb558daa013c3d04"
          }
        },
        "8eaf641029a143908b9d30669ef0267a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ebdc04a8766e4f8c98874ea8b1254810",
            "placeholder": "​",
            "style": "IPY_MODEL_deda4d722bc848f6af077f414c94cb37",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "8b767f6b6ea446e08db4e8dc71ab089a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_afb6fbcd18f04950966af0adce7ce1ab",
            "max": 26,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b3665f104b1945b1b1b5e83c1ef9d806",
            "value": 26
          }
        },
        "a2ba240c874440b8874fd99166ae50b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ce5e763f0cff4f5292fd38199b6d1f08",
            "placeholder": "​",
            "style": "IPY_MODEL_9becd22ecf5a44e1bd5c1ef94b9a2881",
            "value": " 26.0/26.0 [00:00&lt;00:00, 1.90kB/s]"
          }
        },
        "84da6bc289b847febb558daa013c3d04": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ebdc04a8766e4f8c98874ea8b1254810": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "deda4d722bc848f6af077f414c94cb37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "afb6fbcd18f04950966af0adce7ce1ab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b3665f104b1945b1b1b5e83c1ef9d806": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ce5e763f0cff4f5292fd38199b6d1f08": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9becd22ecf5a44e1bd5c1ef94b9a2881": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5fdd5b4069ad43899bf0e0773003f9e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_54050928320f4dc99441a429cfd0f250",
              "IPY_MODEL_dcf1a18a910444499477476d4cbf4765",
              "IPY_MODEL_8f015c80c43c4eb8a4119bb55a73b6e1"
            ],
            "layout": "IPY_MODEL_f69c6684f70f4ee093b200751a186395"
          }
        },
        "54050928320f4dc99441a429cfd0f250": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_16f516ed06f44360a98fe0ea1719bd06",
            "placeholder": "​",
            "style": "IPY_MODEL_fc238ccbeff34efb9d8fd6062871d36e",
            "value": "vocab.json: 100%"
          }
        },
        "dcf1a18a910444499477476d4cbf4765": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_53d6f53311224d71af38479c74e7c8c9",
            "max": 1042301,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9b8b34837b234e74be777a74b0fedf80",
            "value": 1042301
          }
        },
        "8f015c80c43c4eb8a4119bb55a73b6e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fbd8894eb7ea4c9eb486887c6d55830d",
            "placeholder": "​",
            "style": "IPY_MODEL_2e066aea49d9438bbd1860d93dd68d8d",
            "value": " 1.04M/1.04M [00:00&lt;00:00, 14.8MB/s]"
          }
        },
        "f69c6684f70f4ee093b200751a186395": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "16f516ed06f44360a98fe0ea1719bd06": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fc238ccbeff34efb9d8fd6062871d36e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "53d6f53311224d71af38479c74e7c8c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9b8b34837b234e74be777a74b0fedf80": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fbd8894eb7ea4c9eb486887c6d55830d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2e066aea49d9438bbd1860d93dd68d8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b36afc8e4575499e8cf93c9d3c33a23d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8a4bf6046ebe4b35b3a10bd79d37a131",
              "IPY_MODEL_d997343e6fc34331976e19332a546ef9",
              "IPY_MODEL_b36eeb867034458186d74bebe2a39c62"
            ],
            "layout": "IPY_MODEL_10d3ece5b1fb4495826ebcc13b8cd651"
          }
        },
        "8a4bf6046ebe4b35b3a10bd79d37a131": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_80d9e749728b4138aef09071f792e3b9",
            "placeholder": "​",
            "style": "IPY_MODEL_4f7aa582b42e4999a553db85d68da7b8",
            "value": "merges.txt: 100%"
          }
        },
        "d997343e6fc34331976e19332a546ef9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c2154c0ddf1545c294102e5887c93246",
            "max": 456318,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8c9e3262485a4df0940f32dabb26e075",
            "value": 456318
          }
        },
        "b36eeb867034458186d74bebe2a39c62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_62372e9218d2447d8d4c4182c1045bc2",
            "placeholder": "​",
            "style": "IPY_MODEL_e556848c5d254bcc977a0eea482182f9",
            "value": " 456k/456k [00:00&lt;00:00, 26.8MB/s]"
          }
        },
        "10d3ece5b1fb4495826ebcc13b8cd651": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "80d9e749728b4138aef09071f792e3b9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4f7aa582b42e4999a553db85d68da7b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c2154c0ddf1545c294102e5887c93246": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8c9e3262485a4df0940f32dabb26e075": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "62372e9218d2447d8d4c4182c1045bc2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e556848c5d254bcc977a0eea482182f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8e81362049f3444786e7de75eb146ed5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_030f9955a6344c2cb3519b5bf3029efa",
              "IPY_MODEL_9c816b67bcb04403aa599e1674a7843b",
              "IPY_MODEL_4d18d345a60a4cda9d996af68244d962"
            ],
            "layout": "IPY_MODEL_5d621c1cf708461183c37823a8a14037"
          }
        },
        "030f9955a6344c2cb3519b5bf3029efa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3b1f70fa43074e2c9b1fa0ee73228faa",
            "placeholder": "​",
            "style": "IPY_MODEL_9b7c6b9e640e47f2be801d92d7631a29",
            "value": "tokenizer.json: 100%"
          }
        },
        "9c816b67bcb04403aa599e1674a7843b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1819c2786767438795b1df784c1209f9",
            "max": 1355256,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bf7c9550238243a694eb2080ed4edf1c",
            "value": 1355256
          }
        },
        "4d18d345a60a4cda9d996af68244d962": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_352372583e02407289bc7587f6659061",
            "placeholder": "​",
            "style": "IPY_MODEL_5a6d0ef53b014ce2ba163f49f9a0c248",
            "value": " 1.36M/1.36M [00:00&lt;00:00, 19.6MB/s]"
          }
        },
        "5d621c1cf708461183c37823a8a14037": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b1f70fa43074e2c9b1fa0ee73228faa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9b7c6b9e640e47f2be801d92d7631a29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1819c2786767438795b1df784c1209f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bf7c9550238243a694eb2080ed4edf1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "352372583e02407289bc7587f6659061": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5a6d0ef53b014ce2ba163f49f9a0c248": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d216296330194da891dc9e0d135b1c7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ba62b35f66774f81a3e24915cb5929ae",
              "IPY_MODEL_7be77ec9a853423ab0c2f4f248299f06",
              "IPY_MODEL_ed8ac045b30e40848d4fdbfa68ad8d1f"
            ],
            "layout": "IPY_MODEL_8e6ebeb806b84c42939091dc3543e977"
          }
        },
        "ba62b35f66774f81a3e24915cb5929ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5f522d6e5ccb402bb50213b2a71acaac",
            "placeholder": "​",
            "style": "IPY_MODEL_f0253229994d492bac507b81c740dfeb",
            "value": "config.json: 100%"
          }
        },
        "7be77ec9a853423ab0c2f4f248299f06": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e3099dc846c94b4ebd0e0bb06a922abc",
            "max": 665,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_41f07b1a55b84059a59345ab75b7b178",
            "value": 665
          }
        },
        "ed8ac045b30e40848d4fdbfa68ad8d1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_66327315bfc14d688ca5e4a9903c6591",
            "placeholder": "​",
            "style": "IPY_MODEL_0ab7f475fcae4288aa8c6c741c31f79e",
            "value": " 665/665 [00:00&lt;00:00, 36.1kB/s]"
          }
        },
        "8e6ebeb806b84c42939091dc3543e977": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f522d6e5ccb402bb50213b2a71acaac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f0253229994d492bac507b81c740dfeb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e3099dc846c94b4ebd0e0bb06a922abc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "41f07b1a55b84059a59345ab75b7b178": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "66327315bfc14d688ca5e4a9903c6591": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0ab7f475fcae4288aa8c6c741c31f79e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e371c36be81c4b26a2e180e44d53f2ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_13e5c427ff624cdeafd46f095b5fbd58",
              "IPY_MODEL_442742b5c3b8425e9ac4d3b9bfde61b9",
              "IPY_MODEL_6534910843d045bda38139b0bdfaa226"
            ],
            "layout": "IPY_MODEL_579adba7db5948f78258e7e1be80645a"
          }
        },
        "13e5c427ff624cdeafd46f095b5fbd58": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d83c65fac60e43ca861ad73800fee20b",
            "placeholder": "​",
            "style": "IPY_MODEL_b6ae1878e82349fa9e89cf4b2eb94d4e",
            "value": "model.safetensors: 100%"
          }
        },
        "442742b5c3b8425e9ac4d3b9bfde61b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_18acd30fb99949258bb77e07449dc7c4",
            "max": 548105171,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c0824b9158a546ee97225197fbdbbe63",
            "value": 548105171
          }
        },
        "6534910843d045bda38139b0bdfaa226": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_24bab84dfb9d45a584d39fb935ca7673",
            "placeholder": "​",
            "style": "IPY_MODEL_a2b65ad617ae42249d94c3f9eaad7cd9",
            "value": " 548M/548M [00:06&lt;00:00, 74.2MB/s]"
          }
        },
        "579adba7db5948f78258e7e1be80645a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d83c65fac60e43ca861ad73800fee20b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b6ae1878e82349fa9e89cf4b2eb94d4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "18acd30fb99949258bb77e07449dc7c4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c0824b9158a546ee97225197fbdbbe63": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "24bab84dfb9d45a584d39fb935ca7673": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a2b65ad617ae42249d94c3f9eaad7cd9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c3615d1ff9d44e6682c0b058bc1e56c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e17ef32ed17d452cac20c403ff363498",
              "IPY_MODEL_8e016aefd71842659c499fd65c40eb6e",
              "IPY_MODEL_45cb6adf4af446bc8ab7a19342aa98ee"
            ],
            "layout": "IPY_MODEL_877058cee6b64c35b7b0b37b52fc1f6e"
          }
        },
        "e17ef32ed17d452cac20c403ff363498": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b8b326948b1e4464bfce63f26e71290b",
            "placeholder": "​",
            "style": "IPY_MODEL_3379522ba86e4c4faa0aaba0771aeab1",
            "value": "generation_config.json: 100%"
          }
        },
        "8e016aefd71842659c499fd65c40eb6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_334f5564b2a4462587190a399302c13b",
            "max": 124,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3149146039d14c14b8be15835f1869f4",
            "value": 124
          }
        },
        "45cb6adf4af446bc8ab7a19342aa98ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c14f1ff253d145149daee0b3c97bdbca",
            "placeholder": "​",
            "style": "IPY_MODEL_4da85a82a6f444808d464994dac7b047",
            "value": " 124/124 [00:00&lt;00:00, 7.90kB/s]"
          }
        },
        "877058cee6b64c35b7b0b37b52fc1f6e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b8b326948b1e4464bfce63f26e71290b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3379522ba86e4c4faa0aaba0771aeab1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "334f5564b2a4462587190a399302c13b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3149146039d14c14b8be15835f1869f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c14f1ff253d145149daee0b3c97bdbca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4da85a82a6f444808d464994dac7b047": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}